name: Site Health (HTTPS, redirects, cert + Gmail & Telegram alerts)

on:
  schedule:
    - cron: "0 */6 * * *" # Every 6 hours
  workflow_dispatch:

permissions:
  contents: read
  issues: write

concurrency:
  group: site-health
  cancel-in-progress: false

jobs:
  health:
    runs-on: ubuntu-latest
    env: 
    #I was getting alerts every 6 hours no matter what. 
    #This now only alerts me when there's an issue and it gets fixed.
      # Domains to check (one per line)
      DOMAINS: |
        jesse-anderson.net
        opcrime.jesse-anderson.net
      REDIRECT_LIMIT: "10"
      CERT_WARN_DAYS: "21"
    steps:
      - name: Check HTTP/HTTPS behavior (no loops, OK final)
        id: curlcheck
        run: |
            # -u only (keep unset-var safety), handle errors manually
            set -u
            bad=0
            printf "%-35s %-6s %-5s %s\n" "HOST" "CODE" "HOPS" "FINAL_URL"

            while IFS= read -r raw; do
            # Strip CRs and whitespace
            host="${raw//$'\r'/}"
            host="${host#"${host%%[![:space:]]*}"}"
            host="${host%"${host##*[![:space:]]}"}"
            [ -z "$host" ] && continue

            # Try the request; don't crash the step on curl error
            if ! out=$(curl -sIL --max-redirs "${REDIRECT_LIMIT}" \
                            -o /dev/null -w "%{http_code} %{url_effective} %{num_redirects}" \
                            "https://${host}/"); then
                echo "::error ::curl failed for https://${host}/ (exit $?)"
                bad=1
                continue
            fi

            code=$(awk '{print $1}' <<<"$out")
            final=$(awk '{print $2}' <<<"$out")
            hops=$(awk '{print $3}' <<<"$out")
            printf "%-35s %-6s %-5s %s\n" "$host" "$code" "$hops" "$final"

            if [ "$hops" -ge "${REDIRECT_LIMIT}" ]; then
                echo "::error ::Too many redirects ($hops) for https://${host}/"
                bad=1
            fi
            case "$code" in
                200|204|301|302|304) : ;;
                *) echo "::error ::Unexpected final status $code for https://${host}/"; bad=1 ;;
            esac
            done <<< "$DOMAINS"

            # Export result for later steps
            echo "curl_bad=$bad" >> "$GITHUB_OUTPUT"

      - name: Check TLS certificate expiry
        id: certexp
        shell: python
        run: |
            import os, socket, ssl
            from datetime import datetime, timezone

            warn_days = int(os.environ.get("CERT_WARN_DAYS","21"))
            # Trim whitespace/CRs, ignore blank lines
            domains = [d.strip().strip("\r") for d in os.environ["DOMAINS"].splitlines() if d.strip()]
            bad = 0

            for host in domains:
                try:
                    ctx = ssl.create_default_context()
                    ctx.check_hostname = True
                    ctx.verify_mode = ssl.CERT_REQUIRED
                    with socket.create_connection((host, 443), timeout=20) as sock:
                        with ctx.wrap_socket(sock, server_hostname=host) as ssock:
                            cert = ssock.getpeercert()
                    # notAfter is like "Sep 11 23:45:19 2025 GMT"
                    exp = datetime.strptime(cert["notAfter"], "%b %d %H:%M:%S %Y %Z").replace(tzinfo=timezone.utc)
                    days = (exp - datetime.now(timezone.utc)).days
                    print(f">>> {host}: cert expires {exp} (~{days} days)")
                    if days < 1:
                        print(f"::error ::{host} certificate expired")
                        bad = 1
                    elif days < warn_days:
                        print(f"::warning ::{host} certificate expires in {days} days")
                except (socket.gaierror, socket.timeout) as e:
                    print(f"::error ::{host} DNS/TCP error: {e}")
                    bad = 1
                except ssl.SSLError as e:
                    print(f"::error ::{host} TLS error: {e}")
                    bad = 1
                except Exception as e:
                    print(f"::error ::{host} unexpected error: {e}")
                    bad = 1

            # expose status to later steps (so alerts still fire)
            with open(os.environ["GITHUB_OUTPUT"], "a") as fh:
                fh.write(f"certexp_bad={bad}\n")

      - name: Determine overall status
        id: status
        run: |
          set -euo pipefail
          bad=$(( ${{ steps.curlcheck.outputs.curl_bad }} || ${{ steps.certexp.outputs.certexp_bad }} ))
          echo "overall_bad=$bad" >> "$GITHUB_OUTPUT"

      # NEW: single step that manages the alert Issue and exposes transition signals.
      - name: Track alert lifecycle (open/close issue & emit outputs)
        id: alerts
        if: always()
        uses: actions/github-script@v7
        env:
          OVERALL_BAD: ${{ steps.status.outputs.overall_bad }}
        with:
          script: |
            const bad = (process.env.OVERALL_BAD || '1') === '1';
            const { owner, repo } = context.repo;
            const label = "site-health";
            const runUrl = `${process.env.GITHUB_SERVER_URL}/${process.env.GITHUB_REPOSITORY}/actions/runs/${process.env.GITHUB_RUN_ID}`;

            let opened = 0, recovered = 0;

            const openIssues = await github.paginate(github.rest.issues.listForRepo, {
              owner, repo, state: "open", labels: label
            });

            if (bad) {
              if (openIssues.length === 0) {
                await github.rest.issues.create({
                  owner, repo,
                  title: "ðŸš¨ Site health failing",
                  body: `Automated health check is failing.\nRun: ${runUrl}`,
                  labels: [label]
                });
                opened = 1;
              }
            } else {
              for (const i of openIssues) {
                await github.rest.issues.createComment({
                  owner, repo, issue_number: i.number,
                  body: `âœ… Automated health check recovered.\nRun: ${runUrl}`
                });
                await github.rest.issues.update({ owner, repo, issue_number: i.number, state: "closed" });
                recovered = 1;
              }
            }

            core.setOutput('opened', String(opened));       // 1 only when first failing
            core.setOutput('recovered', String(recovered)); // 1 only when actually recovered

      # Failure notifications ONLY when we just opened an issue (first failure)
      - name: Send alert EMAIL via Gmail SMTP
        if: steps.alerts.outputs.opened == '1'
        shell: python
        env:
          GMAIL_USER:     ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASS: ${{ secrets.GMAIL_APP_PASS }}
          EMAIL_TO:       ${{ secrets.EMAIL_TO }}
          RUN_URL:        ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          import os, smtplib, ssl
          from email.message import EmailMessage
          user = os.environ["GMAIL_USER"]
          app  = os.environ["GMAIL_APP_PASS"]
          tos  = [x.strip() for x in os.environ["EMAIL_TO"].split(",") if x.strip()]
          subj = "ðŸš¨ Site health failing"
          body = f"Automated health check failed for jesse-anderson.net.\nRun: {os.environ['RUN_URL']}\n"
          msg = EmailMessage()
          msg["Subject"] = subj
          msg["From"] = user
          msg["To"] = ", ".join(tos)
          msg.set_content(body)
          with smtplib.SMTP("smtp.gmail.com", 587) as s:
            s.starttls(context=ssl.create_default_context())
            s.login(user, app)
            s.send_message(msg)

      - name: Send alert via Telegram
        if: steps.alerts.outputs.opened == '1'
        env:
          TG_TOKEN:  ${{ secrets.TG_BOT_TOKEN }}
          TG_CHAT:   ${{ secrets.TG_CHAT_ID }}
          RUN_URL:   ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          set -euo pipefail
          msg="ðŸš¨ Site health failing for jesse-anderson.net \nRun: ${RUN_URL}"
          curl -sS -X POST "https://api.telegram.org/bot${TG_TOKEN}/sendMessage" \
            -d "chat_id=${TG_CHAT}" \
            -d "text=${msg}"

      # Recovery notifications ONLY when we actually closed at least one open alert issue
      - name: Send recovery EMAIL via Gmail SMTP
        if: steps.alerts.outputs.recovered == '1'
        shell: python
        env:
          GMAIL_USER:     ${{ secrets.GMAIL_USER }}
          GMAIL_APP_PASS: ${{ secrets.GMAIL_APP_PASS }}
          EMAIL_TO:       ${{ secrets.EMAIL_TO }}
          RUN_URL:        ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          import os, smtplib, ssl
          from email.message import EmailMessage
          user = os.environ["GMAIL_USER"]
          app  = os.environ["GMAIL_APP_PASS"]
          tos  = [x.strip() for x in os.environ["EMAIL_TO"].split(",") if x.strip()]
          subj = "âœ… Site health recovered"
          body = f"All checks passed again for jesse-anderson.net.\nRun: {os.environ['RUN_URL']}\n"
          msg = EmailMessage()
          msg["Subject"] = subj
          msg["From"] = user
          msg["To"] = ", ".join(tos)
          msg.set_content(body)
          with smtplib.SMTP("smtp.gmail.com", 587) as s:
            s.starttls(context=ssl.create_default_context())
            s.login(user, app)
            s.send_message(msg)

      - name: Send recovery via Telegram
        if: steps.alerts.outputs.recovered == '1'
        env:
          TG_TOKEN:  ${{ secrets.TG_BOT_TOKEN }}
          TG_CHAT:   ${{ secrets.TG_CHAT_ID }}
          RUN_URL:   ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          set -euo pipefail
          msg="âœ… Site health recovered for jesse-anderson.net \nRun: ${RUN_URL}"
          curl -sS -X POST "https://api.telegram.org/bot${TG_TOKEN}/sendMessage" \
            -d "chat_id=${TG_CHAT}" \
            -d "text=${msg}"
