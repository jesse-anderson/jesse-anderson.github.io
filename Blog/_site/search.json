[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Jesse Anderson's Blog",
    "section": "",
    "text": "General readme.\n\n\nTo push a new blog or rather statically generate one.\n\n\ncd this folder in cmd\n\n\nquarto render"
  },
  {
    "objectID": "posts/VAE_GAN/VAEGAN.html",
    "href": "posts/VAE_GAN/VAEGAN.html",
    "title": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs",
    "section": "",
    "text": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\nDuring my time in the applied machine learning course(CS441) at the University of Illinois at Urbana-Champaign, I embarked on an ambitious journey into the realm of advanced machine learning technologies. The course required a comprehensive understanding of various machine learning concepts, providing both breadth and depth in our studies. For my final optional project, I chose to specialize in some of the most intriguing areas of generative models: Variational AutoEncoders (VAEs), Denoising AutoEncoders, Generative Adversarial Networks (GANs), and the innovative hybrid, Variational Autoencoder Generative Adversarial Networks (VAE-GANs). Here, I provide a high-level overview of each technology and discuss the outcomes of my experiments. For further information about the course please see sample syllabus.\n\nVariational AutoEncoders (VAEs)\nVAEs are powerful generative models that use the principles of probability and statistics to produce new data points that are similar to the training data. Unlike traditional autoencoders, which aim to compress and decompress data, VAEs introduce a probabilistic twist to encode input data into a distribution over latent space. This approach not only helps in generating new data but also improves the model’s robustness and the quality of generated samples. VAEs are particularly effective in tasks where you need a deep understanding of the data’s latent structure, such as in image generation and anomaly detection.\nThe encoder in a VAE is responsible for transforming high-dimensional input data into a lower-dimensional and more manageable representation. However, unlike standard autoencoders that directly encode data into a fixed point in latent space, the encoder in a VAE maps inputs into a distribution over the latent space. This distribution is typically parameterized by means (mu) and variances (sigma), which define a Gaussian probability distribution for each dimension in the latent space.\nThe latent space in VAEs is the core feature that distinguishes them from other types of autoencoders. It is a probabilistic space where each point is defined not just by coordinates, but by a distribution over possible values. This stochastic nature of the latent space allows VAEs to generate new data points by sampling from these distributions, providing a mechanism to capture and represent the underlying probabilistic properties of the data. It essentially acts as a compressed knowledge base of the data’s attributes.\nOnce a point in the latent space is sampled, the decoder part of the VAE takes over to map this probabilistic representation back to the original data space. The decoder learns to reconstruct the input data from its latent representation, aiming to minimize the difference between the original input and its reconstruction. This process is governed by a loss function that has two components: a reconstruction loss that measures how effectively the decoder reconstructs the input data from the latent space, and a regularization term that ensures the distribution characteristics in the latent space do not deviate significantly from a predefined prior (often a standard normal distribution).\nIn practice, the encoder’s output of means and variances provides a smooth and continuous latent space, which is crucial for generating new data points that are similar but not identical to the original data. This property makes VAEs particularly useful in tasks requiring a deep generative model, such as synthesizing new images that share characteristics with a training set, or identifying anomalies by seeing how well data points reconstruct using the learned distributions.\n\n\n\n\n    Attribution Example\n\n\n    By EugenioTL - Own work, CC BY-SA 4.0, Link\n\n\n\n\nDenoising AutoEncoders\nDenoising Autoencoders (DAEs) are specialized neural networks aimed at improving the quality of corrupted input data by learning to restore its original, uncorrupted state. This functionality is crucial in applications such as image restoration, where DAEs enhance image clarity by effectively removing noise. They achieve this through a training process that involves a dataset containing pairs of noisy and clean images. By continually adjusting through this training set, the DAE learns the underlying patterns necessary to filter out the distortions and recover the clean data. This ability to directly process and improve corrupted data makes DAEs valuable for various tasks beyond image restoration, including audio cleaning and improving data quality for analytical purposes.\n\nSource: https://blog.keras.io/building-autoencoders-in-keras.html\n\n\nGenerative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) utilize a unique framework involving two competing neural networks: a generator and a discriminator. These networks engage in an adversarial game, where the generator’s goal is to create synthetic data that is indistinguishable from real-world data, effectively “fooling” the discriminator. The discriminator’s job, on the other hand, is to distinguish between the authentic data and the synthetic creations of the generator.\nThis dynamic creates a feedback loop where the generator continually learns from the discriminator’s ability to detect fakes, driving it to improve its data generation. As the generator gets better, the discriminator’s task becomes more challenging, forcing it to improve its detection capabilities. Over time, this adversarial process leads to the generation of highly realistic and convincing data outputs.\nGANs have been particularly successful in the field of image generation, where they are used to create highly realistic images that are often indistinguishable from actual photographs. A prominent example is the ThisPersonDoesNotExistwebsite, which uses a model called StyleGAN2to generate lifelike images of human faces that do not correspond to real individuals. This technology has also been applied in other areas such as art creation, style transfer, and more. Eerie.\n\nSource: https://developers.google.com/machine-learning/gan/gan_structure\n\n\nVariational Autoencoder Generative Adversarial Networks (VAE-GANs)\nVAE-GANs are an innovative hybrid model that synergistically combines Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to enhance the quality and control of data generation. The model integrates the VAE’s capability for creating a compressed, latent representation of data with the GAN’s strength in generating high-fidelity outputs.\nIn a VAE-GAN system, the encoder part of the VAE compresses input data into a latent space (a condensed representation), which the GAN’s generator then uses to reconstruct outputs that are as realistic as possible. This setup leverages the VAE’s ability to manage and interpret the latent space effectively, providing a structured, meaningful input for the GAN’s generator. The discriminator in the GAN setup then evaluates these outputs against real data, guiding the generator to improve its outputs continually.\nThe fusion of these two models allows for a more controlled generation process, which can lead to higher quality outputs than what might be achieved by either model alone. This approach not only enhances the detail and realism of the generated data but also improves the model’s ability to learn diverse and complex data distributions, making VAE-GANs particularly useful in tasks that require a high level of detail and accuracy, such as in image generation and modification.\n\nSource: Larsen, Anders & Sønderby, Søren & Winther, Ole. (2015). Autoencoding beyond pixels using a learned similarity metric.\n\n\nProject Outcomes\nThe practical application of these models in my project yielded fascinating insights and results. For instance, the VAEs demonstrated an impressive ability to generate new images that closely resembled the original dataset, while the Denoising AutoEncoders more or less restored a significant portion of corrupted images to their original state. Similarly, the GANs produced images that were often indistinguishable from real ones, highlighting their potential in creating synthetic data for training other machine learning models without the need for extensive real-world data collection.\nThe VAE-GANs, however, were the highlight, combining the best aspects of their constituent models to generate supremely realistic and diverse outputs. While I am unable to share specific code snippets of the DAE/VAE due to copyright restrictions on the course content, the qualitative outcomes were highly encouraging and indicative of the powerful capabilities of hybrid generative models.\n\n\nResults\n\n\nDenoising AutoEncoder\n\nAs you can see in the image above, it does an ok job of denoising the middle image. The top image is the original image, the middle is the image with 50% noise, and the bottom is the model’s outputted denoised image. If I trained the model longer and varied up the training data I likely would have been able to get a better result. Additionally Principal Component Analysis and calculation of the Mean Residual Error was performed to determine how well the model works. See below:\n\nLoss over Epochs(Note the plots say reconstruction error when I really meant Mean Squared Error Loss):\n\nI decided to use a standard fixed learning rate here and only trained for 240 epochs.\n\n\nVariational AutoEncoder\nHere we were tasked with coming up with a VAE which would generate images and also be able to interpolate between images.\nSame Digits:\n\nDifferent Digits:\n\nOriginal vs. Reconstructed:\n\nAs you can see the VAE did a fairly good job of generating images.\nLoss over 400 Epochs:\n\nI was playing around with progressively reducing the learning rate as parameters changed(or didn’t) and thus reduced the learning rate progressively. This actually seemed to result in the exponentialLR type scheduler funnily enough. See here. The model was trained for 400 epochs. I likely won’t spin the DAE/VAE back up for videos as I did for the GAN and VAE-GAN.\n\n\nGenerative Adversarial Network\nI decided to go a bit further and try to get a Generative Adversarial Network running to generate new images of numbers. I went beyond the standard requirements of the course, but not too far as I didn’t want to “waste” too much time as there are newer technologies nowadays. Here’s the repo. There is a video below that shows the evolution of the training of the model. Additionally here’s the final result and video below:\n\n\n\n\n\nVideo Player\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\n\nLoad and Play Video\n\n\n\n\n\nLosses over 540 Epochs:\n\n\n\nVAE-GAN\nI finally went a bit further(probably too far) and decided to implement a VAE-GAN. There was a lot more balancing involved between the autoencoder portion and the generative portion and I was able to achieve a passable result, but definitely not worth the time and effort to balance parameters. It was strangely smoothed out, yet blurred where it mattered to generate the images.\nFinal result image and video below:\n\n\n\n\n\nVideo Player\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\n\nLoad and Play Video\n\n\n\n\n\nHere’s the associated Losses over 1000 Epochs(note my discriminator freaking out…):\n\n\n\nFinal Thoughts\nExploring these advanced generative models not only enhanced my understanding of the deep theoretical underpinnings of machine learning but also provided a practical toolkit for addressing complex real-world data generation and enhancement challenges. The knowledge and experience gained through this project are invaluable and have opened up numerous possibilities for further research and application in the field of artificial intelligence. I anticipate broadening my skillset in Generative AI here soon and will continue to skill up. I also got to experience the sheer tedium of “untangling” a neural network wherein something went wrong in my layers… repeatedly."
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html",
    "href": "posts/OPTICSinPython/OPTICS.html",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "href": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engineering/Science/Tech Blog",
    "section": "",
    "text": "OPTICS in Python\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\nML\n\n\nGenerative AI\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to OPTICS\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN Intro\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nDBSCAN\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I launched this blog in April of 2024 as part of a unique project aimed at creating a space for my thoughts and analyses, free from the constraints and subscriptions of platforms like Medium, and distinct from the algorithm-driven feeds of LinkedIn. While there’s certainly more content on the way, I hope this introduction serves as a more engaging placeholder than a simple “This is my blog.”"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "OPTICS in Python\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\nIntro to OPTICS\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\n\n\n\n\n\nDBSCAN Intro\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DBSCAN/index.html",
    "href": "posts/DBSCAN/index.html",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm notable for its proficiency in identifying clusters of varying shapes and sizes in large spatial datasets. This algorithm is especially useful in the field of spatiotemporal data analysis, where the goal is often to group similar data points that are in close proximity over time and space. In this blog post, we’ll delve into the mechanics of DBSCAN, discuss its critical parameters, and provide guidance on adjusting these parameters to achieve optimal clustering results for spatiotemporal data.\n\n\nDBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform.\n\n\n\nThe effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n Core, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here\n\n\n\n\nSpatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here\n\n\n\n\nScale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data.\n\n\n\n\nDBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/DBSCAN/index.html#what-is-dbscan",
    "href": "posts/DBSCAN/index.html#what-is-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform."
  },
  {
    "objectID": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "href": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "The effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n Core, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "href": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Spatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "href": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Scale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data."
  },
  {
    "objectID": "posts/DBSCAN/index.html#conclusion",
    "href": "posts/DBSCAN/index.html#conclusion",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html",
    "href": "posts/OPTICSWriteup/index.html",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Within the realm of unsupervised machine learning, clustering methods play a critical role in unraveling patterns and groupings within datasets where the labels are unknown. Among these clustering techniques, OPTICS(Ordering Points To Identify the Clustering Structure) stands out as a particularly powerful tool when dealing with complex datasets with varying densities or non-globular shapes. This exploration of OPTICS should get you up to speed on the basics of what OPTICS does and will help you understand its mechanics, benefits, and practical applications.\n\n\nDeveloped in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures.\n\n\n\nThe core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it.\n\n\n\n\n\n\nOPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management.\n\n\n\n\n\nTo determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets.\n\n\n\n\n\nXi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually.\n\n\n\n\nAs datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Developed in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "href": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "The core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "href": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "OPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "To determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Xi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "href": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "As datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my inaugural Quarto post! Though this marks my official entry into the Quarto blogging sphere, it isn’t my first dive into writing here. I initially crafted an insightful(?) post about the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a fascinating clustering algorithm that piqued my interest due to its rigorous yet intuitive approach to handling complex data sets.\nMy name is Jesse Anderson, and my academic and professional journey began in the world of chemical engineering, where I specialized in biochemical engineering and process automation. The intricate dance of chemical processes, moving systematically from one unit to another, serendipitously mirrored the logic and flow of computer science. It was this realization that bridged my transition from engineering physical processes to engineering digital ones.\nThis transition felt incredibly natural. In both domains, you’re essentially inputting a series of commands—whether to a machine in a lab or a line of code in a software program—and watching as complex processes unfold, often with a satisfying precision. This similarity is what drew me deeper into the world of computing, leading me to pursue a Master’s in Computer Science at the University of Illinois at Urbana-Champaign (UIUC). My undergraduate studies were completed at the University of Illinois at Chicago (UIC), where I earned a Bachelor of Science in Chemical Engineering.\nDuring my time at UIC, my fascination with optimization and automation took root. This passion wasn’t confined to coursework and theory. I actively engaged in developing research software, working closely with Dr. Ying Hu. My contributions during this period were not only fulfilling but also fruitful, culminating in several scholarly publications.\nOutside of academia, my hobbies closely mirror my professional interests, intertwining a love for optimization with a hands-on approach to problem-solving that spans far beyond algorithms and lab reactors. This passion for efficiency and process improvement extends into crafting tools and scripts that enhance productivity and automate the more mundane tasks of everyday life. But my knack for tool-making isn’t just confined to the digital realm; it’s a skill I’ve honed over many years, influenced heavily by my extensive background as an independent contractor.\nBefore venturing into the world of chemical engineering and computer science, I spent several years in the trades, tackling every aspect of plumbing, electrical work, HVAC, carpentry, and remodeling. This experience taught me the value of precision and strategic planning, skills that are just as applicable to programming as they are to physical construction and maintenance. Each project, whether installing a complex plumbing system or wiring a newly constructed home, sharpened my ability to think critically and adapt quickly—traits that have proven indispensable throughout my career.\nIn addition to my hands-on trade work, I managed the safety and project planning operations at G5 Environmental, a prominent street sweeping company based in Chicago. My role involved ensuring the smooth execution of projects, maintaining rigorous safety standards, and optimizing operations to enhance efficiency. This experience not only broadened my understanding of large-scale project management but also deepened my appreciation for the intricate dance between man, machine, and the environment. At G5 Environmental, we were committed to maintaining the cleanliness and safety of public spaces, a mission that echoes my current work’s focus on sustainability and environmental preservation through technology.\nMore recently, during my tenure at UL Solutions, I further developed my technical skills and applied them to real-world challenges:\n\nAutomation Software Development: I developed automation software in Python and VBA that successfully eliminated over 664+ hours on average of tedious engineering workflows annually.\nCAD/PDF Change Detection: I developed a few python scripts to automatically compare CAD files and compare long PDFs(like Acrobat Pro, but free and simpler).\nReal-Time Data Analysis for Testing: I implemented systems for real-time data analysis to support large-scale testing projects. This advancement has enabled more data-driven decision-making, optimizing testing procedures and outcomes through immediate feedback and analysis.\n\nWhile I massively enjoyed the automation projects I took on at UL, I was seeking broader challenges and opportunities for growth that UL could no longer provide. This realization led me to pursue a more fitting opportunity elsewhere, where I could further expand my expertise and impact in the fields of process automation and data-driven technology.\nThis diverse background has equipped me with a unique perspective that I bring to my current studies and research. Whether refining code to streamline data processing or rethinking a workflow to enhance laboratory operations, I am constantly learning and applying, pushing the boundaries of what I know about both chemical engineering and computer science. These experiences have molded me into a versatile professional capable of adapting to various roles and continuously refining my approach to both new and familiar challenges.\nAs I continue my studies at UIUC, my goal is to further explore how advanced computational techniques can be applied to solve complex problems in biochemical engineering and beyond. The integration of process automation into digital applications opens up exciting possibilities for innovation in both fields. Through this blog, I aim to share insights from my journey, discussing both the theoretical and practical aspects of my work and studies in hopes of connecting with fellow enthusiasts and professionals who share my zeal for optimization and automation.\nStay tuned for more posts where I’ll delve deeper into specific projects, challenges I’ve faced, and solutions I’ve discovered along the way. Thank you for joining me on this adventure in merging the systematic worlds of engineering and computing into one coherent, intertwined narrative."
  }
]