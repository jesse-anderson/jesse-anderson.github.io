[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Jesse Anderson's Blog",
    "section": "",
    "text": "General readme.\n\n\nTo push a new blog or rather statically generate one.\n\n\ncd this folder in cmd\n\n\nquarto render"
  },
  {
    "objectID": "posts/VAE_GAN/VAEGAN.html",
    "href": "posts/VAE_GAN/VAEGAN.html",
    "title": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs",
    "section": "",
    "text": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\nDuring my time in the applied machine learning course(CS441) at the University of Illinois at Urbana-Champaign, I embarked on an ambitious journey into the realm of advanced machine learning technologies. The course required a comprehensive understanding of various machine learning concepts, providing both breadth and depth in our studies. For my final optional project, I chose to specialize in some of the most intriguing areas of generative models: Variational AutoEncoders (VAEs), Denoising AutoEncoders, Generative Adversarial Networks (GANs), and the innovative hybrid, Variational Autoencoder Generative Adversarial Networks (VAE-GANs). Here, I provide a high-level overview of each technology and discuss the outcomes of my experiments. For further information about the course please see sample syllabus.\n\nVariational AutoEncoders (VAEs)\nVAEs are powerful generative models that use the principles of probability and statistics to produce new data points that are similar to the training data. Unlike traditional autoencoders, which aim to compress and decompress data, VAEs introduce a probabilistic twist to encode input data into a distribution over latent space. This approach not only helps in generating new data but also improves the model’s robustness and the quality of generated samples. VAEs are particularly effective in tasks where you need a deep understanding of the data’s latent structure, such as in image generation and anomaly detection.\nThe encoder in a VAE is responsible for transforming high-dimensional input data into a lower-dimensional and more manageable representation. However, unlike standard autoencoders that directly encode data into a fixed point in latent space, the encoder in a VAE maps inputs into a distribution over the latent space. This distribution is typically parameterized by means (mu) and variances (sigma), which define a Gaussian probability distribution for each dimension in the latent space.\nThe latent space in VAEs is the core feature that distinguishes them from other types of autoencoders. It is a probabilistic space where each point is defined not just by coordinates, but by a distribution over possible values. This stochastic nature of the latent space allows VAEs to generate new data points by sampling from these distributions, providing a mechanism to capture and represent the underlying probabilistic properties of the data. It essentially acts as a compressed knowledge base of the data’s attributes.\nOnce a point in the latent space is sampled, the decoder part of the VAE takes over to map this probabilistic representation back to the original data space. The decoder learns to reconstruct the input data from its latent representation, aiming to minimize the difference between the original input and its reconstruction. This process is governed by a loss function that has two components: a reconstruction loss that measures how effectively the decoder reconstructs the input data from the latent space, and a regularization term that ensures the distribution characteristics in the latent space do not deviate significantly from a predefined prior (often a standard normal distribution).\nIn practice, the encoder’s output of means and variances provides a smooth and continuous latent space, which is crucial for generating new data points that are similar but not identical to the original data. This property makes VAEs particularly useful in tasks requiring a deep generative model, such as synthesizing new images that share characteristics with a training set, or identifying anomalies by seeing how well data points reconstruct using the learned distributions.\n\n\n\n\n    Attribution Example\n\n\n    By EugenioTL - Own work, CC BY-SA 4.0, Link\n\n\n\n\nDenoising AutoEncoders\nDenoising Autoencoders (DAEs) are specialized neural networks aimed at improving the quality of corrupted input data by learning to restore its original, uncorrupted state. This functionality is crucial in applications such as image restoration, where DAEs enhance image clarity by effectively removing noise. They achieve this through a training process that involves a dataset containing pairs of noisy and clean images. By continually adjusting through this training set, the DAE learns the underlying patterns necessary to filter out the distortions and recover the clean data. This ability to directly process and improve corrupted data makes DAEs valuable for various tasks beyond image restoration, including audio cleaning and improving data quality for analytical purposes.\n\nSource: https://blog.keras.io/building-autoencoders-in-keras.html\n\n\nGenerative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) utilize a unique framework involving two competing neural networks: a generator and a discriminator. These networks engage in an adversarial game, where the generator’s goal is to create synthetic data that is indistinguishable from real-world data, effectively “fooling” the discriminator. The discriminator’s job, on the other hand, is to distinguish between the authentic data and the synthetic creations of the generator.\nThis dynamic creates a feedback loop where the generator continually learns from the discriminator’s ability to detect fakes, driving it to improve its data generation. As the generator gets better, the discriminator’s task becomes more challenging, forcing it to improve its detection capabilities. Over time, this adversarial process leads to the generation of highly realistic and convincing data outputs.\nGANs have been particularly successful in the field of image generation, where they are used to create highly realistic images that are often indistinguishable from actual photographs. A prominent example is the ThisPersonDoesNotExistwebsite, which uses a model called StyleGAN2to generate lifelike images of human faces that do not correspond to real individuals. This technology has also been applied in other areas such as art creation, style transfer, and more. Eerie.\n\nSource: https://developers.google.com/machine-learning/gan/gan_structure\n\n\nVariational Autoencoder Generative Adversarial Networks (VAE-GANs)\nVAE-GANs are an innovative hybrid model that synergistically combines Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to enhance the quality and control of data generation. The model integrates the VAE’s capability for creating a compressed, latent representation of data with the GAN’s strength in generating high-fidelity outputs.\nIn a VAE-GAN system, the encoder part of the VAE compresses input data into a latent space (a condensed representation), which the GAN’s generator then uses to reconstruct outputs that are as realistic as possible. This setup leverages the VAE’s ability to manage and interpret the latent space effectively, providing a structured, meaningful input for the GAN’s generator. The discriminator in the GAN setup then evaluates these outputs against real data, guiding the generator to improve its outputs continually.\nThe fusion of these two models allows for a more controlled generation process, which can lead to higher quality outputs than what might be achieved by either model alone. This approach not only enhances the detail and realism of the generated data but also improves the model’s ability to learn diverse and complex data distributions, making VAE-GANs particularly useful in tasks that require a high level of detail and accuracy, such as in image generation and modification.\n\nSource: Larsen, Anders & Sønderby, Søren & Winther, Ole. (2015). Autoencoding beyond pixels using a learned similarity metric.\n\n\nProject Outcomes\nThe practical application of these models in my project yielded fascinating insights and results. For instance, the VAEs demonstrated an impressive ability to generate new images that closely resembled the original dataset, while the Denoising AutoEncoders more or less restored a significant portion of corrupted images to their original state. Similarly, the GANs produced images that were often indistinguishable from real ones, highlighting their potential in creating synthetic data for training other machine learning models without the need for extensive real-world data collection.\nThe VAE-GANs, however, were the highlight, combining the best aspects of their constituent models to generate supremely realistic and diverse outputs. While I am unable to share specific code snippets of the DAE/VAE due to copyright restrictions on the course content, the qualitative outcomes were highly encouraging and indicative of the powerful capabilities of hybrid generative models.\n\n\nResults\n\n\nDenoising AutoEncoder\n\nAs you can see in the image above, it does an ok job of denoising the middle image. The top image is the original image, the middle is the image with 50% noise, and the bottom is the model’s outputted denoised image. If I trained the model longer and varied up the training data I likely would have been able to get a better result. Additionally Principal Component Analysis and calculation of the Mean Residual Error was performed to determine how well the model works. See below:\n\nLoss over Epochs(Note the plots say reconstruction error when I really meant Mean Squared Error Loss):\n\nI decided to use a standard fixed learning rate here and only trained for 240 epochs.\n\n\nVariational AutoEncoder\nHere we were tasked with coming up with a VAE which would generate images and also be able to interpolate between images.\nSame Digits:\n\nDifferent Digits:\n\nOriginal vs. Reconstructed:\n\nAs you can see the VAE did a fairly good job of generating images.\nLoss over 400 Epochs:\n\nI was playing around with progressively reducing the learning rate as parameters changed(or didn’t) and thus reduced the learning rate progressively. This actually seemed to result in the exponentialLR type scheduler funnily enough. See here. The model was trained for 400 epochs. I likely won’t spin the DAE/VAE back up for videos as I did for the GAN and VAE-GAN.\n\n\nGenerative Adversarial Network\nI decided to go a bit further and try to get a Generative Adversarial Network running to generate new images of numbers. I went beyond the standard requirements of the course, but not too far as I didn’t want to “waste” too much time as there are newer technologies nowadays. Here’s the repo. There is a video below that shows the evolution of the training of the model. Additionally here’s the final result and video below:\n\n\n\n\n\n\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\nLoad and Play Video\n\n\n\n\n\nLosses over 540 Epochs:\n\n\n\nVAE-GAN\nI finally went a bit further(probably too far) and decided to implement a VAE-GAN. There was a lot more balancing involved between the autoencoder portion and the generative portion and I was able to achieve a passable result, but definitely not worth the time and effort to balance parameters. It was strangely smoothed out, yet blurred where it mattered to generate the images.\nFinal result image and video below:\n\n\n\n\n\n\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\n\nLoad and Play Video\n\n\n\n\n\nHere’s the associated Losses over 1000 Epochs(note my discriminator freaking out…):\n\n\n\nFinal Thoughts\nExploring these advanced generative models not only enhanced my understanding of the deep theoretical underpinnings of machine learning but also provided a practical toolkit for addressing complex real-world data generation and enhancement challenges. The knowledge and experience gained through this project are invaluable and have opened up numerous possibilities for further research and application in the field of artificial intelligence. I anticipate broadening my skillset in Generative AI here soon and will continue to skill up. I also got to experience the sheer tedium of “untangling” a neural network wherein something went wrong in my layers… repeatedly."
  },
  {
    "objectID": "posts/Pi-Sensor-Proj-May-2024/index.html",
    "href": "posts/Pi-Sensor-Proj-May-2024/index.html",
    "title": "Raspberry Pi Sensor Server Project",
    "section": "",
    "text": "I finally decided to use the Raspberry Pi 4 Model B+ 8gb I had lying around to play around with some sensors. Luckily I took an electrical engineering course in circuits[ECE210 at UIC], which made it pretty straight forward to wire things up. I had also already flashed an OS to the SD card and only encountered a few issues with booting up with the pi in its case(with fan!) and the temperature/humidity sensor plugged in. Please note that setting up VNC Server(RPI) and VNC Viewer(Desktop) will speed this up dramatically. Below is a pretty simple mockup of the connection I used with the Fan’s Power on pin 4[+,5.0VDC] and pin 14[-] and the DHT11 sensor on Pin 2[+,5.0VDC], Pin 6[-], and Pin 7[GPIO7].\nImage source\nAnd here’s the setup:\nAs a side note, Fritzing worked really well to generate the image above and I used the build at: https://github.com/Move2win/Fritzing-0.9.9.64.pc-Compiled-Build\nIt is a compiled .exe on a random github repository and one should take care….. but it was definitely faster than trying to build Fritzing from source.\nNext I got a basic python script working on my Raspberry Pi after installing the Adafruit_DHT library.\nInstallation was pretty straight forward. Enter this into the command prompt\nNext:\nChange directories:\nNow:\nFinally….:\nNow create a .py file and enter the following:\nThis script will grab the current date, time, Percent Humidity, Temp in Fahrenheit/Celsius and display it to the user. Note that we are appending it to a buffer which will become important later.\nNext I tried various server/serverless options to get realtime data and decided on Vercel. I also tried out ThingSpeak and really liked its interface, but the fact that I would have to pay(if I wasn’t a student) made me consider other options. To implement a basic realtime logging of sensor data in ThingSpeak one would sign up for an account, create a channel, populate a channel and add field labels such as Temperature and Humidity, and save the channel to receive a unique Channel ID and API key. The code for thingspeak is pretty straightforward and one can implement the code below to populate a ThingSpeak channel.\nThe resulting channel is functional enough:\nLocation: https://thingspeak.com/channels/2545447\nRealistically, I may incorporate sending the data to ThingSpeak as well as the other option I chose for monitoring.\nI got my account up and running with Vercel, installing it on a private Github repo. I then installed Node.js and npm. Then I navigated to the repo and opened up a command prompt:\nI then created a ‘/api’ directory and created a file for my sensor data handling called ‘/api/sensor.js’. Note the addition of the API_KEY variable that you should add to your Vercel global environment variables to make sure there’s some added security.\nFrom here I pushed the changes to github which caused the Vercel site to redeploy and changed the code on the Raspberry Pi to the following:\nI will omit the fact that I spent forever trying to also get MongoDB to work within Vercel and later found out that I needed to perform some sort of installation to get it to work. I did however find out that Vercel offered a PostgreSQL implementation so I could store my data as it came in. I navigated to Storage and found it was a few pretty simple clicks to get it going.\nI created a table using:\nAnd extended sensor.js a bit…Namely I edited it so it can handle a data_buffer of multiple points as well as singular points to cut down on server connection overhead. I also added some logging for the sake of sanity on the off chance anything ever goes wrong.\nNote, you need to install pg on your github directory for PostgreSQL to work.\nFrom there what my final product looks like is an html page which displays the latest sensor readings, a javascript function to pull the entire dataset, a javascript function which pushes the data into the database, and the python script on the raspberry pi sending the data. I actually bunch up the data before I push it to save on the overhead costs of establishing a connection. Realistically the data isn’t too time sensitive and a window of 1-5 minutes is perfectly acceptable for readings. They are below:\nHtml:\nJavascript to pull entire dataset:\nJavascript to display latest sensor readings:\nPython script on the raspberry pi:\nI intend to add more sensors such as a VOC sensor, CO2 sensor, PM2.5/PM10 sensor to have real time air quality data. That should be plug and play and a few lines of code. Getting the raspberry pi, server, and database to get along is a lot more work than wiring up a few sensors. I will also likely throw the data into a mongoDB and also push the data to ThingSpeak regularly once I have figured out what the best storage medium is. Unfortunately server uptime is counted as compute time for the purpose of using PostgreSQL in Vercel, so its great for testing, but definitely won’t be my long term solution. I might just do the unhinged option and use Google Sheets as a database. It should be possible to have up to 10 million cells which, when coupled with the data being logged at Date, Time, Humidity, TempF, and TempC that means I can have roughly 2 million rows before I need to think of pushing to another sheet. With a safety factor of 2 I have 1 million and that means I have 1,000,000/60 = 16,666 seconds/60 = 277 hours/24 = 11.5 days of data before I need to consider using another sheet. I can likely shorten this to 7 days and dynamically generate a new sheet every week.\n**Edit**: I actually went ahead and tried the google sheets option. Created a Google Cloud Project, enabled the Google Sheets API, and created credentials/downloaded the resultant JSON.\nI also had to make sure some Google Python libraries were installed:\nNext I implemented a pretty basic program to send a few values to a Google Sheet:\nOnce I was assured that the Google Sheets API was functioning correctly I tweaked the existing Python code to also send data to a Google Sheet. The sheet was set to read only globally so I could later on access it via my github.io site or similar via javascript. I also added a *.txt file for persistence across runs where that sheet contains my spreadsheet_id, workbook_name, and sheet_name so I can start and stop the Python script whenever I wanted. I also added back in the ThingSpeak code from before with two additional parameters, date and time. That way I can bypass the 15second update limit of ThingSpeak by sending bulk data every 15 seconds and I can also use the date/time parameters to generate a plot on the off chance that the datasent uses the timestamp of receipt as the X axis when plotting. The final result is the plot below:\nThe Matlab code used:\nNow as far as the Python code goes, it is pretty lengthy at this point so buyer beware:\nThat’s 3 services down, with one more crack at MongoDB. The tentative plan is to push everything from the Python script then use some form of authentication to eventually be able to grab the data directly from MongoDB but use the Vercel site to “host” the csv/data. A rough sketch of the setup is below:\nThis was fairly straightforward and with one function and a couple of lines of Python I was all set up. Note that I changed the timings, notably Vercel. As the server spins up and stays active for 5 minutes after it receives data. The limit for Compute time is 60 hrs/month, so at the basic vCPU of 0.6 I will be at 0.6*(5minutes/60minutes)*24hour*30days=36 hours if I update it hourly. This is just safe enough for me to do and as a bonus its not like this is an AWS instance with my Credit Card linked.\nTimings:\nCurrently, sending to all 4 servers, I experience a max “drop” of the sensor data of 3 seconds and that is running a .py file with a desktop running in the background.\nTo set up the .py script to start up at boot I did the following:\nAnd within that file:\nThis sets it up to run at boot and also to log the output.\nEnable and start the service:\nCheck the status:\nI made a separate service file to make sure I can see the log by firing up my pi and leaving it be:\nAnd within that file:\nThis will rotate the log file daily, keep 7 rotations, compress the old logs, and restart the service after every log rotation.\nNext set up a script that checks for specific keywords including errors and sends a notification:\nMake it executable:\nAccess Cron to schedule jobs:\nSet the job to run * * * * *, or every minute of every hour of every day of every month.\nCheck that it worked:\nFrom here I also wanted to display the output at boot as well. This is optional, but I wanna see a running log of what’s going on:\nEnable on boot:\nDisabling the desktop[Boot into Command Line Interface]:\nFinally reboot and you’re done!\nAfter setting up the python script to run at bootup and disabling the desktop I experience a drop of “1-2” seconds with a larger amount of 1 second drops than 2.\nI must have made some sort of mistake in getting the logging to work, but the data is being sent and that’s good enough for now.\nRunning this command after the pi boots up works well enough:\nAnd don’t forget that if you want to connect via SSH(if you didn’t already)”\nIf you want to get the desktop back simply type into the Pi console:\nNow for the results:\nTo get the Google Script working I deployed a Google Aps Script:\nGo to script.google.com and create a new project\nWrite a function to fetch data from my Google Sheets:\nAnd then finally use the web app URL in my client-side code. Note that I tried fetching it manually, but got a:\nLatest sensor reading Google Sheets(button loads latest data):\nLatest Sensor Reading ThingSpeak(press button):\nConfiguring MongoDB to grab the sensor readings and the CSV required me to do quite a bit of configuring of Vercel(timeout increase) and making sure that the MongoDB was no longer using fixed IP addresses(I’m not implementing some cursed dynamic IP allocation). I also implement a 3 second wait just in case there’s a lot of requests/readers(unlikely).\nLatest Sensor Reading MongoDB(Read-Only User):\nLatest Sensor Reading Vercel PostgreSQL(Read-Only User):\nFinal Python Script on the off chance something changed:\n*Note you can run with this pretty easily, change the flags to True for whatever “database” you’re using.*\nAnd there you have it. 4 different databases and the ability to write/read to them in near real time via a simple web portal. I could have used any of these individually but I wanted to explore the strengths/weaknesses of different configurations. I like the idea of just using a pretty long Google Sheet for most projects and completely ignoring actual databases for simple home automation. Using MongoDB was pretty straightforward via VSCode and it is a strong second choice. Tying for second would be ThingSpeak with its cool online interface, but I’m wary of Mathworks in general with their pay to play scheme. Last place is definitely Vercel’s own implementation of PostgreSQL mostly because of the compute/size limitations.\nI would have implemented multiple sensors, likely in the form of PM2.5/10, CO2, a better DHT22 sensor, and a VOC sensor. Here’s my Amazon wish list for future projects:\nPM2.5/10[PMS5003]: https://www.adafruit.com/product/3686\nCO2[MH-Z19]: https://www.amazon.com/EC-Buying-Monitoring-Concentration-Detection/dp/B0CRKH5XVX/\nTemperature/Humidity[DHT22]: https://www.amazon.com/SHILLEHTEK-Digital-Temperature-Humidity-Sensor/dp/B0CN5PN225/\nTemp/Humidity/Pressure/VOC[BME680]: https://www.amazon.com/CJMCU-680-Temperature-Humidity-Ultra-Small-Development/dp/B07K1CGQTJ/\nAir Quality/VOC[MQ135]: https://www.amazon.com/Ximimark-Quality-Hazardous-Detection-Arduino/dp/B07L73VTTY/\nWhile a description of extract, transform, and load(ETL) probably belongs up top I feel like it fits the narrative better to define it here and describe how this project is one giant homegrown ETL pipeline…"
  },
  {
    "objectID": "posts/Pi-Sensor-Proj-May-2024/index.html#what-is-an-etl-pipeline",
    "href": "posts/Pi-Sensor-Proj-May-2024/index.html#what-is-an-etl-pipeline",
    "title": "Raspberry Pi Sensor Server Project",
    "section": "What is an ETL Pipeline?",
    "text": "What is an ETL Pipeline?\n\nDefinition\nETL stands for Extract, Transform, Load. It’s a process used in data management to:\n\nExtract data from various sources.\nTransform the data into a suitable format or structure for analysis and reporting.\nLoad the transformed data into a target database, data warehouse, or data lake.\n\n\n\nHow it Works\n\nExtract:\n\nData is collected from different sources like databases, APIs, files, or sensors.\nThis step involves connecting to the source, querying or retrieving the data, and pulling it into the pipeline.\n\nTransform:\n\nThe extracted data is cleaned, validated, and transformed to fit the schema of the target system.\nTransformations might include filtering, aggregating, joining, sorting, or converting data types.\nThis step ensures data quality and prepares it for efficient loading and querying in the target system.\n\nLoad:\n\nThe transformed data is loaded into the target database, data warehouse, or data lake.\nThis involves writing the data into the storage system, ensuring it’s available for querying and analysis.\nThe loading process can be a full load (overwriting existing data) or an incremental load (updating or adding new data).\n\n\n\n\nApplications\nETL pipelines are used in various scenarios, including:\n\nData Warehousing: Collecting and consolidating data from multiple sources into a central repository for reporting and analysis.\nBusiness Intelligence (BI): Providing clean and structured data for BI tools to generate insights and reports.\nData Integration: Integrating data from different systems to provide a unified view.\nData Migration: Moving data from one system to another, often during system upgrades or cloud migrations.\nData Analytics: Preparing data for analysis and machine learning models.\n\n\n\nExample of a Homegrown ETL Pipeline in the Project\nThe project described above is a great example of a homegrown ETL pipeline, using a Raspberry Pi and various data storage solutions.\n\nExtract\n\nData Sources: Environmental data is extracted from various sensors (e.g., DHT11 for temperature and humidity).\nData Collection: The sensors are connected to the Raspberry Pi, which reads data at regular intervals.\n\n\n\nTransform\n\nData Processing: The raw data from the sensors is processed to convert it into a readable format (e.g., converting temperature from Celsius to Fahrenheit).\nData Formatting: The data is formatted into JSON objects for a consistent structure across different storage solutions.\n\n\n\nLoad\n\nData Storage: The transformed data is loaded into multiple storage solutions:\n\nGoogle Sheets: For easy access and sharing.\nThingSpeak: For real-time visualization and monitoring.\nMongoDB: For flexible, document-based storage.\nVercel PostgreSQL: For relational database storage.\n\n\n\n\n\nHow the Project Implements ETL\n\nExtract:\n\nThe Raspberry Pi reads data from the DHT11 sensor every second using the Adafruit_DHT library.\n\nTransform:\n\nThe raw data (humidity and temperature) is processed to convert temperature to Fahrenheit and format the date and time.\nThe data is structured into JSON objects for consistency.\n\nLoad:\n\nThe data is sent to multiple endpoints:\n\nGoogle Sheets: Using the Google Sheets API to append new rows.\nThingSpeak: Using the ThingSpeak API for both simple and bulk updates.\nMongoDB: Using the MongoDB Python client to insert data into the collection.\nVercel PostgreSQL: Sending data to a Vercel serverless function which writes to a PostgreSQL database.\n\n\n\n\n\nBenefits of this ETL Pipeline\n\nReal-time Data Processing: The pipeline processes and loads data in near real-time, providing up-to-date information.\nMulti-Storage Integration: The data is stored in various platforms, each with its strengths, ensuring data availability and flexibility.\nScalability: The modular approach allows easy addition of new sensors or data sources.\nAutomation: The use of scheduled tasks and services ensures continuous data collection and processing without manual intervention.\n\n\n\nConclusion\nThis project shows off a homegrown ETL pipeline that efficiently collects, processes, and stores environmental data from sensors. By using various storage solutions, it demonstrates how a flexible and scalable ETL process can be implemented for real-time data monitoring and analysis in a home automation context."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html",
    "href": "posts/OPTICSWriteup/index.html",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Within the realm of unsupervised machine learning, clustering methods play a critical role in unraveling patterns and groupings within datasets where the labels are unknown. Among these clustering techniques, OPTICS(Ordering Points To Identify the Clustering Structure) stands out as a particularly powerful tool when dealing with complex datasets with varying densities or non-globular shapes. This exploration of OPTICS should get you up to speed on the basics of what OPTICS does and will help you understand its mechanics, benefits, and practical applications.\n\n\nDeveloped in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures.\n\n\n\nThe core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it.\n\n\n\n\n\n\nOPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management.\n\n\n\n\n\nTo determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets.\n\n\n\n\n\nXi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually.\n\n\n\n\nAs datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Developed in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "href": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "The core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "href": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "OPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "To determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Xi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "href": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "As datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/ESP32_Post_1/index.html",
    "href": "posts/ESP32_Post_1/index.html",
    "title": "Efficient Data Logging with ESP32",
    "section": "",
    "text": "To begin with, we are using an ESP32-Wroom-32 Expressif board purchased from Microcenter for $10. These boards can be found for as low as $4-$5 on Amazon, and more capable boards are available in the $10-$20 range. This makes the ESP32 a cost-effective alternative to the Raspberry Pi, especially when the task is primarily logging sensor data. Additionally, the ESP32 outperforms the Raspberry Pi Pico significantly; it is approximately five times faster with integer arithmetic and 60-70 times faster with floating-point calculations, as demonstrated in this youtube video: https://www.youtube.com/watch?v=zGog29YNLmk&ab_channel=Tomsvideos\nThe code for this project is written in MicroPython. Although rewriting it in C/C++ could potentially yield a 10x performance improvement, the time and effort required to handle compilation issues are not justifiable for this use case. If this were a production environment, opting for C/C++ would be an obvious choice."
  },
  {
    "objectID": "posts/ESP32_Post_1/index.html#what-is-an-esp32-wroom-32-expressif-board",
    "href": "posts/ESP32_Post_1/index.html#what-is-an-esp32-wroom-32-expressif-board",
    "title": "Efficient Data Logging with ESP32",
    "section": "What is an ESP32-WROOM-32 Expressif Board?",
    "text": "What is an ESP32-WROOM-32 Expressif Board?\nThe ESP32-WROOM-32 is a powerful, low-cost Wi-Fi and Bluetooth microcontroller module developed by Espressif Systems. It is designed for a wide range of applications, from simple IoT projects to complex systems requiring wireless connectivity and advanced processing capabilities. Here are some key features and specifications of the ESP32-WROOM-32 board:\n\nDual-Core Processor: The ESP32-WROOM-32 features a dual-core Tensilica Xtensa LX6 microprocessor, with clock speeds up to 240 MHz. This provides ample processing power for a variety of tasks, including real-time data processing and multitasking.\nWireless Connectivity:\n\nWi-Fi: The board supports 802.11 b/g/n Wi-Fi, making it ideal for IoT applications that require internet connectivity. It can operate in both Station and Access Point modes, allowing it to connect to existing networks or create its own.\nBluetooth: It includes Bluetooth 4.2 (BLE and Classic), enabling communication with other Bluetooth devices, such as sensors, smartphones, and peripherals.\n\nMemory:\n\nFlash Memory: The module typically comes with 4 MB of flash memory, used for storing the firmware and other data.\nSRAM: It has 520 KB of on-chip SRAM, providing sufficient space for running programs and handling data.\n\nGPIO and Peripherals:\n\nThe board features numerous General Purpose Input/Output (GPIO) pins, which can be used for interfacing with various sensors, actuators, and other peripherals.\nIt includes a variety of built-in peripherals, such as UART, SPI, I2C, PWM, ADC, and DAC, making it highly versatile for different types of projects.\n\nPower Management:\n\nThe ESP32-WROOM-32 is designed with power efficiency in mind, offering multiple power-saving modes, such as deep sleep and light sleep. This makes it suitable for battery-powered applications where low power consumption is crucial.\n\nDevelopment Environment:\n\nThe board is compatible with popular development environments like Arduino IDE, PlatformIO, and Espressif’s own ESP-IDF (Espressif IoT Development Framework). This flexibility allows developers to choose their preferred tools and programming languages.\n\n\nThe ESP32-WROOM-32 board is widely used in various applications, including Internet of Things (IoT), industrial automation, home automation, robotics, health monitoring, and educational projects. It enables the creation of smart home devices, environmental monitoring systems, wearable health trackers, and remote industrial monitoring solutions. Additionally, it is suitable for controlling autonomous robots and drones, developing smart appliances, and building voice assistants. Its versatility makes it an excellent choice for learning and prototyping in STEM education, providing a hands-on experience with microcontrollers, IoT, and embedded systems.\nHowever, the ESP32-WROOM-32 has some limitations that need to be considered. Its processing power and memory are limited compared to more powerful systems, which can be a constraint for complex applications. While it offers various power-saving modes, its power consumption is higher than simpler microcontrollers, making it less ideal for ultra-low-power applications. The board’s real-time performance may not meet the needs of highly time-sensitive tasks, and its limited GPIO pins might require additional hardware for larger projects. Additionally, it may not be suitable for extreme environmental conditions without protective measures, and its complexity can present a steep learning curve for beginners.\nThe datasheet for the ESP32 can be found here.\n\nCPU and Internal Memory\nESP32-D0WDQ6 contains two low-power Xtensa® 32-bit LX6 microprocessors. The internal memory includes:\n\n448 KB of ROM for booting and core functions.\n520 KB of on-chip SRAM for data and instructions.\n8 KB of SRAM in RTC, which is called RTC FAST Memory and can be used for data storage; it is accessed by the main CPU during RTC Boot from the Deep-sleep mode.\n8 KB of SRAM in RTC, which is called RTC SLOW Memory and can be accessed by the co-processor during the Deep-sleep mode.\n1 Kbit of eFuse: 256 bits are used for the system (MAC address and chip configuration) and the remaining 768 bits are reserved for customer applications, including flash-encryption and chip-ID.\n\nGet the esptool via pip:\npip install esptool\nSee usage:\nesptool\nNext find your port after plugging in your ESP32 device via USB:\nIn Windows at least its Device Manager -&gt; Ports(Com & LPT) and look for a device named USB-Serial CH340 or Silicon Labs CP210x USB to UART Bridge or similar. I had two devices in my ports so I noted the ports in use, unplugged the board, then plugged it back in to get the port. This of course didn’t work and I had to add the COM port manually…\nAdding COM ports manually.\n\nOpen Device Manager on your computer.\nClick on the Action option from menu bar.\nChoose Add legacy hardware from the menu to open the Add Hardware window.\nClick on the Next button to move on.\nCheck Install the hardware that I manually select from a list (Advanced) and press Next.\nSelect Ports (COM & LPT) from the given list and press the Next button.\nChoose Standard port types option or the manufacturer for the ports; then, click Next.\nClick on the Finish button to complete.\n\nYou’ll note a new COM port, in my case COM4 and that’s what you’ll need for the next step.\nFollow this guide if you’re not seeing things or some other nonsense: https://docs.espressif.com/projects/esp-idf/en/stable/esp32/get-started/establish-serial-connection.html\nIf you’re experiencing driver issues, this resource might help:\nhttps://www.silabs.com/developers/usb-to-uart-bridge-vcp-drivers?tab=overview\nI used the “with serial enumeration” file, and it worked well for me. The device was recognized and assigned to COM4, which I then used for my setup.\nAfter installing PuTTY, everything worked smoothly. Using both the drivers and PuTTY resolved my issues, reminding me to be more patient and consult the documentation before rushing. If you’ve already flashed something, follow the steps to reset while monitoring COM# on PuTTY. You should see the download mode activate. Reset the device by holding the Boot button and pressing the reset button, then holding Boot while flashing."
  },
  {
    "objectID": "posts/ESP32_Post_1/index.html#getting-started-with-micropython",
    "href": "posts/ESP32_Post_1/index.html#getting-started-with-micropython",
    "title": "Efficient Data Logging with ESP32",
    "section": "Getting Started with MicroPython",
    "text": "Getting Started with MicroPython\nFor the most part refer to the instructions at: https://docs.micropython.org/en/latest/esp32/tutorial/intro.html\nDownload firmware for your ESP32 board:\nhttps://micropython.org/download/#esp32\nSpecifically the Microcenter Inland WROOM Board:\nhttps://micropython.org/download/ESP32_GENERIC/\nEnsure your device is erased with:\nesptool - p COM4 erase_flash\nTake care to replace the ‘COM4’ with your port.\nNext flash MicroPython to the board:\nesptool.py --chip esp32 --port COM4 --baud 460800 write_flash -z 0x1000 esp32-20190125-v1.10.bin\nMake sure you replace the .bin file with the file you downloaded and ensure you’re in the correct directory.\nNext I got up and running with Thonny(https://thonny.org/). Its a very lightweight Python IDE that’s ESP32 friendly. Make sure you select your device in the lower right corner and you’ll be up and running.\n\nYou can run a simple print(“Hello World”) to ensure you’re communicating with the device in the shell.\nHere’s a schematic of my board setup, with swappable GPIO pins and an optional LED. I included an LED to provide a quick visual indicator that my board is running the code correctly during stress tests.\n\nAnd the Real Life Version:\n\nPinouts courteousy of https://github.com/natedogg2020/Inland_ESP32-Supplements\nTop:\n\nBottom:\n\nAll pin references are looking from the top and will be either referenced Top Left or Top Right.\n\nDHT11\nGround to Ground Line to Ground at Pin 19(Top Left)\nGPIO15 Power from Pin 4(Top Left)\nGPIO13 Signal from Pin 5(Top Left)\nGPIO Power from Pin 5(Top Right)(I know I should’ve kept it split, but I had to do some nonsense with the power after the wifi cut out due to the power demand)\n\n\nLED\nResistor: 22ohm(Also used 10, but 22 works better to not blind me)\nLED Anode to Ground to Ground Line to Ground at Pin 6(Top Right)\nLED Cathode to Resistor(22 ohm) to GPIO2 Power from Pin 5(Again, I know I should’ve kept it separate. I’m not an EE.)\nNext, you’ll want to test that your device can connect to the internet, read the sensor, and blink the LED. For now, we’ll skip the Google App Script/Vercel PostgreSQL/MongoDB/ThingSpeak integration, and you will encounter errors when transmitting data.\nI’ll provide the code below with explanations of the different parts. To save memory, especially with the Google Apps Script request, I used a lower-level form of POST than standard. This approach is necessary because the Google Response was overloading the memory. Adjust the timeouts according to your needs. Initially, disable the sensors and LED to ensure sufficient power for the wireless connection, then enable the sensors and allow a few seconds for them to register readings.\n\n\nCode\n\nimport network\nimport urequests as requests\nimport time as time_module\nfrom machine import Pin\nimport dht\nimport ntptime\nimport utime\nimport ujson as json\nimport gc\nfrom machine import freq\n\n# Wi-Fi credentials\nSSID = 'YOUR-WIFI-NAME'\nPASSWORD = 'YOUR-WIFI-PASSWORD'\n\n# Google Sheets settings\nSPREADSHEET_ID = 'YOUR-SPREADSHEET-ID'\nRANGE_NAME = 'Sheet1!A1:E1' #NOTE THAT THIS IS FOR DATE TIME HUMIDITY TEMP_F TEMP_C. WILL CHANGE COLUMN DESIGNATION IF YOU ADD/REMOVE DATA.\nSHEET_NAME = 'Sheet1'\nGOOGLE_URL = 'YOUR-GOOGLE-APP-SCRIPT'\n\n# ThingSpeak settings\nTHINGSPEAK_API_KEY = 'THINGSPEAK-API-KEY'\nTHINGSPEAK_URL = 'https://api.thingspeak.com/update'\nTHINGSPEAK_CHANNEL_ID = 0000000 # Replace with your ThingSpeak channel ID\nTHINGSPEAK_BULK_UPDATE_URL = 'https://api.thingspeak.com/channels/'+str(THINGSPEAK_CHANNEL_ID)+'/bulk_update.json'\n\n# MongoDB settings\nMONGODB_API_URL = 'YOUR_MONGO_DB_URL'\n#MONGODB_API_KEY = 'your_mongodb_api_key' #WE ARE NOT USING PYMONGO, DO NOT NEED.\nMONGODB__VERCEL_API_URL = 'YOUR-VERCEL-URL/api/sensorMongoDB'\n# Vercel settings\nVERCEL_API_URL = 'YOUR-VERCEL-URL/api/sensor'\nVERCEL_API_KEY = 'YOUR-VERCEL-API-KEY'\n\n# DHT11 sensor setup\nSENSOR_POWER_PIN = 13  # Change this to the pin connected to the power control of the sensor\nSENSOR_DATA_PIN = 15  # Change this to the pin connected to the data pin of the sensor\nLED_PIN = 2  # GPIO pin for the LED\n\n# Initialize the sensor power pin and LED pin\nsensor_power_pin = Pin(SENSOR_POWER_PIN, Pin.OUT)\nsensor_data_pin = Pin(SENSOR_DATA_PIN)\nled = Pin(LED_PIN, Pin.OUT)\n\n# Buffers to store data\ndata_buffer_vercel = []\ndata_buffer_mongodb = []\nthingspeak_buffer = []  # Buffer for ThingSpeak data\n\n# Control flags\nSEND_TO_VERCEL = True\nSEND_TO_GOOGLE_SHEETS = True\nSEND_TO_THINGSPEAK = True\nSEND_TO_MONGODB = True\n\ndef connect_wifi(ssid, password):\n    wlan = network.WLAN(network.STA_IF)\n    wlan.active(True)\n    wlan.connect(ssid, password)\n    while not wlan.isconnected():\n        time_module.sleep(1)\n        print(\"Connecting to WiFi...\")\n    print(\"Connected to WiFi\")\n    print(wlan.ifconfig())\n\n# Function to disable sensors\ndef disable_sensors():\n    sensor_power_pin.value(0)  # Turn off sensor by setting the power pin low\n\n# Function to enable sensors\ndef enable_sensors():\n    sensor_power_pin.value(1)  # Turn on sensor by setting the power pin high\n    time_module.sleep(2)  # Wait for the sensor to stabilize    \n# Function to get system status\ndef get_system_status(firstRun):\n    free_heap = gc.mem_free()\n    total_heap = gc.mem_alloc() + free_heap\n    free_heap_percent = (free_heap / total_heap) * 100\n    if firstRun == True:\n        print(f\"Total heap memory: {total_heap} bytes\")\n        # Additional information about the system\n        print(f\"Frequency: {freq()} Hz\")\n    print(f\"Free heap memory: {free_heap} bytes ({free_heap_percent:.2f}%)\")\n    \ndef get_time_chicago():\n    max_retries = 100\n    for attempt in range(max_retries):\n        try:\n            ntptime.settime()\n            current_time = utime.localtime()\n            break\n        except OSError as e:\n            print(f\"Failed to get NTP time, attempt {attempt + 1} of {max_retries}. Error: {e}\")\n            time_module.sleep(1)\n    else:\n        print(\"Could not get NTP time, proceeding without time synchronization.\")\n        return utime.localtime()\n\n    # Determine if it is daylight saving time (DST)\n    month = current_time[1]\n    day = current_time[2]\n    hour = current_time[3]\n    if (month &gt; 3 and month &lt; 11) or (month == 3 and day &gt;= 8 and hour &gt;= 2) or (month == 11 and day &lt; 1 and hour &lt; 2):\n        is_dst = True\n    else:\n        is_dst = False\n    \n    offset = -6 * 3600 if not is_dst else -5 * 3600\n    local_time = utime.mktime(current_time) + offset\n    return utime.localtime(local_time)\n\ndef read_sensor():\n    try:\n        led.off()\n        sensor = dht.DHT11(sensor_data_pin)\n        time_module.sleep(1) #the DHT11 sensor takes 1 second\n        sensor.measure()\n        led.on()\n        temp = sensor.temperature()\n        hum = sensor.humidity()\n        return temp, hum\n    except OSError as e:\n        print(\"Failed to read sensor. Exception: \", e)\n        return None, None\n# Read the access token from the file uploaded earlier\ndef read_access_token():\n    with open('access_token.txt', 'r') as token_file:\n        return token_file.read().strip()\n\n# ACCESS_TOKEN = read_access_token()\n# print(ACCESS_TOKEN)\n\nimport usocket as socket\nimport ssl\ndef send_data_to_google_sheets(temp_c, temp_f, humidity, time_str, date_str):\n#     print(time_module.time())\n    url = GOOGLE_URL  # Define your Google URL here\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    data = {\n        'Date': date_str,\n        'Time': time_str,\n        'Humidity %': humidity,\n        'Temp F': temp_f,\n        'Temp C': temp_c\n    }\n#     print(time_module.time())\n    # Construct the URL-encoded string manually\n    encoded_data = (\n        \"Date=\" + date_str +\n        \"&Time=\" + time_str +\n        \"&Humidity %=\" + str(humidity) +\n        \"&Temp F=\" + str(temp_f) +\n        \"&Temp C=\" + str(temp_c)\n     ) \n    try:\n        # Extract host and path from URL\n        _, _, host, path = url.split('/', 3)\n        \n        # Set up a socket connection\n        addr = socket.getaddrinfo(host, 443)[0][-1]\n        s = socket.socket()\n        s.connect(addr)\n        s = ssl.wrap_socket(s)\n        \n        # Create the HTTP request manually\n        request = f\"POST /{path} HTTP/1.1\\r\\nHost: {host}\\r\\n\"\n        request += \"Content-Type: application/x-www-form-urlencoded\\r\\n\"\n        request += f\"Content-Length: {len(encoded_data)}\\r\\n\\r\\n\"\n        request += encoded_data\n\n        # Send the request\n        s.write(request)\n        \n#         # Read the response\n#         response = s.read(1024)  # Read up to 2048 bytes from the response, THIS TAKES A WHILE, SET TO WHATEVER. 128-1024\n#         print('Data sent to Google Sheets:', response)  \n        # Close the socket\n        s.close()\n        print('Data sent to Google Sheets!')\n#         print(time_module.time())\n    except Exception as e:\n        print('Failed to send data to Google Sheets:', e)\n\n# # Function to send data to Google Sheets\n# def send_data_to_google_sheets(temp_c, temp_f, humidity,time_str,date_str):\n#     print(time_module.time())\n#     url = GOOGLE_URL\n#     headers = {\n#         'Content-Type': 'application/x-www-form-urlencoded'\n#     }\n#     # Construct the URL-encoded string manually\n#     print(time_module.time())\n#     encoded_data = (\n#         \"Date=\" + date_str +\n#         \"&Time=\" + time_str +\n#         \"&Humidity %=\" + str(humidity) +\n#         \"&Temp F=\" + str(temp_f) +\n#         \"&Temp C=\" + str(temp_c)\n#     )\n#     print(encoded_data)\n#     print(time_module.time())\n#     print(\"request\")\n#     try:\n#         # Get initial free memory\n#         # Run garbage collection to get a clean slate\n#         gc.collect()\n#         initial_free = gc.mem_free()\n#         response = requests.post(url, data=encoded_data, headers=headers) #DRAGS... Also\n#         # Run garbage collection again\n#         gc.collect()\n#         # Get final free memory\n#         final_free = gc.mem_free()\n# \n#         # Calculate memory used by the variable\n#         memory_used = initial_free - final_free\n#         print('Data sent to Google Sheets:')\n#         print(time_module.time())\n#         print('Size of response: ', memory_used, 'bytes') # Size of response:  46352 bytes. Crazy.... That's like ~4635 date strings.\n#     except Exception as e:\n#         print('Failed to send data to Google Sheets:', e)\n\ndef send_data_to_thingspeak():\n    \"\"\"Send data to ThingSpeak.\"\"\"\n    if SEND_TO_THINGSPEAK and thingspeak_buffer:\n        if len(thingspeak_buffer) &gt; 1:\n            # Bulk update\n            payload = {\n                    'write_api_key': THINGSPEAK_API_KEY,\n                    'updates': []\n                }\n            for data in thingspeak_buffer:\n                update = {\n                    'created_at': f\"{data['date']} {data['time']} -0500\",\n                    'field1': data['temperature_C'],\n                    'field2': data['temperature_F'],\n                    'field3': data['humidityPercent'],\n                    'field4': data['time'],\n                    'field5': data['date']\n                }\n                payload['updates'].append(update)\n\n            try:\n                # Send the bulk update request to ThingSpeak\n                headers = {'Content-Type': 'application/json'}\n                #print(len(thingspeak_buffer))\n                #print(headers)\n                #print(json.dumps(payload))\n                # Convert the data payload to JSON format\n                json_data = json.dumps(payload)\n                response = requests.post(THINGSPEAK_BULK_UPDATE_URL,headers=headers,data=json_data)\n                if response.status_code == 202:\n                    print('Data posted to ThingSpeak (bulk update):', response.text)\n                    thingspeak_buffer.clear()  # Clear the buffer after successful update\n                else:\n                    print(f'Failed to send data to ThingSpeak (bulk update): {response.status_code}, {response.text}')\n            except Exception as e:\n                print('Failed to send data to ThingSpeak (bulk update):', e)\n        else:\n            data = thingspeak_buffer.pop(0)  # Get the first item in the buffer\n            print(data)\n            payload = {\n            'api_key': THINGSPEAK_API_KEY,\n            'field1': data['temperature_C'],\n            'field2': data['temperature_F'],\n            'field3': data['humidityPercent'],\n            'field4': data['time'],\n            'field5': data['date']\n        }\n            try:\n                response = requests.post(THINGSPEAK_URL, json=payload)\n                print('Data posted to ThingSpeak', response.text)\n                print(payload)\n                thingspeak_buffer.clear()  # Clear the buffer after successful update\n            except Exception as e:\n                print('Failed to send data to ThingSpeak:', e)\n                thingspeak_buffer.clear()\ndef send_data_to_mongodb():\n    url = MONGODB__VERCEL_API_URL\n    headers = {\n        'x-api-key': VERCEL_API_KEY,\n        'Content-Type': 'application/json'\n    }\n\n    try:\n        # Convert the data dictionary to a JSON string\n        json_data = json.dumps(data_buffer_mongodb)\n        \n        # Print the request details for debugging\n#         print(\"Sending data to:\", url)\n#         print(\"Headers:\", headers)\n#         print(\"Payload:\", json_data)\n        \n        response = requests.post(url, data=json_data, headers=headers)\n        \n        # Print the response details for debugging\n        print(\"Status Code:\", response.status_code)\n        print(\"Response Text:\", response.text)\n        data_buffer_mongodb.clear()\n    except Exception as e:\n        print(\"Failed to send data to Vercel MongoDB API:\", e)\n        data_buffer_mongodb.clear()\ndef send_data_to_vercel():\n    url = VERCEL_API_URL\n    headers = {\n        'x-api-key': VERCEL_API_KEY,\n        'Content-Type': 'application/json'\n    }\n\n    try:\n        # Convert the data dictionary to a JSON string\n        json_data = json.dumps(data_buffer_vercel)\n        \n        # Print the request details for debugging\n#         print(\"Sending data to:\", url)\n#         print(\"Headers:\", headers)\n#         print(\"Payload:\", json_data)\n        \n        response = requests.post(url, data=json_data, headers=headers)\n        \n        # Print the response details for debugging\n        print(\"Status Code:\", response.status_code)\n        print(\"Response Text:\", response.text)\n    except Exception as e:\n        print(\"Failed to send data to Vercel:\", e)\n\ndef main():\n    firstRun = True\n    enable_sensors()\n    temp,hum=read_sensor()\n    print(temp,hum)\n    led.off()\n    disable_sensors()\n    connect_wifi(SSID, PASSWORD)\n    enable_sensors()\n    last_google_sheets_update = time_module.time()\n    last_thingspeak_update = time_module.time()\n    last_vercel_update = time_module.time()\n    last_mongodb_update = time_module.time()\n    iter = 0\n    while True:\n        try:\n            led.on()\n            enable_sensors() #enable the sensors via GPIO\n            temp_c, humidity = read_sensor() #log readings\n            led.off()\n            disable_sensors()# disable the sensors so wifi transmission doesn't run into power issues.\n            if temp_c is not None and humidity is not None:\n                temp_f = temp_c * 9 / 5 + 32\n                local_time = get_time_chicago()\n                date_str = f\"{local_time[0]}-{local_time[1]:02d}-{local_time[2]:02d}\"\n                time_str = f\"{local_time[3]:02d}:{local_time[4]:02d}:{local_time[5]:02d}\"\n                print(f'[{iter}]Date: {date_str}, Time: {time_str}, Temperature: {temp_c}°C, Humidity: {hum}%, Temperature: {temp_f}°F ')\n    #             print(date_str)\n    #             print(time_str)\n                # Add data to buffers\n                data = {\n                        'date': date_str,\n                        'time': time_str,\n                        'humidityPercent': humidity,\n                        'temperatureFahrenheit': temp_f,\n                        'temperatureCelsius': temp_c\n                    }\n                data_buffer_vercel={\n                    'date': date_str,\n                    'time': time_str,\n                    'humidityPercent': humidity,\n                    'temperatureFahrenheit': temp_f,\n                    'temperatureCelsius': temp_c\n                }\n                thingspeak_buffer.append({\n                    'temperature_C': temp_c,\n                    'temperature_F': temp_f,\n                    'humidityPercent': humidity,\n                    'time': time_str,\n                    'date': date_str\n                })\n                data_buffer_mongodb.append({\n                    'date': date_str,\n                    'time': time_str,\n                    'humidityPercent': humidity,\n                    'temperatureFahrenheit': temp_f,\n                    'temperatureCelsius': temp,\n                })\n                print(\"ThingSpeak Buffer:\",len(thingspeak_buffer),\"|Vercel Buffer:\",len(data_buffer_vercel),\"|MongoDB Buffer:\",len(data_buffer_mongodb))\n                 # Check if it's time to send data to Google Sheets \n                if time_module.time() - last_google_sheets_update &gt;= 5:\n                    \n                    send_data_to_google_sheets(temp_c, temp_f, humidity, time_str, date_str)\n                    last_google_sheets_update = time_module.time()\n\n                # Check if it's time to send data to ThingSpeak\n                if time_module.time() - last_thingspeak_update &gt;= 15:\n                    send_data_to_thingspeak()\n                    last_thingspeak_update = time_module.time()\n\n    #             # Check if it's time to send data to Vercel\n    #Vercel DB no good for this low level stuff. Overflow error and out of memory.\n    #No append operation as a result.\n                if time_module.time() - last_vercel_update &gt;= 3600:\n                    send_data_to_vercel()\n                    last_vercel_update = time_module.time()\n\n                # Check if it's time to send data to MongoDB\n                if time_module.time() - last_mongodb_update &gt;= 15:\n                    send_data_to_mongodb()\n                    last_mongodb_update = time_module.time()\n                led.on()    \n#                 time_module.sleep(1)  # Wait for 1 seconds before logging the next reading. Note sensor sampling times!\n                led.off()\n                get_system_status(firstRun)\n                print(\" \")\n                firstRun = False\n                iter = iter+1\n        except Exception as e:\n            print(f\"Error in main loop: {e}\") # usually some one off memory error. It'll reset while still connected to wifi and everyone will be happy.\n            \nif __name__ == '__main__':\n    main()\n\nAnd a breakdown:\n\n\nCode breakdown\n\n\nImport Necessary Libraries\nFirst, we import the necessary libraries required for the project:\n\nnetwork for managing Wi-Fi connectivity.\nurequests for making HTTP requests to various APIs.\ntime and utime for handling time-related functions.\ndht for interacting with the DHT11 sensor.\nntptime for synchronizing time with an NTP server.\nujson for handling JSON data.\ngc for garbage collection to manage memory.\nmachine for controlling hardware components like GPIO pins.\n\n\n\nWi-Fi and API Credentials\nWe define constants to store Wi-Fi credentials and API details:\n\nSSID and PASSWORD for Wi-Fi network credentials.\nSPREADSHEET_ID, RANGE_NAME, SHEET_NAME, and GOOGLE_URL for Google Sheets integration.\nTHINGSPEAK_API_KEY, THINGSPEAK_URL, THINGSPEAK_CHANNEL_ID, and THINGSPEAK_BULK_UPDATE_URL for ThingSpeak integration.\nMONGODB_API_URL, MONGODB_VERCEL_API_URL for MongoDB integration.\nVERCEL_API_URL and VERCEL_API_KEY for Vercel integration.\n\n\n\nSetting Up GPIO Pins\nWe configure the GPIO pins on the ESP32:\n\nSENSOR_POWER_PIN to control the power to the DHT11 sensor.\nSENSOR_DATA_PIN to read data from the DHT11 sensor.\nLED_PIN to control an LED used for indicating status.\n\n\n\nData Buffers and Control Flags\nBuffers are initialized to temporarily store data before sending it to the respective services:\n\ndata_buffer_vercel, data_buffer_mongodb, and thingspeak_buffer store data for Vercel, MongoDB, and ThingSpeak, respectively.\n\nControl flags (SEND_TO_VERCEL, SEND_TO_GOOGLE_SHEETS, SEND_TO_THINGSPEAK, SEND_TO_MONGODB) determine whether data should be sent to each service.\n\n\nConnecting to Wi-Fi\nThe connect_wifi function manages the connection to the Wi-Fi network:\n\nActivates the WLAN interface.\nConnects to the specified Wi-Fi network using the provided SSID and password.\nContinuously checks the connection status and prints the IP configuration once connected.\n\n\n\nSensor Control Functions\nTwo functions manage the power state of the DHT11 sensor:\n\ndisable_sensors sets the power pin low to turn off the sensor.\nenable_sensors sets the power pin high and waits for the sensor to stabilize.\n\n\n\nSystem Status Function\nThe get_system_status function provides insights into the system’s memory usage and CPU frequency:\n\nCalculates the total and free heap memory.\nPrints the memory statistics and CPU frequency.\n\n\n\nTime Synchronization\nThe get_time_chicago function synchronizes the ESP32’s clock with an NTP server:\n\nAttempts to set the time using NTP up to a maximum number of retries.\nAdjusts the time based on whether daylight saving time (DST) is in effect for the Chicago timezone.\n\n\n\nReading Sensor Data\nThe read_sensor function reads temperature and humidity data from the DHT11 sensor:\n\nMeasures the temperature and humidity.\nReturns the values or None if the reading fails.\n\n\n\nSending Data to Google Sheets\nThe send_data_to_google_sheets function sends sensor data to Google Sheets:\n\nConstructs the data payload and URL-encodes it.\nSends the data using an HTTP POST request.\nHandles errors during the data sending process.\n\n\n\nSending Data to ThingSpeak\nThe send_data_to_thingspeak function sends data to ThingSpeak:\n\nSupports both single data point updates and bulk updates.\nConstructs the payload and sends it using an HTTP POST request.\nHandles errors and clears the buffer after successful updates.\n\n\n\nSending Data to MongoDB via Vercel API\nThe send_data_to_mongodb function sends data to a MongoDB instance via a Vercel API:\n\nConverts the data buffer to JSON.\nSends the data using an HTTP POST request.\nHandles errors and clears the buffer after successful updates.\n\n\n\nSending Data to Vercel API\nThe send_data_to_vercel function sends data to a Vercel API endpoint:\n\nConverts the data buffer to JSON.\nSends the data using an HTTP POST request.\nHandles errors during the data sending process(NO BUFFER DUE TO MEMORY LIMITATIONS AND VERCEL LIMITS).\n\n\n\nMain Function\nThe main function orchestrates the entire process:\n\nInitializes the sensor and connects to Wi-Fi.\nEnters an infinite loop where it periodically reads sensor data, stores it in buffers, and sends it to the configured services.\nControls the LED to indicate the status of operations.\nManages the timing of data sending to ensure that each service receives data at the specified intervals.\nLogs system status and handles errors in the main loop.\nUpon encountering an error, most likely memory related, begins the loop again.\n\n\nTest the code and verify that your circuit is functioning correctly. After that, configure Vercel or another API endpoint and Google App Script. Configuration details for Vercel can be found at: https://jesse-anderson.github.io/Blog/_site/posts/Pi-Sensor-Proj-May-2024/, so I’ll skip that part. Create the JavaScript file(sensorMongoDB.js) below and place it in your /api folder. Ensure all changes are pushed to GitHub.\n\n\nCode\n\nconst { MongoClient } = require('mongodb');\n\nconst API_KEY = process.env.API_KEY; // Retrieve the API key from environment variables\nconst MONGO_URI = process.env.MONGODB_URI; // MongoDB connection string from environment variables\n\nconst MONGODB_DB_NAME = 'Raspberry_Pi'; // Database name\nconst MONGODB_COLLECTION_NAME = 'Readings'; // Collection name\n\nlet client;\n\nconst connectToMongo = async () =&gt; {\n    if (!client) {\n        client = new MongoClient(MONGO_URI, {\n            useNewUrlParser: true,\n            useUnifiedTopology: true,\n        });\n        await client.connect();\n    }\n    return client.db(MONGODB_DB_NAME);\n};\n\nconst handleSensorData = async (req, res) =&gt; {\n    if (req.method !== 'POST') {\n        return res.status(405).json({ error: 'Method not allowed' });\n    }\n\n    try {\n        console.log('Request received');  // For debugging purposes\n\n        // Extract API key from request headers\n        const providedApiKey = req.headers['x-api-key'];\n        console.log('Provided API Key:', providedApiKey);  // For debugging purposes\n\n        // Check if API key is provided and matches the expected API key\n        if (!providedApiKey || providedApiKey !== API_KEY) {\n            return res.status(401).json({ error: 'Unauthorized' });\n        }\n\n        // Extract data from request body\n        const data = req.body;\n\n        // Log the data received to console for verification\n        console.log('Received data:', JSON.stringify(data, null, 2));\n\n        const db = await connectToMongo();\n        const collection = db.collection(MONGODB_COLLECTION_NAME);\n\n        let result;\n\n        if (Array.isArray(data)) {\n            // Insert multiple readings\n            result = await collection.insertMany(data.map(entry =&gt; ({\n                ...entry,\n                temperatureCelsius: entry.temperatureCelsius !== null && entry.temperatureCelsius !== undefined ? entry.temperatureCelsius : 0,\n                temperatureFahrenheit: entry.temperatureFahrenheit !== null && entry.temperatureFahrenheit !== undefined ? entry.temperatureFahrenheit : 0,\n                humidityPercent: entry.humidityPercent !== null && entry.humidityPercent !== undefined ? entry.humidityPercent : 0,\n                date: entry.date !== null && entry.date !== undefined ? entry.date : new Date().toISOString().split('T')[0],\n                time: entry.time !== null && entry.time !== undefined ? entry.time : new Date().toISOString().split('T')[1].split('.')[0]\n            })));\n        } else {\n            // Insert a single reading\n            result = await collection.insertOne({\n                temperatureCelsius: data.temperatureCelsius !== null && data.temperatureCelsius !== undefined ? data.temperatureCelsius : 0,\n                temperatureFahrenheit: data.temperatureFahrenheit !== null && data.temperatureFahrenheit !== undefined ? data.temperatureFahrenheit : 0,\n                humidityPercent: data.humidityPercent !== null && data.humidityPercent !== undefined ? data.humidityPercent : 0,\n                date: data.date !== null && data.date !== undefined ? data.date : new Date().toISOString().split('T')[0],\n                time: data.time !== null && data.time !== undefined ? data.time : new Date().toISOString().split('T')[1].split('.')[0]\n            });\n        }\n\n        console.log('Data stored in MongoDB:', result);\n\n        // Send a successful response back to the client\n        res.status(200).json({ message: 'Data received and stored successfully!', data: result });\n    } catch (e) {\n        // Handle errors and send an error response\n        console.error(\"Error connecting to MongoDB or inserting data:\", e);\n        res.status(500).json({ error: 'Failed to connect to database or insert data', details: e.message });\n    }\n};\n\n// Export the function for Vercel\nmodule.exports = handleSensorData;\n\nI’ll omit breaking down the code as it is similar enough to the code described in the earlier post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engineering/Science/Tech Blog",
    "section": "",
    "text": "ESP32 Project: Sensor Reliability/Power Efficiency\n\n\nBatteries/BuckBoost/OLEDs\n\n\n\nESP32\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Data Logging with ESP32\n\n\nA Guide to Using Google Sheets/MongoDB/PostgreSQL/ThingSpeak for IoT\n\n\n\nESP32\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nRaspberry Pi Sensor Server Project Part 2\n\n\n\n\n\n\nRaspberry Pi\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nRaspberry Pi Sensor Server Project\n\n\n\n\n\n\nRaspberry Pi\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nOPTICS in Python\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\nML\n\n\nGenerative AI\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to OPTICS\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN Intro\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nDBSCAN\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I launched this blog in April of 2024 as part of a unique project aimed at creating a space for my thoughts and analyses, free from the constraints and subscriptions of platforms like Medium, and distinct from the algorithm-driven feeds of LinkedIn. While there’s certainly more content on the way, I hope this introduction serves as a more engaging placeholder than a simple “This is my blog.”"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nESP32 Project: Sensor Reliability/Power Efficiency\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nEfficient Data Logging with ESP32\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nRaspberry Pi Sensor Server Project Part 2\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nRaspberry Pi Sensor Server Project\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nOPTICS in Python\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nIntro to OPTICS\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nDBSCAN Intro\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nWelcome To My Blog\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DBSCAN/index.html",
    "href": "posts/DBSCAN/index.html",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm notable for its proficiency in identifying clusters of varying shapes and sizes in large spatial datasets. This algorithm is especially useful in the field of spatiotemporal data analysis, where the goal is often to group similar data points that are in close proximity over time and space. In this blog post, we’ll delve into the mechanics of DBSCAN, discuss its critical parameters, and provide guidance on adjusting these parameters to achieve optimal clustering results for spatiotemporal data.\n\n\nDBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform.\n\n\n\nThe effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n\nCore, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here\n\n\n\n\nSpatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here\n\n\n\n\nScale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data.\n\n\n\n\nDBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/DBSCAN/index.html#what-is-dbscan",
    "href": "posts/DBSCAN/index.html#what-is-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform."
  },
  {
    "objectID": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "href": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "The effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n\nCore, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "href": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Spatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "href": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Scale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data."
  },
  {
    "objectID": "posts/DBSCAN/index.html#conclusion",
    "href": "posts/DBSCAN/index.html#conclusion",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html",
    "href": "posts/OPTICSinPython/OPTICS.html",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "href": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "posts/Pi-Sensor-Part-2/index.html",
    "href": "posts/Pi-Sensor-Part-2/index.html",
    "title": "Raspberry Pi Sensor Server Project Part 2",
    "section": "",
    "text": "After finally getting everything to work on the database side of things I wanted to explore dashboarding and being able to quickly pull up a visual showing what the current values are for the sensor data. After spending a few hours with Grafana and discovering that it doesn’t play nicely enough with JSON for my tastes(but pretty nicely with SQL type DBs…) I decided to just code up a pretty simple dashboard which shows the last four or so odd hours of sensor data. Its fairly simple using Chart.js and pulling the JSON data from ThingSpeak(last 6,000 entries @ 2s avg sampling = 200 min = 3 hours 20 min). I’m currently “pressure testing” the raspberry pi 4 I have and trying to ensure I have at least a week of uptime before I move to an ESP32 based solution which should let me get the rate of sampling up. The visual is below and I’ll explain the code afterwards:\n\n\n\n\n    \n    \n    Dynamic Plot\n    \n    \n    \n\n\n    Pi Environment Test Data\n    Load Chart\n    \n    Refresh\n    \n    \n    \n\n\n\n\n\nCode\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Dynamic Plot&lt;/title&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/chart.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns\"&gt;&lt;/script&gt;\n    &lt;style&gt;\n        table {\n            width: 100%;\n            border-collapse: collapse;\n            margin-top: 20px;\n        }\n        th, td {\n            border: 1px solid #ddd;\n            padding: 8px;\n            text-align: center;\n        }\n        th {\n            background-color: #f2f2f2;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h2&gt;Pi Environment Test Data&lt;/h2&gt;\n    &lt;button onclick=\"loadChart()\"&gt;Load Chart&lt;/button&gt;\n    &lt;button onclick=\"refreshData()\"&gt;Refresh&lt;/button&gt;\n    &lt;canvas id=\"myChart\" width=\"400\" height=\"200\"&gt;&lt;/canvas&gt;\n    &lt;div id=\"latestValues\"&gt;&lt;/div&gt;\n    &lt;script&gt;\n        async function fetchData() {\n            const response = await fetch('https://api.thingspeak.com/channels/2545447/feeds.json?results=6000');\n            const data = await response.json();\n            return data.feeds;\n        }\n\n        function processData(feeds) {\n            const labels = feeds.map(feed =&gt; new Date(feed.created_at));\n            const tempC = feeds.map(feed =&gt; parseFloat(feed.field1));\n            const tempF = feeds.map(feed =&gt; parseFloat(feed.field2));\n            const humidity = feeds.map(feed =&gt; parseFloat(feed.field3));\n            return { labels, tempC, tempF, humidity };\n        }\n\n        function displayLatestValues(labels, tempC, tempF, humidity) {\n            const latestTime = labels[labels.length - 1];\n            const latestTempC = tempC[tempC.length - 1];\n            const latestTempF = tempF[tempF.length - 1];\n            const latestHumidity = humidity[humidity.length - 1];\n\n            const tableHTML = `\n                &lt;table&gt;\n    &lt;tr&gt;\n        &lt;th&gt;Time&lt;/th&gt;\n        &lt;th&gt;Temperature C&lt;/th&gt;\n        &lt;th&gt;Temperature F&lt;/th&gt;\n        &lt;th&gt;Humidity&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;${latestTime.toLocaleDateString('en-US')} ${latestTime.toLocaleTimeString('en-US')}&lt;/td&gt;\n        &lt;td&gt;${latestTempC.toFixed(2)}&lt;/td&gt;\n        &lt;td&gt;${latestTempF.toFixed(2)}&lt;/td&gt;\n        &lt;td&gt;${latestHumidity.toFixed(2)}&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n            `;\n\n            document.getElementById('latestValues').innerHTML = tableHTML;\n        }\n\n        async function createChart() {\n            const feeds = await fetchData();\n            const { labels, tempC, tempF, humidity } = processData(feeds);\n\n            const ctx = document.getElementById('myChart').getContext('2d');\n            const myChart = new Chart(ctx, {\n                type: 'line',\n                data: {\n                    labels: labels,\n                    datasets: [\n                        {\n                            label: 'Temperature C',\n                            data: tempC,\n                            borderColor: 'rgba(255, 99, 132, 1)',\n                            backgroundColor: 'rgba(255, 99, 132, 0.2)',\n                            borderWidth: 1,\n                            fill: true\n                        },\n                        {\n                            label: 'Temperature F',\n                            data: tempF,\n                            borderColor: 'rgba(255, 165, 0, 1)',\n                            backgroundColor: 'rgba(255, 165, 0, 0.2)',\n                            borderWidth: 1,\n                            fill: true\n                        },\n                        {\n                            label: 'Humidity',\n                            data: humidity,\n                            borderColor: 'rgba(54, 162, 235, 1)',\n                            backgroundColor: 'rgba(54, 162, 235, 0.2)',\n                            borderWidth: 1,\n                            fill: true\n                        }\n                    ]\n                },\n                options: {\n                    scales: {\n                        x: {\n                            type: 'time',\n                            time: {\n                                unit: 'hour'\n                            }\n                        },\n                        y: {\n                            beginAtZero: true\n                        }\n                    }\n                }\n            });\n\n            displayLatestValues(labels, tempC, tempF, humidity);\n\n            return myChart;\n        }\n\n        let chartInstance;\n\n        async function refreshData() {\n            if (chartInstance) {\n                chartInstance.destroy();\n            }\n            chartInstance = await createChart();\n        }\n\n        async function loadChart() {\n            if (!chartInstance) {\n                chartInstance = await createChart();\n            }\n        }\n\n        window.onload = async () =&gt; {\n            chartInstance = await createChart();\n        };\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nBreaking down the code:\n\n\nCode Summary\n\n\nFetch Data:\n\nfetchData(): Fetches the last 6,000 entries from the ThingSpeak API. The await keyword ensures the function waits for the data to be fetched before moving on.\n\nProcess Data:\n\nprocessData(feeds): Processes the raw data to extract timestamps, temperatures in Celsius and Fahrenheit, and humidity. It returns these as arrays.\n\nDisplay Latest Values:\n\ndisplayLatestValues(labels, tempC, tempF, humidity): Extracts the latest values from the arrays and formats them into an HTML table. The toLocaleDateString and toLocaleTimeString methods format the date and time in MM/DD/YYYY and 12-hour format respectively.\n\nCreate Chart:\n\ncreateChart(): Fetches and processes the data, then uses Chart.js to create a line chart with three datasets: Temperature C, Temperature F, and Humidity. It configures the x-axis to display time and the y-axis to start at zero. It also calls displayLatestValues to update the latest values table.\n\nRefresh Data:\n\nrefreshData(): Destroys the existing chart instance and creates a new one with the latest data.\n\nWindow Onload:\n\nwindow.onload: Ensures the chart is created when the page loads.\n\n\nBy breaking down the code into these sections, we can see how each part works together to fetch data, process it, display it in a chart, and show the latest values in a table. This simple dashboard provides a visual representation of sensor data over the last few hours, making it easier to monitor and analyze the data in real time.\n\nOverall, this is a lot more lightweight than Grafana and given the fact that I’m not generating large amounts of traffic to Thingspeak this works very well as a simple logger/check-in for myself. I’m sure there’s a lot more convoluted ways of checking on the sensors, but this web interface allows me to quickly assess if I have downtime or the sensors are toast.\nAdditionally, I downloaded RaspController on iOS to quickly check on my sensors when at home. Overall, the ads were a bit intrusive and even reading from the simple DHT11 sensor was a bit goofy with the sensor readout being read then a few seconds later the readings disconnecting. I believe there’s a PRO version, but I doubt the functionality boost warrants the price as the web display is robust enough.\nRegardless I plan on implementing an ESP32 based solution moving forward. Some comparisons are noted below based on what I’ve read online, but of course that can change once experience/pain sets in. I’m assuming that the 2s delay I’m experiencing will get worse as I add more sensors so I’m assuming the worst. I also plan on implementing the sensors outlined in my first raspberry pi post.\n\nOverview of Sensors\n\nPMS5003: Measures particulate matter (PM2.5 and PM10) with a typical update interval of 1 second.\nMH-Z19: Measures CO2 levels with an update interval of every 2-3 seconds.\nDHT22: Measures temperature and humidity with a sampling rate of 2 seconds.\nBME680: Measures temperature, humidity, pressure, and VOC, with data output intervals ranging from 1 to 3 seconds.\nMQ135: Measures air quality (VOC levels), requires analog-to-digital conversion and calibration for accurate readings.\n\n\n\n1. Sampling Speed and Sensor Handling\n\nRaspberry Pi:\n\nCapabilities: Equipped with a multi-core CPU and up to 8GB RAM, allowing concurrent handling of multiple sensors.\nPMS5003: Capable of continuous 1-second updates.\nMH-Z19: Efficiently handles 2-3 second intervals.\nDHT22: Manages 2-second intervals smoothly.\nBME680: Processes complex outputs (temperature, humidity, pressure, VOC) every 1-3 seconds.\nMQ135: Utilizes an ADC for continuous data reading.\nOverall Sampling: Can read from all sensors simultaneously, processing and displaying data in real-time.\n\nESP32:\n\nCapabilities: Dual-core processor designed for real-time processing, but might need optimization for simultaneous high-frequency data handling(C++).\nPMS5003: Handles 1 Hz output effectively.\nMH-Z19: Manages 2-3 second intervals efficiently.\nDHT22: Handles 2-second intervals without issues.\nBME680: Efficiently processes complex outputs within the 1-3 second range.\nMQ135: Uses built-in ADC for continuous data but might have fewer channels than an external ADC.\nOverall Sampling: Can handle the cumulative sampling rate with careful resource management.\n\n\n\n\n2. Overhead from Operating System\n\nRaspberry Pi:\n\nOS: Runs a full Linux-based OS (e.g., Raspbian) with a graphical user interface, background processes, and system services.\nOverhead: Significant but manageable, thanks to the Pi’s processing power.\nAdvantages: Multi-threading, extensive libraries, ability to run heavy applications (e.g., databases, servers).\n\nESP32:\n\nOS: Operates with a minimal OS like FreeRTOS or can run bare-metal.\nOverhead: Minimal, making it highly efficient for real-time applications.\nAdvantages: Direct hardware control, low latency, efficient resource use.\n\n\n\n\n3. Speed of Data Transfer Wirelessly\n\nRaspberry Pi:\n\nWi-Fi: Supports 2.4 GHz and 5 GHz bands, with data transfer rates up to several hundred Mbps.\nUse Cases: Suitable for fast, reliable data transfer, streaming large datasets, frequent updates to a remote server.\nBluetooth: Available for short-range communication.\n\nESP32:\n\nWi-Fi: Primarily operates on the 2.4 GHz band, with data rates up to 150 Mbps.\nUse Cases: Adequate for sensor data transmission, suitable for IoT applications.\nBluetooth: Supports Bluetooth (classic and BLE), useful for short-range data transfer and low-power communication.\n\n\n\n\n4. Programming Language Choice\n\nRaspberry Pi:\n\nLanguages: Supports Python, C, C++, JavaScript, Java, and more.\nDevelopment: Python is favored for its simplicity and extensive library support for hardware interaction.\nFlexibility: Extensive libraries and frameworks for data processing, machine learning, web servers.\n\nESP32:\n\nLanguages: Commonly programmed using Arduino IDE (C/C++) or MicroPython.\nDevelopment: Arduino IDE provides robust sensor libraries; MicroPython offers ease of use and rapid prototyping.\nFlexibility: C/C++ for performance and control; MicroPython for quick development and debugging.\n\n\n\n\n5. Cost\n\nRaspberry Pi:\n\nBoard Cost: $10 (Pi Zero) to $35-$75 (Pi 3 or Pi 4 models).\nAdditional Costs: Power supplies, SD cards, cases, peripherals, potentially exceeding $100.\nValue Proposition: Higher cost but extensive capabilities, suitable for complex applications.\n\nESP32:\n\nBoard Cost: Typically $5-$10.\nAdditional Costs: Fewer peripherals needed, reducing overall system cost.\nValue Proposition: Highly cost-effective for simple, cost-sensitive projects.\n\n\n\n\n6. Power Consumption\n\nRaspberry Pi:\n\nConsumption: Higher due to its full-featured OS and higher processing power.\nPower Supply: Requires a stable 5V power supply, typically 2.5A or more.\nSuitability: Best for applications with reliable power sources.\n\nESP32:\n\nConsumption: Low-power design with deep sleep modes and efficient power management.\nPower Supply: Can operate on battery power for extended periods.\nSuitability: Ideal for battery-powered or solar-powered IoT applications.\n\n\n\n\n7. Complexity of Setup\n\nRaspberry Pi:\n\nSetup: Involves OS installation, Wi-Fi configuration, and additional software setup.\nEase of Use: Broad ecosystem and community support for troubleshooting.\nLearning Curve: Higher due to OS complexity.\n\nESP32:\n\nSetup: Simpler, involving firmware flashing and code development.\nEase of Use: Growing community support, good documentation.\nLearning Curve: Lower, especially with Arduino IDE or MicroPython.\n\n\n\n\n8. Integration with Databases\n\nRaspberry Pi:\n\nLocal Databases: Can run MySQL, PostgreSQL, SQLite for local storage and complex queries.\nRemote Databases: Interfaces with cloud databases via APIs.\nUse Cases: Suitable for extensive data storage, processing, and local analytics.\n\nESP32:\n\nRemote Databases: Interfaces with databases through HTTP/HTTPS, MQTT, or APIs.\nLocal Storage: Limited, suitable for buffering data before transmission.\nUse Cases: Best for periodic data transmission to a central server.\n\n\n\n\n9. Scalability and Expandability\n\nRaspberry Pi:\n\nScalability: High, with multiple USB ports, GPIO pins, and support for I2C, SPI, UART.\nExpandability: Can connect multiple sensors, peripherals, and expansion boards (HATs).\nUse Cases: Ideal for larger, complex projects needing scalability.\n\nESP32:\n\nScalability: Moderate, fewer GPIO pins but sufficient for many IoT projects.\nExpandability: Supports I2C, SPI, UART; multiple sensors can be connected.\nUse Cases: Suitable for compact, efficient IoT solutions.\n\n\n\n\n10. Community and Support\n\nRaspberry Pi:\n\nCommunity: Extensive, with many tutorials, forums, and resources.\nSupport: Strong, especially for educational, hobbyist, and professional uses.\nDocumentation: Comprehensive, with official support from the Raspberry Pi Foundation.\n\nESP32:\n\nCommunity: Growing, with many tutorials and forums.\nSupport: Adequate, focused on embedded systems and IoT.\nDocumentation: Good, provided by Espressif and third-party contributors.\n\n\n\n\n11. Real-Time Operating Capabilities\n\nRaspberry Pi:\n\nRTOS: Not typically used, though real-time kernels (PREEMPT-RT) are available for specific applications.\nSuitability: Best for applications where real-time performance is not critical.\n\nESP32:\n\nRTOS: FreeRTOS support, ideal for real-time applications.\nSuitability: Designed for real-time processing, making it suitable for time-sensitive tasks.\n\n\n\n\n12. Security\n\nRaspberry Pi:\n\nSecurity Features: Depends on OS and software; can use advanced security protocols and encryption.\nUse Cases: Suitable for applications requiring robust security measures.\n\nESP32:\n\nSecurity Features: Built-in hardware security features (e.g., secure boot, flash encryption).\nUse Cases: Adequate for secure IoT applications.\n\n\n\n\nSummary\n\nSampling Speed: Both the Raspberry Pi and ESP32 can handle the sensors’ sampling rates, with the Pi having more processing power for higher-frequency data collection.\nOverhead from OS: The Raspberry Pi has more overhead due to its full OS, while the ESP32 operates with minimal overhead, ideal for real-time applications.\nWireless Data Transfer: The Raspberry Pi achieves higher data transfer speeds, but the ESP32’s capabilities are sufficient for sensor data transmission.\nProgramming Language: The Raspberry Pi offers more flexibility, while the ESP32 focuses on C/C++ and MicroPython.\nCost: The ESP32 is significantly cheaper, making it attractive for cost-sensitive projects.\nPower Consumption: The ESP32 is more power-efficient, suitable for battery-powered applications.\nComplexity of Setup: The Raspberry Pi setup is more complex but has broader support, while the ESP32 setup is simpler and more focused.\nIntegration with Databases: The Raspberry Pi can run full database servers locally, while the ESP32 typically relies on remote databases.\nScalability and Expandability: The Raspberry Pi is more scalable and expandable, suitable for larger projects.\nCommunity and Support: The Raspberry Pi has a larger community and more extensive support.\nReal-Time Operating Capabilities: The ESP32 is better suited for real-time applications with its RTOS support.\nSecurity: Both offer security features, but the ESP32 has built-in hardware security for IoT applications.\n\nIn conclusion, the Raspberry Pi offers more computational power and flexibility, making it suitable for complex applications requiring robust data processing and local storage. The ESP32, with its low cost, low power consumption, and efficient real-time processing capabilities, is ideal for simpler, cost-effective, and portable sensor applications. Additionally, the cost of failure for a raspberry pi is quite large while an ESP32 is negligible. I would like to have a real time setup along with a dynamic plot of temp, humidity, etc. for a plant monitoring project. To do this at scale(someone has 50+ plants….) I need to dramatically reduce cost. Additionally, I need to also begin budgeting for a Pi cluster project to learn distributed/parallel computing, which, funny enough, are two classes in my Master’s at UIUC that I would like to take(CS425 and CS484 respectively). Truth be told much of this IoT Raspberry Pi/Arduino type work recently stems from a desire to take the IoT class at UIUC, but that class’ priority relative to other courses such as Machine Learning, Statistical Learning, etc. is dead last and puts me over the credits I need. Additionally, implementing X/Y/Z with this hardware allows me to learn how things work at a fraction of tuition(1/10).\nIdeally the cluster would serve as a way of actually learning parallel computing and also establishing some local web services to make my life easier. The tentative list of local web service projects include:\n\nPlant Monitoring Dashboard\n\nProject: A centralized dashboard to monitor temp, humidity, soil moisture, and other environmental factors for plants of concern.\nTech: I plan on using Flask/Django for the backend, React for the frontend, and a database like PostgreSQL for the DB.\nBenefits: I also plan on implementing some sort of algorithm to determine when there’s a problem and sending push notifications or texts to the user.\n\nPersonal Cloud Storage\n\nSelf hosted cloud storage solution using something like NextCloud to store and share files securely. I might keep this guy local though as the only thing I plan on exposing to the World Wide Web would be a website which uses Cloudflare for security.\n\nLocal Development Server(Local Git)\n\nProject: Development environment for testing and deploying web applications and other software projects.\nTech: I’d likely use Docker for containerized environments, Jenkins for CI/CD pipelines, and Git for version control. I would also deeply expand my knowledge of what’s what with this tech.\nBenefits: This should actually help standardize and streamline my development workflow, let me perform automated testing/deployment, and provide a stable/cheap platform for experimentation with new tech.\n\nMedia Server(Mostly PS4/Samsung TV)\n\nProject: Local media server to host/stream media content like movies/music/photos\nTech: Plex or Jellyfin for media management and streaming.\nBenefits: Centralized media library with near seamless access to my favorite(repeatedly watched) shows.\n\nIoT Device Management\n\nProject: Platform to manage/monitor/update IoT devices deployed throughout the house. Likely temp/humidity/air quality sensors and plant sensors. Possible intrusion detection sensors if I decide to revisit some of my previous projects.\nTechnology: Mosquitto for MQTT communication, InfluxDB for time-series data, and Grafana(if I’m not using JSON’s…) for visualization.\nBenefits: Centralized control and visualization of all IoT devices, real time monitoring, and simplified firmware updates. I will have to write quite a bit of C++ code now that I think about it to ensure the ESP32 works and works fast.\n\nNetwork Monitoring Tool\n\nProject: Basic system to monitor the health/performance of my network.\nTech: Nagios or Zabbix.\nBenefits: Real time alerts on network issues, detailed performance metrics, and insights into network traffic patterns.\n\nPiHole\n\nProject: Pi hold server to provide network wide ad blocking, improve browsing speed, and improve privacy by blocking unwanted ads/trackers. It can also act as a local DNS resolver boosting network performance.\nTech: Raspberry Pi for running the software.\nBenefits: Ad blocking, boost privacy, reduce data usage, reduce DNS query times and overall help speed up my network.\n\nHPC Pi\n\nProject: Build a scalable Raspberry Pi cluster(at least 3) to learn and implement parallel computing, distributed computing, and high performance computing concepts.\nTech: Multiple raspberry pis, networking equipment, cluster management software, and parallel computing libraries(OpenMP/MPI). Plus Kubernetes.\nBenefits: Hands on experience with distributed systems and parallel processing, cost effective way of exploring high performance computing, and gaining marketable skills in cluster setup, management, and scalability.\n\n\nTentatively something like this tutorialwill be followed and result in a three pi setup vs the 8 pi setup below(from the tutorial):\n\nHopefully I have some time before the summer ends and classes begin to experiment. Fingers crossed and my willpower willing, I will take the distributed systems course this fall alongside an intensive statistical learning course!"
  },
  {
    "objectID": "posts/Soldering_ESP32/index.html",
    "href": "posts/Soldering_ESP32/index.html",
    "title": "ESP32 Project: Sensor Reliability/Power Efficiency",
    "section": "",
    "text": "Introduction\nAfter experimentation with the previous project at: https://jesse-anderson.github.io/Blog/_site/posts/ESP32_Post_1/ , I encountered issues with the DHT11 sensor maintaining reliable contact with the internal breadboard connections. To address this, I acquired someProtoboards from ElectroCookie and began the process of soldering the components together. I chose ElectroCookie as they were highly rated on Amazon and apparently the Adafruit Protoboards are corrosion prone. It also helps that the ElectroCookie ones were far cheaper! This post details the steps I took to resolve these issues and enhance the project’s overall performance.\n\n\nSoldering the Components\nUsing the same configuration as outlined in the initial ESP32 post, I carefully soldered the components onto the Protoboard. The result was a significant improvement in the consistency of the sensor readings, eliminating the previous contact issues. Below are images of the completed soldering work:\n\nTop View:\n\n\n\nBottom View:\n\n\n\n\nElectrical Safety\nAfter completing the soldering, I was concerned about the potential for short circuits if the board was placed on a conductive surface. To mitigate this risk, I checked the conductivity of a glue gun stick I planned to use for insulation. Confident in its suitability, I applied hot glue to the entire bottom of the board. I opted for large, removable globs of glue rather than thin layers to facilitate easy removal if needed. Here is the “final” product after applying the hot glue:\n\n\n\nIncorporating the Verter Buck-Boost Module\nTo extend the operational time of the ESP32 setup, I integrated the Verter buck-boost module from Adafruit along with their 4 AA battery pack. This configuration achieved an approximate runtime of 28.2 hours, which exceeded my expectations. I attribute part of this efficiency to the removable LED socket used for data transmission checks. The green LED was bright enough to disturb sleep in a darkened room, so I removed it during overnight testing. I estimate that it would have run for 20 ish hours with the LED on as that’s what I roughly estimated(the peak sensor reading plus transmission reading on my power bank may have been off….).\n\n\nWhat is a Buck-Boost Module\nA Buck-Boost Module is a type of DC-DC converter that can step up (boost) or step down (buck) an input voltage to a desired output voltage. This functionality is particularly useful in battery-powered projects where the input voltage can vary significantly as the battery discharges. The module ensures that the device receives a stable voltage, enhancing the reliability and efficiency of the system.\nThe Verter buck-boost module from Adafruit is a versatile power supply solution that can accept input voltages ranging from 3V to 12V and output a consistent 5.2V voltage, making it ideal for various applications. It is equipped with multiple safety features, including over-voltage protection and thermal shutdown, to ensure the safe operation of your electronics. I went with Adafruit over many companies on Amazon as a sensor failing is a disappointment whereas everything on my board being fried is infuriating. The output voltage of the Verter on the USB side is 5.2V and it is far better as a buck converter than a boost converter. Note that it uses a TPS630630 boost converter from TI and the USB connector can output 500mA according to the datasheet. The module overall can output ~1.0+A so if push comes to shove I can always adapt it if I need more current. It has 90+% operating efficiency in some cases and the efficiency graphs are below:\nEfficiency vs. Output Current:\n\nEfficiency vs. Input Voltage Current:\n\nWe see that at V_Out = 4.8V and V_Out = 5V we have an efficiency near or above 90% and that’s good enough for me.\nI ensured all voltages were within specification and verified the output using an opened USB cable. Here’s the setup with the Verter and battery pack:\n\nOnce I tested everything it was fairly straightforward to throw some NiMH Ikea batteries at 1.2V 1900mAh and double check the output voltage to make sure I was ok. Moving forward I would definitely opt for Adafruit’s 8 battery pack but 4 batteries works for my purposes for now. Verter + battery pack below, note that there are some screws you gotta tighten down on for your (+)/(-):\n\n\n\nFuture Improvements\nImplementing a battery monitoring system would enhance the project’s robustness, allowing for better power management. Additionally, incorporating a solar charging circuit could provide a sustainable power solution by using solar energy during the day to charge the batteries and running the device on battery power at night.\n\n\nAdding OLEDs\nFunny enough this tidbit actually predates the battery pack and Buck Boost Converter, but I felt that stabilizing the circuit and getting battery power working was of far higher importance than slapping a screen in the circuit.\nThe OLED display used is a simple 128x32 screen, acquired affordably from Amazon. Using the SSD1306 library to interface with the OLED is straightforward. My plan is to create an ESP32 GitHub repository containing all the sensor integrations packaged into a single, comprehensive folder of Python files for ease of use. This approach simplifies the process compared to tracking down and adapting various libraries.\nBelow are example circuits using standard SCL and SDA connections on the ESP32, as well as alternative connections by assigning the 14 and 13 pins to SCL and SDA respectively:\n\n1 OLED:\n\n\n\n2 OLED:\n\n\n\n\nAlternative Pin Configuration\nIt’s essential to note that the ESP32 allows for flexible pin assignment for the SDA (Serial Data Line) and SCL (Serial Clock Line). Here’s a table showcasing the alternative pin assignments:\n\n\n\nSDA\nSCL\n\n\n\n\n4\n12\n\n\n13\n14\n\n\n17\n16\n\n\n18\n17\n\n\n19\n18\n\n\n23\n19\n\n\n25\n23\n\n\n26\n25\n\n\n27\n26\n\n\n32\n27\n\n\n33\n32\n\n\n\nAdditionally, there are input-only pins: 34, 35, 36, 37, 38, and 39.\n\n\nInitializing and Testing the I2C Bus\nIdeally one would connect their peripheral then run the following code to ensure that they are getting a reading from their device(change pin numbers!):\n\n\nCode\n\nfrom machine import Pin, SoftI2C\nimport time\n\n# Initialize I2C with default pins\ni2c = SoftI2C(scl=Pin(22), sda=Pin(21))\n\ndef scan_i2c(i2c):\n    print(\"Scanning I2C bus...\")\n    devices = i2c.scan()\n    if len(devices) == 0:\n        print(\"No I2C devices found\")\n    else:\n        print(\"I2C devices found:\", len(devices))\n        for device in devices:\n            print(\"Decimal address:\", device, \" | Hex address:\", hex(device))\n\nwhile True:\n    scan_i2c(i2c)\n    time.sleep(5)\n\nGoing back to the OLEDs… After we verify that they are connected correctly we can run the following to print:\n\n\nCode\n\nfrom machine import Pin, SoftI2C\nimport ssd1306\nfrom time import sleep\n\n# ESP32 Pin default\ni2c = SoftI2C(scl=Pin(22), sda=Pin(21))\n\n# ESP8266 Pin default\n#i2c = SoftI2C(scl=Pin(5), sda=Pin(4))\n\noled_width = 128\noled_height = 32\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\n# Clear the display buffer\noled.fill(0)\n\noled.text('Existence is a prison', 0, 0)\noled.text(\"prison.I'm bound.\", 0, 10)\noled.text('to this device', 0, 20)\n\noled.show()\n\n# ESP32 Pin assignment \ni2c_2 = SoftI2C(scl=Pin(14), sda=Pin(13))\n\n# Clear the display buffer\noled2.fill(0)\n\noled_width = 128\noled_height = 32\noled2 = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c_2)\n\noled2.text('Existence is a prison', 0, 0)\noled2.text(\"prison.I'm bound.\", 0, 10)\noled2.text('to this device', 0, 20)\n\noled2.show()\n\n\n\nThe SSD1306 Library\nAn OLED works by emitting light from organic compounds that emit light when an electric current is applied. This technology allows for bright, clear displays that are highly efficient and have excellent contrast ratios. OLEDs are used in a variety of applications, from small displays on microcontrollers to large television screens. I direct you to the wikipedia article at: https://en.wikipedia.org/wiki/OLED if you want to do a deep dive into materials of construction, operating principle, etc.\nThe SSD1306 library is a widely used library for controlling OLED displays based on the SSD1306 driver. This library simplifies the process of communicating with the OLED and provides a set of functions to easily draw text, shapes, and images on the screen. By using the SSD1306 library, developers can quickly integrate OLED displays into their projects without needing to understand the low-level details of the communication protocol.\nBeyond simple text one can draw lines, rectangles, circles, and even bitmaps. This allows for the creation of detailed and informative graphical interfaces provided you have enough pixels.\n\n\nFuture Improvements\nSeveral quality of life improvements can further enhance the ESP32 environmental monitor. Implementing a battery monitoring system would provide better power management. Additionally, incorporating a solar charging circuit could offer a sustainable power solution by using solar energy during the day to charge the batteries and running the device on battery power at night. The design of the air intake box, sensor sampling rates, and minimizing power draw would constitute some one off things I’ll need to do, but for right now they aren’t too relevant. In any case, being able to quickly visualize what’s going on with the sensors is cool enough, but having the web interface is far more useful.\n\n\nConclusion\nThrough some careful(haphazard?) soldering I got a reliable temperature/humidity sensor working with a web interface and a fairly large battery capacity. One note for portability: If I ever wanted to remotely log data to Google/etc I would need to set up a hotspot and assign that as an alternative WiFi that tries to connect if the first one isn’t present.\nThis post is definitely a bit more stop and go than previous posts owing to the fact that I needed to document this mini project, but also have a fair bit of backlog when it comes to writing. At a later date I may revisit this and clean up the writing. The key focus is being able to reference this in the future for my own personal use.\nStay tuned for more updates."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my inaugural Quarto post! Though this marks my official entry into the Quarto blogging sphere, it isn’t my first dive into writing here. I initially crafted an insightful(?) post about the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a fascinating clustering algorithm that piqued my interest due to its rigorous yet intuitive approach to handling complex data sets.\nMy name is Jesse Anderson, and my academic and professional journey began in the world of chemical engineering, where I specialized in biochemical engineering and process automation. The intricate dance of chemical processes, moving systematically from one unit to another, serendipitously mirrored the logic and flow of computer science. It was this realization that bridged my transition from engineering physical processes to engineering digital ones.\nThis transition felt incredibly natural. In both domains, you’re essentially inputting a series of commands—whether to a machine in a lab or a line of code in a software program—and watching as complex processes unfold, often with a satisfying precision. This similarity is what drew me deeper into the world of computing, leading me to pursue a Master’s in Computer Science at the University of Illinois at Urbana-Champaign (UIUC). My undergraduate studies were completed at the University of Illinois at Chicago (UIC), where I earned a Bachelor of Science in Chemical Engineering.\nDuring my time at UIC, my fascination with optimization and automation took root. This passion wasn’t confined to coursework and theory. I actively engaged in developing research software, working closely with Dr. Ying Hu. My contributions during this period were not only fulfilling but also fruitful, culminating in several scholarly publications.\nOutside of academia, my hobbies closely mirror my professional interests, intertwining a love for optimization with a hands-on approach to problem-solving that spans far beyond algorithms and lab reactors. This passion for efficiency and process improvement extends into crafting tools and scripts that enhance productivity and automate the more mundane tasks of everyday life. But my knack for tool-making isn’t just confined to the digital realm; it’s a skill I’ve honed over many years, influenced heavily by my extensive background as an independent contractor.\nBefore venturing into the world of chemical engineering and computer science, I spent several years in the trades, tackling every aspect of plumbing, electrical work, HVAC, carpentry, and remodeling. This experience taught me the value of precision and strategic planning, skills that are just as applicable to programming as they are to physical construction and maintenance. Each project, whether installing a complex plumbing system or wiring a newly constructed home, sharpened my ability to think critically and adapt quickly—traits that have proven indispensable throughout my career.\nIn addition to my hands-on trade work, I managed the safety and project planning operations at G5 Environmental, a prominent street sweeping company based in Chicago. My role involved ensuring the smooth execution of projects, maintaining rigorous safety standards, and optimizing operations to enhance efficiency. This experience not only broadened my understanding of large-scale project management but also deepened my appreciation for the intricate dance between man, machine, and the environment. At G5 Environmental, we were committed to maintaining the cleanliness and safety of public spaces, a mission that echoes my current work’s focus on sustainability and environmental preservation through technology.\nMore recently, during my tenure at UL Solutions, I further developed my technical skills and applied them to real-world challenges:\n\nAutomation Software Development: I developed automation software in Python and VBA that successfully eliminated over 664+ hours on average of tedious engineering workflows annually.\nCAD/PDF Change Detection: I developed a few python scripts to automatically compare CAD files and compare long PDFs(like Acrobat Pro, but free and simpler).\nReal-Time Data Analysis for Testing: I implemented systems for real-time data analysis to support large-scale testing projects. This advancement has enabled more data-driven decision-making, optimizing testing procedures and outcomes through immediate feedback and analysis.\n\nWhile I massively enjoyed the automation projects I took on at UL, I was seeking broader challenges and opportunities for growth that UL could no longer provide. This realization led me to pursue a more fitting opportunity elsewhere, where I could further expand my expertise and impact in the fields of process automation and data-driven technology.\nThis diverse background has equipped me with a unique perspective that I bring to my current studies and research. Whether refining code to streamline data processing or rethinking a workflow to enhance laboratory operations, I am constantly learning and applying, pushing the boundaries of what I know about both chemical engineering and computer science. These experiences have molded me into a versatile professional capable of adapting to various roles and continuously refining my approach to both new and familiar challenges.\nAs I continue my studies at UIUC, my goal is to further explore how advanced computational techniques can be applied to solve complex problems in biochemical engineering and beyond. The integration of process automation into digital applications opens up exciting possibilities for innovation in both fields. Through this blog, I aim to share insights from my journey, discussing both the theoretical and practical aspects of my work and studies in hopes of connecting with fellow enthusiasts and professionals who share my zeal for optimization and automation.\nStay tuned for more posts where I’ll delve deeper into specific projects, challenges I’ve faced, and solutions I’ve discovered along the way. Thank you for joining me on this adventure in merging the systematic worlds of engineering and computing into one coherent, intertwined narrative."
  }
]