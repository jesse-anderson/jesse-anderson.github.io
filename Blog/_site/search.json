[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Jesse Anderson's Blog",
    "section": "",
    "text": "General readme.\n\n\nTo push a new blog or rather statically generate one.\n\n\ncd this folder in cmd\n\n\nquarto render"
  },
  {
    "objectID": "posts/VAE_GAN/VAEGAN.html",
    "href": "posts/VAE_GAN/VAEGAN.html",
    "title": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs",
    "section": "",
    "text": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\nDuring my time in the applied machine learning course(CS441) at the University of Illinois at Urbana-Champaign, I embarked on an ambitious journey into the realm of advanced machine learning technologies. The course required a comprehensive understanding of various machine learning concepts, providing both breadth and depth in our studies. For my final optional project, I chose to specialize in some of the most intriguing areas of generative models: Variational AutoEncoders (VAEs), Denoising AutoEncoders, Generative Adversarial Networks (GANs), and the innovative hybrid, Variational Autoencoder Generative Adversarial Networks (VAE-GANs). Here, I provide a high-level overview of each technology and discuss the outcomes of my experiments. For further information about the course please see sample syllabus.\n\nVariational AutoEncoders (VAEs)\nVAEs are powerful generative models that use the principles of probability and statistics to produce new data points that are similar to the training data. Unlike traditional autoencoders, which aim to compress and decompress data, VAEs introduce a probabilistic twist to encode input data into a distribution over latent space. This approach not only helps in generating new data but also improves the model’s robustness and the quality of generated samples. VAEs are particularly effective in tasks where you need a deep understanding of the data’s latent structure, such as in image generation and anomaly detection.\nThe encoder in a VAE is responsible for transforming high-dimensional input data into a lower-dimensional and more manageable representation. However, unlike standard autoencoders that directly encode data into a fixed point in latent space, the encoder in a VAE maps inputs into a distribution over the latent space. This distribution is typically parameterized by means (mu) and variances (sigma), which define a Gaussian probability distribution for each dimension in the latent space.\nThe latent space in VAEs is the core feature that distinguishes them from other types of autoencoders. It is a probabilistic space where each point is defined not just by coordinates, but by a distribution over possible values. This stochastic nature of the latent space allows VAEs to generate new data points by sampling from these distributions, providing a mechanism to capture and represent the underlying probabilistic properties of the data. It essentially acts as a compressed knowledge base of the data’s attributes.\nOnce a point in the latent space is sampled, the decoder part of the VAE takes over to map this probabilistic representation back to the original data space. The decoder learns to reconstruct the input data from its latent representation, aiming to minimize the difference between the original input and its reconstruction. This process is governed by a loss function that has two components: a reconstruction loss that measures how effectively the decoder reconstructs the input data from the latent space, and a regularization term that ensures the distribution characteristics in the latent space do not deviate significantly from a predefined prior (often a standard normal distribution).\nIn practice, the encoder’s output of means and variances provides a smooth and continuous latent space, which is crucial for generating new data points that are similar but not identical to the original data. This property makes VAEs particularly useful in tasks requiring a deep generative model, such as synthesizing new images that share characteristics with a training set, or identifying anomalies by seeing how well data points reconstruct using the learned distributions.\n\n\n\n\n    Attribution Example\n\n\n    By EugenioTL - Own work, CC BY-SA 4.0, Link\n\n\n\n\nDenoising AutoEncoders\nDenoising Autoencoders (DAEs) are specialized neural networks aimed at improving the quality of corrupted input data by learning to restore its original, uncorrupted state. This functionality is crucial in applications such as image restoration, where DAEs enhance image clarity by effectively removing noise. They achieve this through a training process that involves a dataset containing pairs of noisy and clean images. By continually adjusting through this training set, the DAE learns the underlying patterns necessary to filter out the distortions and recover the clean data. This ability to directly process and improve corrupted data makes DAEs valuable for various tasks beyond image restoration, including audio cleaning and improving data quality for analytical purposes.\n\nSource: https://blog.keras.io/building-autoencoders-in-keras.html\n\n\nGenerative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) utilize a unique framework involving two competing neural networks: a generator and a discriminator. These networks engage in an adversarial game, where the generator’s goal is to create synthetic data that is indistinguishable from real-world data, effectively “fooling” the discriminator. The discriminator’s job, on the other hand, is to distinguish between the authentic data and the synthetic creations of the generator.\nThis dynamic creates a feedback loop where the generator continually learns from the discriminator’s ability to detect fakes, driving it to improve its data generation. As the generator gets better, the discriminator’s task becomes more challenging, forcing it to improve its detection capabilities. Over time, this adversarial process leads to the generation of highly realistic and convincing data outputs.\nGANs have been particularly successful in the field of image generation, where they are used to create highly realistic images that are often indistinguishable from actual photographs. A prominent example is the ThisPersonDoesNotExistwebsite, which uses a model called StyleGAN2to generate lifelike images of human faces that do not correspond to real individuals. This technology has also been applied in other areas such as art creation, style transfer, and more. Eerie.\n\nSource: https://developers.google.com/machine-learning/gan/gan_structure\n\n\nVariational Autoencoder Generative Adversarial Networks (VAE-GANs)\nVAE-GANs are an innovative hybrid model that synergistically combines Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to enhance the quality and control of data generation. The model integrates the VAE’s capability for creating a compressed, latent representation of data with the GAN’s strength in generating high-fidelity outputs.\nIn a VAE-GAN system, the encoder part of the VAE compresses input data into a latent space (a condensed representation), which the GAN’s generator then uses to reconstruct outputs that are as realistic as possible. This setup leverages the VAE’s ability to manage and interpret the latent space effectively, providing a structured, meaningful input for the GAN’s generator. The discriminator in the GAN setup then evaluates these outputs against real data, guiding the generator to improve its outputs continually.\nThe fusion of these two models allows for a more controlled generation process, which can lead to higher quality outputs than what might be achieved by either model alone. This approach not only enhances the detail and realism of the generated data but also improves the model’s ability to learn diverse and complex data distributions, making VAE-GANs particularly useful in tasks that require a high level of detail and accuracy, such as in image generation and modification.\n\nSource: Larsen, Anders & Sønderby, Søren & Winther, Ole. (2015). Autoencoding beyond pixels using a learned similarity metric.\n\n\nProject Outcomes\nThe practical application of these models in my project yielded fascinating insights and results. For instance, the VAEs demonstrated an impressive ability to generate new images that closely resembled the original dataset, while the Denoising AutoEncoders more or less restored a significant portion of corrupted images to their original state. Similarly, the GANs produced images that were often indistinguishable from real ones, highlighting their potential in creating synthetic data for training other machine learning models without the need for extensive real-world data collection.\nThe VAE-GANs, however, were the highlight, combining the best aspects of their constituent models to generate supremely realistic and diverse outputs. While I am unable to share specific code snippets of the DAE/VAE due to copyright restrictions on the course content, the qualitative outcomes were highly encouraging and indicative of the powerful capabilities of hybrid generative models.\n\n\nResults\n\n\nDenoising AutoEncoder\n\nAs you can see in the image above, it does an ok job of denoising the middle image. The top image is the original image, the middle is the image with 50% noise, and the bottom is the model’s outputted denoised image. If I trained the model longer and varied up the training data I likely would have been able to get a better result. Additionally Principal Component Analysis and calculation of the Mean Residual Error was performed to determine how well the model works. See below:\n\nLoss over Epochs(Note the plots say reconstruction error when I really meant Mean Squared Error Loss):\n\nI decided to use a standard fixed learning rate here and only trained for 240 epochs.\n\n\nVariational AutoEncoder\nHere we were tasked with coming up with a VAE which would generate images and also be able to interpolate between images.\nSame Digits:\n\nDifferent Digits:\n\nOriginal vs. Reconstructed:\n\nAs you can see the VAE did a fairly good job of generating images.\nLoss over 400 Epochs:\n\nI was playing around with progressively reducing the learning rate as parameters changed(or didn’t) and thus reduced the learning rate progressively. This actually seemed to result in the exponentialLR type scheduler funnily enough. See here. The model was trained for 400 epochs. I likely won’t spin the DAE/VAE back up for videos as I did for the GAN and VAE-GAN.\n\n\nGenerative Adversarial Network\nI decided to go a bit further and try to get a Generative Adversarial Network running to generate new images of numbers. I went beyond the standard requirements of the course, but not too far as I didn’t want to “waste” too much time as there are newer technologies nowadays. Here’s the repo. There is a video below that shows the evolution of the training of the model. Additionally here’s the final result and video below:\n\n\n\n\n\n\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\nLoad and Play Video\n\n\n\n\n\nLosses over 540 Epochs:\n\n\n\nVAE-GAN\nI finally went a bit further(probably too far) and decided to implement a VAE-GAN. There was a lot more balancing involved between the autoencoder portion and the generative portion and I was able to achieve a passable result, but definitely not worth the time and effort to balance parameters. It was strangely smoothed out, yet blurred where it mattered to generate the images.\nFinal result image and video below:\n\n\n\n\n\n\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\n\nLoad and Play Video\n\n\n\n\n\nHere’s the associated Losses over 1000 Epochs(note my discriminator freaking out…):\n\n\n\nFinal Thoughts\nExploring these advanced generative models not only enhanced my understanding of the deep theoretical underpinnings of machine learning but also provided a practical toolkit for addressing complex real-world data generation and enhancement challenges. The knowledge and experience gained through this project are invaluable and have opened up numerous possibilities for further research and application in the field of artificial intelligence. I anticipate broadening my skillset in Generative AI here soon and will continue to skill up. I also got to experience the sheer tedium of “untangling” a neural network wherein something went wrong in my layers… repeatedly."
  },
  {
    "objectID": "posts/SCD41/index.html",
    "href": "posts/SCD41/index.html",
    "title": "SCD41: On Demand CO2 Sensor",
    "section": "",
    "text": "Original Post: 06/01/2024\nUpdate: I updated this post as of 09/02/24 to reflect the changes made to the CO2 sensor, namely using 5V for stability, an OLED screen for display, switches for wifi and calibration, and finally an obnoxious buzzer to let me know I should open my windows/door(&gt;1500ppm).\nDisclaimer: Please note that the information presented in this article is for informational purposes only. It is not intended to serve as health advice, engineering advice, or any form of professional guidance. Readers should consult with qualified professionals for specific health or engineering concerns and should not rely solely on the content of this article for making decisions. The authors and publishers of this article are not responsible for any actions taken based on the information provided herein.\nIn this post I intend to cover the basics of SCD41 sensors, their technical performance, some MicroPython code to use with an ESP32 microcontroller(yes I know there are other ones out there), and some of the limitations/precautions one should take with this particular sensor.\n\nWhat is an SCD41 Sensor?\nAn SCD41 sensor is Sensirion’s miniature CO2 sensor. It uses a photoacoustic NDIR sensing principle along with Sensirion’s patented technology to offer high accuracy at a competitive price. I personally picked up this sensor for $28.99, and it could likely be found at a lower price directly from the manufacturer or via platforms like AliExpress. The sensor also includes on-chip signal compensation with a built-in SHT4x humidity and temperature sensor to account for fluctuations in environmental conditions, ensuring accurate CO2 readings.\n\n\nTechnical Performance\n\nGeneralized CO2 Accuracy Specification:\n\n400-1,000 ppm: ±(50 ppm + 2.5% of reading)\n1,001-2,000 ppm: ±(50 ppm + 3% of reading)\n2,001-5,000 ppm: ±(40 ppm + 5% of reading)\n\n\n\nTemperature and Humidity Specifications:\n\nTemperature Accuracy: -10°C to 60°C range with an accuracy of ±0.8°C\nHumidity Accuracy: 0-95% RH range with an accuracy of ±6% RH\n\n\n\nMore Detailed Specifications…\n\nSCD41 Sensor Series Specifications\n\n\n\n\n\n\n\n\nSpecification\nValue\nUnit\n\n\n\n\nCO₂ Measurement Range\n0 - 40,000\nppm\n\n\nCO₂ Measurement Accuracy (SCD41)\n400 - 1,000\n±(50 ppm + 2.5% of reading) ppm\n\n\n\n1,001 - 2,000\n±(50 ppm + 3% of reading) ppm\n\n\n\n2,001 - 5,000\n±(40 ppm + 5% of reading) ppm\n\n\nCO₂ Repeatability\nTypical\n±10 ppm\n\n\nCO₂ Response Time (τ63%)\nTypical\n60 s\n\n\nAdditional Accuracy Drift (5 years)\n400 - 2,000, with ASC enabled\n±(5 ppm + 0.5% of reading) ppm\n\n\nHumidity Measurement Range\n0 - 100\n%RH\n\n\nHumidity Accuracy (typical)\n15 °C – 35 °C, 20 %RH – 65 %RH\n±6 %RH\n\n\n\n-10 °C – 60 °C, 0 %RH – 100 %RH\n±9 %RH\n\n\nHumidity Repeatability\nTypical\n±0.4 %RH\n\n\nHumidity Response Time (τ63%)\nTypical\n90 s\n\n\nHumidity Accuracy Drift\nYearly\n&lt;0.25 %RH\n\n\nTemperature Measurement Range\n-10 - 60\n°C\n\n\nTemperature Accuracy (typical)\n15 °C – 35 °C\n±0.8 °C\n\n\n\n-10 °C – 60 °C\n±1.5 °C\n\n\nTemperature Repeatability\n-\n±0.1 °C\n\n\nTemperature Response Time (τ63%)\nTypical\n120 s\n\n\nTemperature Accuracy Drift\nYearly\n&lt;0.03 °C\n\n\nSupply Voltage\n2.4 - 5.5\nV\n\n\nAverage Supply Current\nTypical\n15 mA\n\n\nMax. Supply Current\n-\n205 mA\n\n\n\n\nThe response time is roughly every 2 minutes for temperature and every 90 seconds for humidity. Using the longest response time as a basis for our sampling, we get a sampling rate of once every 2 minutes. The sensor’s extensive command set, clearly detailed in the datasheet, simplifies programming but requires familiarity with numerous commands.\nBefore diving into the fundamentals let’s visit why we even care about CO2 to begin with.\n\n\n\nCO2 Levels in an Enclosed Space\nCO2 levels increase in an enclosed room predictably with time as it is an enclosed system with n number of producers(people). We can actually do a mass balance around the system if we really wanted to to evaluate the amount of CO2 if we had values per person. A mass balance for CO2 in an enclosed room involves calculating how the amount of CO2 changes over time. Since the room is sealed, we start with an initial amount of CO2 and add the CO2 produced by people inside the room. By knowing how much CO2 each person produces per minute, we can estimate the total CO2 in the room after a certain period. This helps us predict how CO2 levels will rise as people continue to breathe, ensuring the air quality is monitored and maintained.\nAs CO2 levels increase a number of health complaints can arise and these health complaints are CO2 concentration dependent. The Wisconsin Department of Health Services outline a ppm to health complaint table:\n\n400 ppm: average outdoor air level.\n400–1,000 ppm: typical level found in occupied spaces with good air exchange.\n1,000–2,000 ppm: level associated with complaints of drowsiness and poor air.\n2,000–5,000 ppm: level associated with headaches, sleepiness, and stagnant, stale, stuffy air. Poor concentration, loss of attention, increased heart rate and slight nausea may also be present.\n5,000 ppm: this indicates unusual air conditions where high levels of other gases could also be present. Toxicity or oxygen deprivation could occur. This is the permissible exposure limit for daily workplace exposures.\n40,000 ppm: this level is immediately harmful due to oxygen deprivation.\n\nWe also see plenty of examples of CO2 levels reaching dangerous levels in enclosed spaces and I am including images along with associated posts:\nFrom https://vair-monitor.com/:\n\nFrom https://cambridgecarbonfootprint.org/:\n\nNow if we move to a more engineering based analysis we go to https://www.engineeringtoolbox.com/pollution-concentration-rooms-d_692.html and find that the carbon dioxide concentration in a room is a function of:\nCO2 Concentration Calculation\n    The carbon dioxide concentration in a room filled with persons after a time \\( t \\) can be calculated as:\n    c = (q / (n V)) [1 - (1 / en t)] + (c0 - ci) (1 / en t) + ci                              \n    \n    where:\n    \n        c = carbon dioxide concentration in the room (m3/m3)\n        q = carbon dioxide supplied to the room (m3/h)\n        V = volume of the room (m3)\n        e = the constant 2.718...\n        n = number of air shifts per hour (1/h)\n        t = time (hour, h)\n        ci = carbon dioxide concentration in the inlet ventilation air (m3/m3)\n        c0 = carbon dioxide concentration in the room at start, \\( t=0 \\) (m3/m3)\n    \nCalculator(the same formula as Engineering Toolbox): Defaults: 2 people, 10ft by 10ft room with 10ft feet ceilings, 1 air change per hour, and the makeup air is the standard 400ppm CO2. This is of course assuming absolutely no gaps and you’re truly enclosed. Worst case scenario and as this is not cited anywhere take with a grain of salt!\n\n\n\n    \n    \n    CO2 Concentration Calculator\n    \n\n\n    CO2 Concentration Calculator\n    \n        q - Carbon dioxide supplied to the room per person (m3/h person):\n        \n\n        Number of persons in the room:\n        \n\n        V - Volume of the room (m3):\n        \n\n        n - Number of air shifts per hour (1/h):\n        \n\n        t - Time (hour, h):\n        \n\n        ci - Carbon dioxide concentration in the make up air (m3/m3):\n        \n\n        c0 - Carbon dioxide concentration in the room at start, t=0 (m3/m3):\n        \n\n        Calculate\n    \n\n    CO2 concentration: 0.00657 m3/m3 (6574 ppm)\n\n    \n\n\nWe also have a similar calculator here: https://www.soletairpower.fi/co2-calculator/\nFor a more scientifically based study one can navigate to the following: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7411428/\nThe study gives a CO2 based occupancy estimation by correlating the levels of CO2 with the number of occupants in a room with the ultimate goal of reducing energy use by dynamically adjusting HVAC systems in real time. This means you don’t ventilate rooms that are unoccupied or ventilate less if they are under-occupied. Its probably possible to tease out a solid mathematical formula that could be put into a calculator, but that is time away from the goal of this article SCD41 and CO2 measurement.\n\n\nCharacteristics of NDIR Sensors\nNDIR sensors measure CO2 by exploiting its property to absorb IR light at around 4.2 µm. They use a non-dispersive band-pass filter to allow only the relevant IR wavelengths to pass, hence the name Non-Dispersive Infrared. An image from Wikipedia below outlines various gases and their Mid-infrared absorption spectra:\n\n\n\nHow does Photoacoustic NDIR Work?\nPhotoacoustic Non-Dispersive Infrared (NDIR) technology is an advanced method used in sensors like the SCD41 to measure gas concentrations, such as CO2, using light and sound. Here’s a brief explanation of how it works:\n\n\nKey Steps in Photoacoustic NDIR\n\nInfrared Light Source: An infrared (IR) light source emits light at wavelengths absorbed by CO2.\nGas Absorption: CO2 molecules absorb this light, causing them to heat up and vibrate.\nPressure Waves: The vibrations create tiny sound waves.\nMicrophone Detection: A microphone detects these pressure waves.\nSignal Processing: The sensor converts these waves into electrical signals to determine CO2 concentration.\nOutput: The final CO2 concentration is displayed for monitoring or further processing.\n\nThis method offers high sensitivity and accuracy, making it suitable for real-time CO2 monitoring in various applications, including indoor air quality and industrial processes. The SCD41 sensor is compact, low-power, and cost-effective, ideal for integration into diverse systems.\n\n\nTransmissive NDIR\nThese sensors feature an IR emitter and an optical detector at opposite ends of an optical cavity. Here’s a quick rundown of how they work:\n\nIR Emitter: Emits light through the gas sample.\nAbsorption by CO2: CO2 absorbs specific wavelengths of the IR light.\nDetection: The detector measures the transmitted IR light.\nCalculation: CO2 concentration is calculated based on the difference in emitted and transmitted light.\n\nTransmissive NDIR sensors require precise positioning and minimal optical path length to ensure accurate readings.\n\n\nTVOC Sensors and eCO2 Readings\nUsing Total Volatile Organic Compounds (TVOC) sensors, such as the Sensirion SGP30, to estimate CO2 levels is generally unreliable. TVOC sensors measure the concentration of various organic compounds in the air, which can include emissions from household products, cooking, cleaning agents, and even human breath. While these sensors are adept at detecting a wide range of VOCs, they are not designed to specifically measure CO2.\n\nWhy TVOC Sensors Fall Short for CO2 Estimation\n\nBroad Detection Spectrum: TVOC sensors detect a broad range of organic compounds, not just CO2. This means they can be influenced by numerous sources of VOCs that are unrelated to CO2 levels, such as air fresheners, deodorizers, and various chemical products used indoors.\nLack of Specificity: TVOC sensors lack the specificity required to accurately distinguish between CO2 and other VOCs. The presence of other VOCs can cause the sensor to give false indications of high CO2 levels.\nEnvironmental Factors: Various environmental factors such as temperature, humidity, and the presence of other gases can affect the readings of TVOC sensors, further complicating their accuracy when estimating CO2 levels.\n\nDespite these limitations, some vendors and manufacturers still promote TVOC sensors for CO2 estimation. This can be misleading for consumers who may believe they are getting accurate CO2 measurements. It is important for users to understand these limitations and consider more reliable methods, such as NDIR or photoacoustic sensors, for precise CO2 monitoring.\n\n\nRecommended Use of TVOC Sensors\nWhile TVOC sensors are not suitable for accurate CO2 measurement, they are valuable tools for:\n\nIndoor Air Quality Monitoring: Detecting the presence of harmful VOCs and identifying sources of indoor air pollution.\nHealth and Safety: Ensuring that indoor environments are free from harmful concentrations of volatile organic compounds.\nIndustrial Applications: Monitoring air quality in industrial settings to ensure compliance with safety regulations.\n\n\n\n\nComparing Low-Cost CO2 Sensors\n\nNDIR Sensors\nNDIR sensors measure CO2 based on gas absorption of IR light. They typically feature an IR emitter, dual channels for reference and measurement, and calculate CO2 concentration by comparing light absorption in these channels.\n\n\nPhoto-Acoustic Sensors\nPhoto-acoustic sensors also measure absorption but use a microphone to detect the resulting pressure waves. They are smaller and do not rely on line-of-sight, making them suitable for compact applications.\nIn the image below we can see a comparison of the photoacoustic and transmissive NDIR sensors:\n\nWorking Principle References:\nhttps://www.airgradient.com/blog/co2-sensors-photo-acoustic-vs-ndir/\nhttps://www.sensirion.com/resource/application_note/ndir-sensors-types\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC7248969/\n\n\n\nSetting up the Circuit\nThe circuit used here is fairly simple as the SCD41 has 4 inputs.\n\nGROUND = GND\nVoltage_in = VDD\nSCL = Serial Clock Line\nSDA = Serial Data Line\n\nWe also use an LCD to monitor the output and one switch to control calibration on startup(two switches for toggling wifi). For the sake of being able to recreate this fast with the same board here are the associated pin numbers and side.\nSCD41\n\nSDA: 14-Left\nSCL: 17-Left\nVDD: 1-Right\nGND: 19-Left\n\nSwitch Calibration(Left Switch)\n\nTop(ON): 12-Right\nGND: Ground Bar-Right\nBottom: N/A\n\nSwitch Wifi(Right Switch)\n\nTop(ON): 13-Right\nGND:Ground Bar - Right\nBottom: N/A\n\nLCD\n\nSDA:10-Left\nSCL:7-Left\nVCC: 12-Right\nGND: Ground Bar-Right\n\nBuzzer\n\n15-Right\nGround Bar - Right\n\nNote that with some changes to the code one can omit the LCD and the switches, but being able to calibrate on the fly with fresh air is nice as is having an offline version of the sensor. Also note that at 3.3V the peak supply current is typical 175mA with a max of 205mA and at 5.0V typical is 115mA with a max of 137mA. I personally experienced many issues using the 3.3V and switched to 5V and experienced absolutely no issues.\n\nThe MicroPython library used is below. Simply open up the file in Thonny and save it to the device as SCD41.py so it can be imported in the main.py script. Note that the comments can be removed to cut down on space on your device. Implementing the Google Drive database and the ThingSpeak database are separate posts. See Raspberry Pi Sensor Server Project for more details on setting that up.\n\n\nLibrary Code\n\n# SCD41.py\n#This code is pretty heavily based off of the Sensirion type code at: https://github.com/octaprog7/SCD4x\n# And this library written by Sensirion: https://github.com/Sensirion/python-i2c-scd\n# This file should be saved as SCD41.py and called as import SCD41. See main.py in this folder for an example usage.\nimport time\nimport math\nfrom machine import SoftI2C, Pin, SPI\nimport micropython\nimport ustruct\n\n@micropython.native\ndef _calc_crc(sequence) -&gt; int:\n    \"\"\"\n    Calculate CRC-8 checksum for the given sequence.\n    \"\"\"\n    return crc8(sequence, polynomial=0x31, init_value=0xFF)\n\n@micropython.native\ndef check_value(value: int, valid_range, error_msg: str) -&gt; int:\n    \"\"\"\n    Check if the value is within the valid range. Raise ValueError if not.\n    \"\"\"\n    if value not in valid_range:\n        raise ValueError(error_msg)\n    return value\n\nclass Device:\n    \"\"\"Base class for devices.\"\"\"\n    def __init__(self, adapter, address: [int, SPI], big_byte_order: bool):\n        \"\"\"\n        Initialize the device with adapter, address, and byte order.\n        \"\"\"\n        self.adapter = adapter\n        self.address = address\n        self.big_byte_order = big_byte_order\n        self.msb_first = True\n\n    def _get_byteorder_as_str(self) -&gt; tuple:\n        \"\"\"\n        Return the byte order as a string ('big' or 'little') and format character ('&gt;' or '&lt;').\n        \"\"\"\n        if self.is_big_byteorder():\n            return 'big', '&gt;'\n        else:\n            return 'little', '&lt;'\n\n    def unpack(self, fmt_char: str, source: bytes, redefine_byte_order: str = None) -&gt; tuple:\n        \"\"\"\n        Unpack the given source bytes according to the format character and byte order.\n        \"\"\"\n        if not fmt_char:\n            raise ValueError(f\"Invalid length fmt_char parameter: {len(fmt_char)}\")\n        bo = self._get_byteorder_as_str()[1]\n        if redefine_byte_order is not None:\n            bo = redefine_byte_order[0]\n        return ustruct.unpack(bo + fmt_char, source)\n\n    @micropython.native\n    def is_big_byteorder(self) -&gt; bool:\n        \"\"\"\n        Check if the device uses big byte order.\n        \"\"\"\n        return self.big_byte_order\n\nclass BaseSensor(Device):\n    \"\"\"Base class for sensors.\"\"\"\n    def get_id(self):\n        raise NotImplementedError\n\n    def soft_reset(self):\n        raise NotImplementedError\n\nclass Iterator:\n    \"\"\"Iterator class for sensors.\"\"\"\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        raise NotImplementedError\n\nclass BitField:\n    \"\"\"Class for working with bit fields.\"\"\"\n    def __init__(self, start: int, stop: int, alias: [str, None]):\n        \"\"\"\n        Initialize the bit field with start, stop, and alias.\n        \"\"\"\n        check(start, stop)\n        self.alias = alias\n        self.start = start\n        self.stop = stop\n        self.bitmask = _bitmask(start, stop)\n\n    def put(self, source: int, value: int) -&gt; int:\n        \"\"\"\n        Write the value to the specified bit range in the source.\n        \"\"\"\n        src = source & ~self.bitmask\n        src |= (value &lt;&lt; self.start) & self.bitmask\n        return src\n\n    def get(self, source: int) -&gt; int:\n        \"\"\"\n        Get the value from the specified bit range in the source.\n        \"\"\"\n        return (source & self.bitmask) &gt;&gt; self.start\n\n@micropython.native\ndef put(start: int, stop: int, source: int, value: int) -&gt; int:\n    \"\"\"\n    Write the value to the specified bit range in the source.\n    \"\"\"\n    check(start, stop)\n    bitmask = _bitmask(start, stop)\n    src = source & bitmask\n    src |= (value &lt;&lt; start) & bitmask\n    return src\n\n@micropython.native\ndef _bitmask(start: int, stop: int) -&gt; int:\n    \"\"\"\n    Generate a bitmask from start to stop bits.\n    \"\"\"\n    res = 0\n    for i in range(start, 1 + stop):\n        res |= 1 &lt;&lt; i\n    return res\n\ndef check(start: int, stop: int):\n    \"\"\"\n    Check if start is less than or equal to stop. Raise ValueError if not.\n    \"\"\"\n    if start &gt; stop:\n        raise ValueError(f\"Invalid start: {start}, stop value: {stop}\")\n\ndef _mpy_bl(value: int) -&gt; int:\n    \"\"\"\n    Calculate the bit length of the value.\n    \"\"\"\n    if 0 == value:\n        return 0\n    return 1 + int(math.log2(abs(value)))\n\nclass BusAdapter:\n    \"\"\"Adapter class for bus communication.\"\"\"\n    def __init__(self, bus: [I2C, SPI]):\n        \"\"\"\n        Initialize the adapter with the bus.\n        \"\"\"\n        self.bus = bus\n\n    def get_bus_type(self) -&gt; type:\n        \"\"\"\n        Return the type of the bus.\n        \"\"\"\n        return type(self.bus)\n\n    def read_register(self, device_addr: [int, Pin], reg_addr: int, bytes_count: int) -&gt; bytes:\n        raise NotImplementedError\n\n    def write_register(self, device_addr: [int, Pin], reg_addr: int, value: [int, bytes, bytearray],\n                       bytes_count: int, byte_order: str):\n        raise NotImplementedError\n\n    def read(self, device_addr: [int, Pin], n_bytes: int) -&gt; bytes:\n        raise NotImplementedError\n\n    def write(self, device_addr: [int, Pin], buf: bytes):\n        raise NotImplementedError\n\n    def write_const(self, device_addr: [int, Pin], val: int, count: int):\n        \"\"\"\n        Write a constant value to the device multiple times.\n        \"\"\"\n        if 0 == count:\n            return\n        bl = _mpy_bl(val)\n        if bl &gt; 8:\n            raise ValueError(f\"The value must take no more than 8 bits! Current: {bl}\")\n        _max = 16\n        if count &lt; _max:\n            _max = count\n        repeats = count // _max\n        b = bytearray([val for _ in range(_max)])\n        for _ in range(repeats):\n            self.write(device_addr, b)\n        remainder = count - _max * repeats\n        if remainder:\n            b = bytearray([val for _ in range(remainder)])\n            self.write(device_addr, b)\n\nclass I2cAdapter(BusAdapter):\n    \"\"\"Adapter class for I2C bus communication.\"\"\"\n    def __init__(self, bus: I2C):\n        super().__init__(bus)\n\n    def write_register(self, device_addr: int, reg_addr: int, value: [int, bytes, bytearray],\n                       bytes_count: int, byte_order: str):\n        \"\"\"\n        Write to the register of the device.\n        \"\"\"\n        buf = None\n        if isinstance(value, int):\n            buf = value.to_bytes(bytes_count, byte_order)\n        if isinstance(value, (bytes, bytearray)):\n            buf = value\n        return self.bus.writeto_mem(device_addr, reg_addr, buf)\n\n    def read_register(self, device_addr: int, reg_addr: int, bytes_count: int) -&gt; bytes:\n        \"\"\"\n        Read from the register of the device.\n        \"\"\"\n        return self.bus.readfrom_mem(device_addr, reg_addr, bytes_count)\n\n    def read(self, device_addr: int, n_bytes: int) -&gt; bytes:\n        \"\"\"\n        Read bytes from the device.\n        \"\"\"\n        return self.bus.readfrom(device_addr, n_bytes)\n\n    def readfrom_into(self, device_addr: int, buf):\n        \"\"\"\n        Read bytes from the device into the buffer.\n        \"\"\"\n        return self.bus.readfrom_into(device_addr, buf)\n\n    def read_buf_from_mem(self, device_addr: int, mem_addr, buf):\n        \"\"\"\n        Read bytes from the device memory into the buffer.\n        \"\"\"\n        return self.bus.readfrom_mem_into(device_addr, mem_addr, buf)\n\n    def write(self, device_addr: int, buf: bytes):\n        \"\"\"\n        Write bytes to the device.\n        \"\"\"\n        return self.bus.writeto(device_addr, buf)\n\n    def write_buf_to_mem(self, device_addr: int, mem_addr, buf):\n        \"\"\"\n        Write bytes to the device memory.\n        \"\"\"\n        return self.bus.writeto_mem(device_addr, mem_addr, buf)\n\nclass SCD4xSensirion(BaseSensor, Iterator):\n    \"\"\"Class for SCD4x Sensirion CO2 sensor.\"\"\"\n    def __init__(self, adapter: I2cAdapter, address=0x62, this_is_scd41: bool = True, check_crc: bool = True):\n        \"\"\"\n        Initialize the sensor with adapter, address, and settings.\n        \"\"\"\n        super().__init__(adapter, address, True)\n        self._buf_3 = bytearray((0 for _ in range(3)))\n        self._buf_9 = bytearray((0 for _ in range(9)))\n        self.check_crc = check_crc\n        self._low_power_mode = False\n        self._single_shot_mode = False\n        self._rht_only = False\n        self._isSCD41 = this_is_scd41\n        self.byte_order = self._get_byteorder_as_str()\n\n    def _get_local_buf(self, bytes_for_read: int) -&gt; [None, bytearray]:\n        \"\"\"\n        Return the local buffer for reading.\n        \"\"\"\n        if bytes_for_read not in (0, 3, 9):\n            raise ValueError(f\"Invalid value for bytes_for_read: {bytes_for_read}\")\n        if not bytes_for_read:\n            return None\n        if 3 == bytes_for_read:\n            return self._buf_3\n        return self._buf_9\n\n    def _to_bytes(self, value, length: int):\n        \"\"\"\n        Convert value to bytes with specified length.\n        \"\"\"\n        byteorder = self.byte_order[0]\n        return value.to_bytes(length, byteorder)\n\n    def _write(self, buf: bytes) -&gt; bytes:\n        \"\"\"\n        Write buffer to the device.\n        \"\"\"\n        return self.adapter.write(self.address, buf)\n\n    def _readfrom_into(self, buf):\n        \"\"\"\n        Read bytes from the device into the buffer.\n        \"\"\"\n        return self.adapter.readfrom_into(self.address, buf)\n\n    def _send_command(self, cmd: int, value: [bytes, None], wait_time: int = 0, bytes_for_read: int = 0,\n                      crc_index: range = None, value_index: tuple = None) -&gt; [bytes, None]:\n        \"\"\"\n        Send a command to the sensor.\n        \"\"\"\n        raw_cmd = self._to_bytes(cmd, 2)\n        raw_out = raw_cmd\n        if value:\n            raw_out += value\n            raw_out += self._to_bytes(_calc_crc(value), 1)\n        self._write(raw_out)\n        if wait_time:\n            time.sleep_ms(wait_time)\n        if not bytes_for_read:\n            return None\n        b = self._get_local_buf(bytes_for_read)\n        self._readfrom_into(b)\n        check_value(len(b), (bytes_for_read,), f\"Invalid buffer length for cmd: {cmd}. Received {len(b)} out of {bytes_for_read}\")\n        if self.check_crc:\n            crc_from_buf = [b[i] for i in crc_index]\n            calculated_crc = [_calc_crc(b[rng.start:rng.stop]) for rng in value_index]\n            if crc_from_buf != calculated_crc:\n                raise ValueError(f\"Invalid CRC! Calculated{calculated_crc}. From buffer {crc_from_buf}\")\n        return b\n\n    def save_config(self):\n        \"\"\"\n        Save the sensor configuration to EEPROM.\n        \"\"\"\n        cmd = 0x3615\n        self._send_command(cmd, None, 800)\n\n    def get_id(self) -&gt; tuple:\n        \"\"\"\n        Get the unique serial number of the sensor.\n        \"\"\"\n        cmd = 0x3682\n        b = self._send_command(cmd, None, 0, bytes_for_read=9,\n                               crc_index=range(2, 9, 3), value_index=(range(2), range(3, 5), range(6, 8)))\n        return tuple([(b[i] &lt;&lt; 8) | b[i+1] for i in range(0, 9, 3)])\n\n    def soft_reset(self):\n        \"\"\"\n        Perform a soft reset of the sensor.\n        \"\"\"\n        return None\n\n    def exec_self_test(self) -&gt; bool:\n        \"\"\"\n        Execute self-test on the sensor. Returns True if successful.\n        \"\"\"\n        cmd = 0x3639\n        length = 3\n        b = self._send_command(cmd, None, wait_time=10_000,\n                               bytes_for_read=length, crc_index=range(2, 3), value_index=(range(2),))\n        res = self.unpack(\"H\", b)[0]\n        return 0 == res\n\n    def reinit(self) -&gt; None:\n        \"\"\"\n        Reinitialize the sensor by reloading user settings from EEPROM.\n        \"\"\"\n        cmd = 0x3646\n        self._send_command(cmd, None, 20)\n\n    def set_temperature_offset(self, offset: float):\n        \"\"\"\n        Set the temperature offset for the sensor.\n        \"\"\"\n        cmd = 0x241D\n        offset_raw = self._to_bytes(int(374.49142857 * offset), 2)\n        self._send_command(cmd, offset_raw, 1)\n\n    def get_temperature_offset(self) -&gt; float:\n        \"\"\"\n        Get the temperature offset from the sensor.\n        \"\"\"\n        cmd = 0x2318\n        b = self._send_command(cmd, None, wait_time=1, bytes_for_read=3, crc_index=range(2, 3), value_index=(range(2),))\n        temp_offs = self.unpack(\"H\", b)[0]\n        return 0.0026702880859375 * temp_offs\n\n    def set_altitude(self, masl: int):\n        \"\"\"\n        Set the altitude for the sensor in meters above sea level.\n        \"\"\"\n        cmd = 0x2427\n        masl_raw = self._to_bytes(masl, 2)\n        self._send_command(cmd, masl_raw, 1)\n\n    def get_altitude(self) -&gt; int:\n        \"\"\"\n        Get the altitude from the sensor in meters above sea level.\n        \"\"\"\n        cmd = 0x2322\n        b = self._send_command(cmd, None, wait_time=1, bytes_for_read=3, crc_index=range(2, 3), value_index=(range(2),))\n        return self.unpack(\"H\", b)[0]\n\n    def set_ambient_pressure(self, pressure: float):\n        \"\"\"\n        Set the ambient pressure for the sensor in Pascals.\n        \"\"\"\n        cmd = 0xE000\n        press_raw = self._to_bytes(int(pressure // 100), 2)\n        self._send_command(cmd, press_raw, 1)\n\n    def force_recalibration(self, target_co2_concentration: int) -&gt; int:\n        \"\"\"\n        Force recalibration of the sensor with the target CO2 concentration.\n        \"\"\"\n        check_value(target_co2_concentration, range(2**16),\n                    f\"Invalid target CO2 concentration: {target_co2_concentration} ppm\")\n        cmd = 0x362F\n        target_raw = self._to_bytes(target_co2_concentration, 2)\n        b = self._send_command(cmd, target_raw, 400, 3, crc_index=range(2, 3), value_index=(range(2),))\n        return self.unpack(\"h\", b)[0]\n\n    def is_auto_calibration(self) -&gt; bool:\n        \"\"\"\n        Check if automatic self-calibration is enabled on the sensor.\n        \"\"\"\n        cmd = 0x2313\n        b = self._send_command(cmd, None, 1, 3, crc_index=range(2, 3), value_index=(range(2),))\n        return 0 != self.unpack(\"H\", b)[0]\n\n    def set_auto_calibration(self, value: bool):\n        \"\"\"\n        Enable or disable automatic self-calibration on the sensor.\n        \"\"\"\n        cmd = 0x2416\n        value_raw = self._to_bytes(value, 2)\n        self._send_command(cmd, value_raw, 1, 3)\n\n    def set_measurement(self, start: bool, single_shot: bool = False, rht_only: bool = False):\n        \"\"\"\n        Start or stop periodic measurements, or perform a single shot measurement.\n        \"\"\"\n        if single_shot:\n            return self._single_shot_meas(rht_only)\n        return self._periodic_measurement(start)\n\n    def _periodic_measurement(self, start: bool):\n        \"\"\"\n        Start or stop periodic measurements.\n        \"\"\"\n        wt = 0\n        if start:\n            cmd = 0x21AC if self._low_power_mode else 0x21B1\n        else:\n            cmd = 0x3F86\n            wt = 500\n        self._send_command(cmd, None, wt)\n        self._single_shot_mode = False\n        self._rht_only = False\n\n    def get_meas_data(self) -&gt; tuple:\n        \"\"\"\n        Get the measurement data from the sensor (CO2, temperature, and humidity).\n        \"\"\"\n        cmd = 0xEC05\n        val_index = (range(2), range(3, 5), range(6, 8))\n        b = self._send_command(cmd, None, 1, bytes_for_read=9,\n                               crc_index=range(2, 9, 3), value_index=val_index)\n        words = [self.unpack(\"H\", b[val_rng.start:val_rng.stop])[0] for val_rng in val_index]\n        return words[0], -45 + 0.0026703288 * words[1], 0.0015259022 * words[2]\n\n    def is_data_ready(self) -&gt; bool:\n        \"\"\"\n        Check if the measurement data is ready to be read from the sensor.\n        \"\"\"\n        cmd = 0xE4B8\n        b = self._send_command(cmd, None, 1, 3, crc_index=range(2, 3), value_index=(range(2),))\n        return bool(self.unpack(\"H\", b)[0] & 0b0000_0111_1111_1111)\n\n    @micropython.native\n    def get_conversion_cycle_time(self) -&gt; int:\n        \"\"\"\n        Get the conversion cycle time of the sensor in milliseconds.\n        \"\"\"\n        if self.is_single_shot_mode and self.is_rht_only:\n            return 50\n        return 5000\n\n    def set_power(self, value: bool):\n        \"\"\"\n        Power up or power down the sensor.\n        \"\"\"\n        if not self._isSCD41:\n            return\n        cmd = 0x36F6 if value else 0x36E0\n        wt = 20 if value else 1\n        self._send_command(cmd, None, wt)\n\n    def _single_shot_meas(self, rht_only: bool = False):\n        \"\"\"\n        Perform a single shot measurement.\n        \"\"\"\n        if not self._isSCD41:\n            return\n        cmd = 0x2196 if rht_only else 0x219D\n        self._send_command(cmd, None, 0)\n        self._single_shot_mode = True\n        self._rht_only = rht_only\n\n    @property\n    def is_single_shot_mode(self) -&gt; bool:\n        \"\"\"\n        Check if the sensor is in single shot mode.\n        \"\"\"\n        return self._single_shot_mode\n\n    @property\n    def is_rht_only(self) -&gt; bool:\n        \"\"\"\n        Check if the sensor is in RHT-only mode.\n        \"\"\"\n        return self._rht_only\n\n    def __iter__(self):\n        return self\n\n    def __next__(self) -&gt; [tuple, None]:\n        \"\"\"\n        Get the next set of measurement data.\n        \"\"\"\n        if self._single_shot_mode:\n            return None\n        if self.is_data_ready():\n            return self.get_meas_data()\n        return None\n\ndef pa_mmhg(value: float) -&gt; float:\n    \"\"\"\n    Convert air pressure from Pascals to millimeters of mercury.\n    \"\"\"\n    return 7.50062E-3 * value\n\ndef crc8(sequence, polynomial: int, init_value: int = 0x00):\n    \"\"\"\n    Calculate CRC-8 checksum for the given sequence.\n    \"\"\"\n    mask = 0xFF\n    crc = init_value & mask\n    for item in sequence:\n        crc ^= item & mask\n        for _ in range(8):\n            if crc & 0x80:\n                crc = mask & ((crc &lt;&lt; 1) ^ polynomial)\n            else:\n                crc = mask & (crc &lt;&lt; 1)\n    return crc\n\ndef check_device_presence(i2c, address):\n    \"\"\"\n    Check if a device with the given address is present on the I2C bus.\n    \"\"\"\n    devices = i2c.scan()\n    return address in devices\n\nAnd once you have that done run the following code below on your device or save it to main.py, whatever you’d like. Note that we power cycle every 6 hours on the device to ensure that we don’t run into memory issues. I encountered a memory leak that became a problem roughly 5 days in and felt that chasing it down was a waste compared to a simple power cycling using machine.reset().\n\n\nMain Code\n\n# main.py\n\nfrom machine import SoftI2C, Pin\nimport time\nfrom SCD41 import SCD4xSensirion, I2cAdapter, check_device_presence\nfrom ssd1306 import SSD1306_I2C  # Ensure you have the SSD1306 library\nimport urequests as requests\nimport network\nimport json\nimport utime\nimport usocket as socket\nimport ssl\nimport ntptime\n\n# ThingSpeak settings\nTHINGSPEAK_API_KEY = 'YOUR_KEY_HERE'\nTHINGSPEAK_URL = 'https://api.thingspeak.com/update'\nTHINGSPEAK_CHANNEL_ID = '00000000'\nTHINGSPEAK_BULK_UPDATE_URL = 'https://api.thingspeak.com/channels/'+str(THINGSPEAK_CHANNEL_ID)+'/bulk_update.json'\nSEND_TO_THINGSPEAK = True\n\nthingspeak_buffer = []  # Buffer for ThingSpeak data\n\n# Google Sheets settings\nSPREADSHEET_ID = 'VERY_LONG_SPREADSHEET_ID_HERE'\nRANGE_NAME = 'Sheet1!A1:C1'\nSHEET_NAME = 'Sheet1'\nGOOGLE_URL = 'https://script.google.com/macros/s/VERY_LONG_URL/exec'\n\n# WiFi settings\nSSID = 'YOUR_SSID'\nPASSWORD = 'WIFI_PSWD'\ndef connect_wifi(ssid, password):\n    wlan = network.WLAN(network.STA_IF)\n    wlan.active(True)\n    wlan.connect(ssid, password)\n    while not wlan.isconnected():\n        time.sleep(1)\n        print(\"Connecting to WiFi...\")\n    print(\"Connected to WiFi\")\n    print(wlan.ifconfig())\n\ndef get_time_chicago():\n    max_retries = 100\n    for attempt in range(max_retries):\n        try:\n            ntptime.settime()\n            current_time = utime.localtime()\n            break\n        except OSError as e:\n            print(f\"Failed to get NTP time, attempt {attempt + 1} of {max_retries}. Error: {e}\")\n            time.sleep(1)\n    else:\n        print(\"Could not get NTP time, proceeding without time synchronization.\")\n        return utime.localtime()\n\n    # Determine if it is daylight saving time (DST)\n    month = current_time[1]\n    day = current_time[2]\n    hour = current_time[3]\n    if (month &gt; 3 and month &lt; 11) or (month == 3 and day &gt;= 8 and hour &gt;= 2) or (month == 11 and day &lt; 1 and hour &lt; 2):\n        is_dst = True\n    else:\n        is_dst = False\n    \n    offset = -6 * 3600 if not is_dst else -5 * 3600\n    local_time = utime.mktime(current_time) + offset\n    return utime.localtime(local_time)\n\n# Function to sound the buzzer\ndef sound_buzzer():\n    for _ in range(5):\n        buzzer.value(1)  # Turn on buzzer\n        time.sleep(0.5)  # 500 ms delay\n        buzzer.value(0)  # Turn off buzzer\n        time.sleep(0.5)  # 500 ms delay\n    \ndef send_data_to_google_sheets(data):\n    url = GOOGLE_URL  # Define your Google URL here\n    headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n    encoded_data = (\n        \"Date=\" + data['date'] +\n        \"&Time=\" + data['time'] +\n        \"&Temp=\" + str(data['temp_f']) +\n        \"&Humidity=\" + str(data['humidity']) +\n        \"&Lux=\" + str(data['lux']) +\n        \"&CCT=\" + str(data['color_temp']) +\n        \"&Soil Moisture=\" + str(data['soil_moisture']) +\n        \"&Soil Temp=\" + str(data['soil_temp']) +\n        \"&R=\" + str(data['r']) +\n        \"&G=\" + str(data['g']) +\n        \"&B=\" + str(data['b']) +\n        \"&HTML=\" + data['html']\n    )\n    try:\n        # Extract host and path from URL\n        _, _, host, path = url.split('/', 3)\n        \n        # Set up a socket connection\n        addr = socket.getaddrinfo(host, 443)[0][-1]\n        s = socket.socket()\n        s.connect(addr)\n        s = ssl.wrap_socket(s)\n        \n        # Create the HTTP request manually\n        request = f\"POST /{path} HTTP/1.1\\r\\nHost: {host}\\r\\n\"\n        request += \"Content-Type: application/x-www-form-urlencoded\\r\\n\"\n        request += f\"Content-Length: {len(encoded_data)}\\r\\n\\r\\n\"\n        request += encoded_data\n\n        # Send the request\n        s.write(request)\n        \n        # Close the socket\n        s.close()\n        print('Data sent to Google Sheets!')\n    except Exception as e:\n        print('Failed to send data to Google Sheets:', e)\n        \ndef send_data_to_thingspeak():\n    \"\"\"Send data to ThingSpeak.\"\"\"\n    if SEND_TO_THINGSPEAK and thingspeak_buffer:\n        if len(thingspeak_buffer) &gt; 1:\n            # Bulk update\n            payload = {\n                'write_api_key': THINGSPEAK_API_KEY,\n                'updates': []\n            }\n            for data in thingspeak_buffer:\n                update = {\n                    'created_at': f\"{data['date']} {data['time']} -0500\",\n                    'field1': data['soil_temp'],\n                    'field2': data['soil_moisture'],\n                    'field3': data['lux'],\n                    'field4': data['color_temp'],\n                    'field5': data['temp_f'],\n                    'field6': data['humidity']\n                }\n                payload['updates'].append(update)\n\n            try:\n                headers = {'Content-Type': 'application/json'}\n                json_data = json.dumps(payload)\n                response = requests.post(THINGSPEAK_BULK_UPDATE_URL, headers=headers, data=json_data)\n                if response.status_code == 202:\n                    print('Data posted to ThingSpeak (bulk update):', response.text)\n                    thingspeak_buffer.clear()  # Clear the buffer after successful update\n                else:\n                    print(f'Failed to send data to ThingSpeak (bulk update): {response.status_code}, {response.text}')\n            except Exception as e:\n                print('Failed to send data to ThingSpeak (bulk update):', e)\n        else:\n            data = thingspeak_buffer.pop(0)  # Get the first item in the buffer\n            payload = {\n                'api_key': THINGSPEAK_API_KEY,\n                'field1': data['soil_temp'],\n                'field2': data['soil_moisture'],\n                'field3': data['lux'],\n                'field4': data['color_temp'],\n                'field5': data['temp_f'],\n                'field6': data['humidity']\n            }\n            try:\n                response = requests.post(THINGSPEAK_URL, json=payload)\n                if response.status_code == 200:\n                    print('Data posted to ThingSpeak:', response.text)\n                else:\n                    print(f'Failed to send data to ThingSpeak: {response.status_code}, {response.text}')\n            except Exception as e:\n                print('Failed to send data to ThingSpeak:', e)\n\n# Pin definitions\nDHT_PIN = Pin(27)\nDHT_POWER_PIN = Pin(26, Pin.OUT)\n# SEESAW_POWER_PIN = Pin(25, Pin.OUT)\nTCS34725_POWER_PIN = Pin(33, Pin.OUT)\nOLED_POWER_PIN = Pin(32, Pin.OUT)\nTCS34725_LED_PIN = Pin(23, Pin.OUT)  # GPIO 23 for TCS34725 LED control\n\n# Setup GPIO for Buzzer\nBUZZER_GPIO = 13  # Use GPIO 25 for the buzzer\nbuzzer = machine.Pin(BUZZER_GPIO, Pin.OUT)\n\n# Initialize separate I2C buses\ni2c_oled = SoftI2C(scl=Pin(22), sda=Pin(21))\ni2c_seesaw = SoftI2C(scl=Pin(16), sda=Pin(17))\ni2c_tcs = SoftI2C(scl=Pin(18), sda=Pin(19))\n\n# Scan I2C buses\nOLED_POWER_PIN.value(1)\nprint('Scanning I2C bus for OLED...')\ndevices_oled = i2c_oled.scan()\nOLED_POWER_PIN.value(0)\nprint('OLED I2C devices:', [hex(device) for device in devices_oled])\ntime.sleep(1)\n\nprint('Scanning I2C bus for Seesaw...')\n# SEESAW_POWER_PIN.value(1)\ndevices_seesaw = i2c_seesaw.scan()\n# SEESAW_POWER_PIN.value(0)\nprint('Seesaw I2C devices:', [hex(device) for device in devices_seesaw])\ntime.sleep(1)\n\nprint('Scanning I2C bus for TCS34725...')\nTCS34725_POWER_PIN.value(1)\nTCS34725_LED_PIN.value(0)  # Start with the LED off\ndevices_tcs = i2c_tcs.scan()\nprint('TCS34725 I2C devices:', [hex(device) for device in devices_tcs])\n\n# Initialize DHT22 sensor and OLED display\nDHT_POWER_PIN.value(1)\ndht_sensor = DHT22(DHT_PIN)\nDHT_POWER_PIN.value(0)\n\nOLED_POWER_PIN.value(1)\noled = SSD1306_I2C(128, 64, i2c_oled)\n\n# Ensure the addresses are correct (default addresses used)\nif 0x36 in devices_seesaw:\n    soil_sensor = StemmaSoilSensor(i2c_seesaw)\nelse:\n    print(\"Seesaw sensor not found!\")\n    \nif 0x39 in devices_tcs:\n    tcs = TCS34725(i2c_tcs)\nelse:\n    print(\"TCS34725 sensor not found!\")\n    \n# # Function to adjust gain and integration time based on Lux value\n# def adjust_tcs_settings(tcs, lux):\n#     print(lux)\n#     if lux &gt; 10000:\n#         integration_time = 50  # Short integration time\n#         gain = 1  # Low gain\n#     elif lux &gt; 1000:\n#         integration_time = 100  # Moderate integration time\n#         gain = 1  # Moderate gain\n#     elif lux &gt; 100:\n#         integration_time = 150  # Long integration time\n#         gain = 4  # High gain\n#     else:\n#         integration_time = 200  # Longest integration time\n#         gain = 16  # Highest gain\n# \n#     tcs.integration_time(integration_time)\n#     tcs.gain(gain)\n#     print(f'Adjusted TCS34725 settings: Integration Time = {integration_time} ms, Gain = {gain}')\n\n# Function to reinitialize the TCS34725 sensor with retries and initial Lux adjustment\ndef reinitialize_tcs_sensor(i2c_tcs, retries=9):\n    for attempt in range(retries):\n        try:\n            tcs = TCS34725(i2c_tcs)\n            tcs.gain(4)\n            tcs.integration_time(150)\n            time.sleep(1)\n            tcs.active(True)\n            tcs.led(False)  # Ensure LED is off\n            time.sleep(1)  # Wait for the sensor to stabilize\n            cct, lux = tcs.read()  # Get initial readings to dynamically adjust Lux values\n            cct, lux = tcs.read()  # Get initial readings to dynamically adjust Lux values\n            print(\"Intial CCT:\",cct)\n            print(\"Initial Lux:\",lux)\n            \n            cct, lux = tcs.read()  # Get initial readings to dynamically adjust Lux values\n#             adjust_tcs_settings(tcs, lux)  # Adjust settings based on Lux\n            print(\"2nd CCT:\",cct)\n            print(\"2nd Lux:\",lux)\n            time.sleep(1)  # Wait for settings to take effect\n            print(f\"TCS34725 sensor initialized on attempt {attempt + 1}\")\n            return tcs, cct, lux\n        except Exception as e:\n            print(f\"Error initializing TCS34725 sensor: {e}, retrying...\")\n            time.sleep(1)\n    raise RuntimeError(\"Failed to initialize TCS34725 sensor after multiple attempts\")\ndef main():\n    # Connect to WiFi\n    connect_wifi(SSID, PASSWORD)\n    count = 0\n    # Main loop\n    while True:\n        try:\n            count = count +1\n            # Read DHT22 sensor\n            print(\"Reading DHT22 sensor...\")\n            DHT_POWER_PIN.value(1)\n            time.sleep(2)  # Wait for the sensor to stabilize\n            dht_sensor.measure()\n            temp_dht22 = dht_sensor.temperature()\n            humidity = dht_sensor.humidity()\n            temp_fahrenheit = int(temp_dht22 * 9 / 5 + 32)\n            humidity = int(humidity)\n            DHT_POWER_PIN.value(0)\n\n            # Read Seesaw (Soil) sensor if it is found\n            if 'soil_sensor' in locals():\n                print(\"Reading Seesaw sensor...\")\n                soil_temp = soil_sensor.get_temp()\n                soil_temp = int(soil_temp * 9 / 5 + 32)\n                soil_moisture = soil_sensor.get_moisture()\n            else:\n                soil_temp = None\n                soil_moisture = None\n\n            # Reinitialize and read TCS34725 sensor\n            print(\"Reinitializing and reading TCS34725 sensor...\")\n            TCS34725_POWER_PIN.value(1)\n            TCS34725_LED_PIN.value(0)  # Turn off the LED\n            tcs, cct, lux = reinitialize_tcs_sensor(i2c_tcs)\n            time.sleep(1)  # Wait for the sensor to stabilize\n\n            try:\n                raw_data = tcs.read(True)\n                print(f\"Raw data: {raw_data}\")\n                r, g, b, c = raw_data\n                cct, lux = tcs.read()\n                print(f\"RGB: ({r}, {g}, {b}), Clear: {c}, CCT: {cct}, Lux: {lux}\")\n                html_rgb_val = tcs.html_rgb(raw_data)\n                html_hex_val = tcs.html_hex(raw_data)\n                print(f\"HTML RGB: {html_rgb_val}, HTML Hex: {html_hex_val}\")\n            except RuntimeError as e:\n                print(\"Error reading TCS34725:\", e)\n                html_hex_val = \"Error\"\n                lux = cct = 0\n\n            TCS34725_LED_PIN.value(0)  # Turn off the LED\n            TCS34725_POWER_PIN.value(0)\n            \n             # Get current date and time\n            current_time = get_time_chicago()\n            date_str = \"{:04}-{:02}-{:02}\".format(current_time[0], current_time[1], current_time[2])\n            time_str = \"{:02}:{:02}:{:02}\".format(current_time[3], current_time[4], current_time[5])\n\n            # Print sensor data to the console\n            print(f'Color: {html_hex_val}')\n            print(f'Lux: {lux}')\n            print(f'CCT: {cct}')\n            print(f'Temp: {temp_fahrenheit} Hum:{humidity}')\n            if soil_temp is not None and soil_moisture is not None:\n                print(f'Soil Temp: {soil_temp} F')\n                print(f'Soil Moisture: {soil_moisture}')\n            # Display data on OLED\n            print(\"Updating OLED display...\")\n            oled.fill(0)\n            if soil_temp is not None:\n                oled.text(f'Soil Temp: {soil_temp}F', 0, 0)\n            if soil_moisture is not None:\n                print(\"Count: \", count)\n                oled.text(f'Moisture: {soil_moisture}', 0, 10)\n                if count &gt; 60:\n                    count = 0\n                    if soil_moisture &lt; 600:\n                        sound_buzzer()\n                        print(\"Buzzer sounded\")\n            oled.text(f'RGB: {int(html_rgb_val[0])},{int(html_rgb_val[1])},{int(html_rgb_val[2])}', 0, 20)\n            oled.text(f'Lux:{int(lux)}', 0, 30)\n            oled.text(f'CCT:{int(cct)}',0,40)\n            oled.text(f'Temp:{temp_fahrenheit}F Hum:{humidity}%', 0, 50)\n            oled.show()\n            print(' ')\n            # Prepare data for ThingSpeak\n            data = {\n                'soil_temp': soil_temp,\n                'soil_moisture': soil_moisture,\n                'lux': lux,\n                'color_temp': cct,\n                'temp_f': temp_fahrenheit,\n                'humidity': humidity\n            }\n            \n            # Prepare data for ThingSpeak\n            thingspeak_buffer.append(data)\n\n            # Send data to ThingSpeak\n            send_data_to_thingspeak()\n            \n            data = {\n                'date': date_str,\n                'time': time_str,\n                'temp_f': temp_fahrenheit,\n                'humidity': humidity,\n                'lux': lux,\n                'color_temp': cct,\n                'soil_moisture': soil_moisture,\n                'soil_temp': soil_temp,\n                'r': int(html_rgb_val[0]),\n                'g': int(html_rgb_val[1]),\n                'b': int(html_rgb_val[2]),\n                'html': html_hex_val\n            }    \n            # Send data to Google Sheets\n            send_data_to_google_sheets(data)\n\n\n\n            # Wait before the next update\n            time.sleep(45)\n\n        except Exception as e:\n            print(\"Error occurred:\", e)\n            time.sleep(5)  # Wait for 5 seconds before retrying\nif __name__ == '__main__':\n    main()\n\nI’m not going to the go too much into the code as the SCD41 chip supports a variety of different modes and unless you do a deep dive into the datasheet and the code for some cursed reason you should be fine with the code above. I was able to log readings fairly close to the reported values for my town and breathing on the device and near the device increased the sensor readings reliably. I did note that the sensor took some time to get back down to baseline and took a bit to get up to the max with me breathing on it. It was roughly 2.5 minutes to get back down to baseline and that’s roughly in line with what we would ideally sample at due to the humidity sampling interval mentioned in the technical specs above. See image below:\n\n\n\nLimitations of the SCD41\nAs mentioned above we will have a slight delay in logging values so the results won’t be instantaneous. Additionally as mentioned by: https://github.com/octaprog7/SCD4x one will notice an increase in temperature reading from the sensor if the interval is less than 15 seconds as the sensor will self heat. Additionally, both the SCD40/41 sensors have an auto-calibrate mode which will take the lowest CO2 value from the past 7 days and assumes it is 400ppm. This can cause sensor drift over time unless one regularly(once a week) exposes the sensor to fresh air. It is possible to turn off this auto-calibration mode and one will note that in the code above[set_auto_calibration]. An anecdote from User Anx2K on the r/ESP32 Reddit mentions that there is roughly a 40ppm(10%) drift year to year if one turns off auto calibration. Calibration takes 5 minutes, but is an inherently manual process. It would be nice to be somewhere near a weather monitoring station and set up this CO2 monitor alongside a MH-Z19B/C and MH-Z1+ which use NDIR and compare the initial and end of 24 hour values and assess drift.\n\n\nApplications of the SCD41\nI am particularly interested in environmental monitoring to assess indoor air quality at a competitive price, allowing users to swap sensors in and out as needed. This flexibility is crucial for adapting to different monitoring requirements without significant additional costs.\nAdditionally, monitoring transient dump flows from businesses during off-hours (midnight to 3 AM) is vital. This involves tracking the release of CO2 and other gaseous components using the MQ-* sensor series. While this approach may not provide quantitative measurements due to various confounding factors, it serves as an initial assessment tool. This preliminary analysis can justify the purchase or lease of more advanced testing equipment, costing under $100 in upfront expenses and requiring only a few hours of labor.\nAnother intriguing experiment involves mapping a town with sensors placed every few blocks to monitor localized CO2 concentrations. This data can be correlated with consumer, commercial, and industrial activities, providing valuable insights into the town’s environmental impact.\nOther applications include monitoring the respiratory activity of plants by measuring CO2 exchange in greenhouse or agricultural research settings. This can offer insights into plant health, photosynthetic efficiency, and growth patterns. Pairing this with R/G/B, UV, and light sensors can help determine several growth parameters. Note that NPK (Nitrogen/Phosphorous/Potassium) sensors are often unreliable based on my research. Therefore, it’s better to use actual chemical testing methods or automate the chemical testing apparatus rather than relying on electronic sensors for these measurements.\nIn biotechnology and biochemical engineering contexts, monitoring CO2 levels in cellular cultures and bioreactors is essential. Maintaining precise CO2 concentrations ensures the optimal growth of microorganisms, which can significantly reduce production costs. This is particularly important for the pharmaceutical industry, biofuel production, and other bio-based manufacturing processes.\n\n\nPersonal Testing\nIn the recent heat wave I noted that it took roughly 30 minutes to air out the house and cut CO2 levels from ~1200ppm to ~650ppm.\n\nIts interesting to note that the temperature jumped only a few degrees over the 30 minutes per below:\n\nThe actual “enclosure” of the sensor is just an old Raspberry Pi 5 Case box, a FLIRC model. The solid wire coming from the main board is hooked up to a smaller board used for data display, switching wifi/calibration on/off, and alerting users to high CO2(1500ppm). Note that I didn’t feel like potentially burning the board or buying a soldering attachment to safely remove the LED so I covered it with some cardboard to remove some of the interference caused by the light on the sensor. See below:\nRaw Boards, no case\n\nWifi/Calibration Screen, with case\n\nNormal Output, with case\n\n\n\nConclusion\nThe SCD41 sensor offers a versatile and accurate solution for CO2 monitoring in various applications. For our particular application, provided we either calibrate it once at the beginning then once more every year we can enjoy accurate CO2 monitoring at a low price."
  },
  {
    "objectID": "posts/Pi-Sensor-Part-2/index.html",
    "href": "posts/Pi-Sensor-Part-2/index.html",
    "title": "Raspberry Pi Sensor Server Project Part 2",
    "section": "",
    "text": "After finally getting everything to work on the database side of things I wanted to explore dashboarding and being able to quickly pull up a visual showing what the current values are for the sensor data. After spending a few hours with Grafana and discovering that it doesn’t play nicely enough with JSON for my tastes(but pretty nicely with SQL type DBs…) I decided to just code up a pretty simple dashboard which shows the last four or so odd hours of sensor data. Its fairly simple using Chart.js and pulling the JSON data from ThingSpeak(last 6,000 entries @ 2s avg sampling = 200 min = 3 hours 20 min). I’m currently “pressure testing” the raspberry pi 4 I have and trying to ensure I have at least a week of uptime before I move to an ESP32 based solution which should let me get the rate of sampling up. The visual is below and I’ll explain the code afterwards:\n\n\n\n\n    \n    \n    Dynamic Plot\n    \n    \n    \n\n\n    Pi Environment Test Data\n    Load Chart\n    \n    Refresh\n    \n    \n    \n\n\n\n\n\nCode\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Dynamic Plot&lt;/title&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/chart.js\"&gt;&lt;/script&gt;\n    &lt;script src=\"https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns\"&gt;&lt;/script&gt;\n    &lt;style&gt;\n        table {\n            width: 100%;\n            border-collapse: collapse;\n            margin-top: 20px;\n        }\n        th, td {\n            border: 1px solid #ddd;\n            padding: 8px;\n            text-align: center;\n        }\n        th {\n            background-color: #f2f2f2;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h2&gt;Pi Environment Test Data&lt;/h2&gt;\n    &lt;button onclick=\"loadChart()\"&gt;Load Chart&lt;/button&gt;\n    &lt;button onclick=\"refreshData()\"&gt;Refresh&lt;/button&gt;\n    &lt;canvas id=\"myChart\" width=\"400\" height=\"200\"&gt;&lt;/canvas&gt;\n    &lt;div id=\"latestValues\"&gt;&lt;/div&gt;\n    &lt;script&gt;\n        async function fetchData() {\n            const response = await fetch('https://api.thingspeak.com/channels/2545447/feeds.json?results=6000');\n            const data = await response.json();\n            return data.feeds;\n        }\n\n        function processData(feeds) {\n            const labels = feeds.map(feed =&gt; new Date(feed.created_at));\n            const tempC = feeds.map(feed =&gt; parseFloat(feed.field1));\n            const tempF = feeds.map(feed =&gt; parseFloat(feed.field2));\n            const humidity = feeds.map(feed =&gt; parseFloat(feed.field3));\n            return { labels, tempC, tempF, humidity };\n        }\n\n        function displayLatestValues(labels, tempC, tempF, humidity) {\n            const latestTime = labels[labels.length - 1];\n            const latestTempC = tempC[tempC.length - 1];\n            const latestTempF = tempF[tempF.length - 1];\n            const latestHumidity = humidity[humidity.length - 1];\n\n            const tableHTML = `\n                &lt;table&gt;\n    &lt;tr&gt;\n        &lt;th&gt;Time&lt;/th&gt;\n        &lt;th&gt;Temperature C&lt;/th&gt;\n        &lt;th&gt;Temperature F&lt;/th&gt;\n        &lt;th&gt;Humidity&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tr&gt;\n        &lt;td&gt;${latestTime.toLocaleDateString('en-US')} ${latestTime.toLocaleTimeString('en-US')}&lt;/td&gt;\n        &lt;td&gt;${latestTempC.toFixed(2)}&lt;/td&gt;\n        &lt;td&gt;${latestTempF.toFixed(2)}&lt;/td&gt;\n        &lt;td&gt;${latestHumidity.toFixed(2)}&lt;/td&gt;\n    &lt;/tr&gt;\n&lt;/table&gt;\n            `;\n\n            document.getElementById('latestValues').innerHTML = tableHTML;\n        }\n\n        async function createChart() {\n            const feeds = await fetchData();\n            const { labels, tempC, tempF, humidity } = processData(feeds);\n\n            const ctx = document.getElementById('myChart').getContext('2d');\n            const myChart = new Chart(ctx, {\n                type: 'line',\n                data: {\n                    labels: labels,\n                    datasets: [\n                        {\n                            label: 'Temperature C',\n                            data: tempC,\n                            borderColor: 'rgba(255, 99, 132, 1)',\n                            backgroundColor: 'rgba(255, 99, 132, 0.2)',\n                            borderWidth: 1,\n                            fill: true\n                        },\n                        {\n                            label: 'Temperature F',\n                            data: tempF,\n                            borderColor: 'rgba(255, 165, 0, 1)',\n                            backgroundColor: 'rgba(255, 165, 0, 0.2)',\n                            borderWidth: 1,\n                            fill: true\n                        },\n                        {\n                            label: 'Humidity',\n                            data: humidity,\n                            borderColor: 'rgba(54, 162, 235, 1)',\n                            backgroundColor: 'rgba(54, 162, 235, 0.2)',\n                            borderWidth: 1,\n                            fill: true\n                        }\n                    ]\n                },\n                options: {\n                    scales: {\n                        x: {\n                            type: 'time',\n                            time: {\n                                unit: 'hour'\n                            }\n                        },\n                        y: {\n                            beginAtZero: true\n                        }\n                    }\n                }\n            });\n\n            displayLatestValues(labels, tempC, tempF, humidity);\n\n            return myChart;\n        }\n\n        let chartInstance;\n\n        async function refreshData() {\n            if (chartInstance) {\n                chartInstance.destroy();\n            }\n            chartInstance = await createChart();\n        }\n\n        async function loadChart() {\n            if (!chartInstance) {\n                chartInstance = await createChart();\n            }\n        }\n\n        window.onload = async () =&gt; {\n            chartInstance = await createChart();\n        };\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nBreaking down the code:\n\n\nCode Summary\n\n\nFetch Data:\n\nfetchData(): Fetches the last 6,000 entries from the ThingSpeak API. The await keyword ensures the function waits for the data to be fetched before moving on.\n\nProcess Data:\n\nprocessData(feeds): Processes the raw data to extract timestamps, temperatures in Celsius and Fahrenheit, and humidity. It returns these as arrays.\n\nDisplay Latest Values:\n\ndisplayLatestValues(labels, tempC, tempF, humidity): Extracts the latest values from the arrays and formats them into an HTML table. The toLocaleDateString and toLocaleTimeString methods format the date and time in MM/DD/YYYY and 12-hour format respectively.\n\nCreate Chart:\n\ncreateChart(): Fetches and processes the data, then uses Chart.js to create a line chart with three datasets: Temperature C, Temperature F, and Humidity. It configures the x-axis to display time and the y-axis to start at zero. It also calls displayLatestValues to update the latest values table.\n\nRefresh Data:\n\nrefreshData(): Destroys the existing chart instance and creates a new one with the latest data.\n\nWindow Onload:\n\nwindow.onload: Ensures the chart is created when the page loads.\n\n\nBy breaking down the code into these sections, we can see how each part works together to fetch data, process it, display it in a chart, and show the latest values in a table. This simple dashboard provides a visual representation of sensor data over the last few hours, making it easier to monitor and analyze the data in real time.\n\nOverall, this is a lot more lightweight than Grafana and given the fact that I’m not generating large amounts of traffic to Thingspeak this works very well as a simple logger/check-in for myself. I’m sure there’s a lot more convoluted ways of checking on the sensors, but this web interface allows me to quickly assess if I have downtime or the sensors are toast.\nAdditionally, I downloaded RaspController on iOS to quickly check on my sensors when at home. Overall, the ads were a bit intrusive and even reading from the simple DHT11 sensor was a bit goofy with the sensor readout being read then a few seconds later the readings disconnecting. I believe there’s a PRO version, but I doubt the functionality boost warrants the price as the web display is robust enough.\nRegardless I plan on implementing an ESP32 based solution moving forward. Some comparisons are noted below based on what I’ve read online, but of course that can change once experience/pain sets in. I’m assuming that the 2s delay I’m experiencing will get worse as I add more sensors so I’m assuming the worst. I also plan on implementing the sensors outlined in my first raspberry pi post.\n\nOverview of Sensors\n\nPMS5003: Measures particulate matter (PM2.5 and PM10) with a typical update interval of 1 second.\nMH-Z19: Measures CO2 levels with an update interval of every 2-3 seconds.\nDHT22: Measures temperature and humidity with a sampling rate of 2 seconds.\nBME680: Measures temperature, humidity, pressure, and VOC, with data output intervals ranging from 1 to 3 seconds.\nMQ135: Measures air quality (VOC levels), requires analog-to-digital conversion and calibration for accurate readings.\n\n\n\n1. Sampling Speed and Sensor Handling\n\nRaspberry Pi:\n\nCapabilities: Equipped with a multi-core CPU and up to 8GB RAM, allowing concurrent handling of multiple sensors.\nPMS5003: Capable of continuous 1-second updates.\nMH-Z19: Efficiently handles 2-3 second intervals.\nDHT22: Manages 2-second intervals smoothly.\nBME680: Processes complex outputs (temperature, humidity, pressure, VOC) every 1-3 seconds.\nMQ135: Utilizes an ADC for continuous data reading.\nOverall Sampling: Can read from all sensors simultaneously, processing and displaying data in real-time.\n\nESP32:\n\nCapabilities: Dual-core processor designed for real-time processing, but might need optimization for simultaneous high-frequency data handling(C++).\nPMS5003: Handles 1 Hz output effectively.\nMH-Z19: Manages 2-3 second intervals efficiently.\nDHT22: Handles 2-second intervals without issues.\nBME680: Efficiently processes complex outputs within the 1-3 second range.\nMQ135: Uses built-in ADC for continuous data but might have fewer channels than an external ADC.\nOverall Sampling: Can handle the cumulative sampling rate with careful resource management.\n\n\n\n\n2. Overhead from Operating System\n\nRaspberry Pi:\n\nOS: Runs a full Linux-based OS (e.g., Raspbian) with a graphical user interface, background processes, and system services.\nOverhead: Significant but manageable, thanks to the Pi’s processing power.\nAdvantages: Multi-threading, extensive libraries, ability to run heavy applications (e.g., databases, servers).\n\nESP32:\n\nOS: Operates with a minimal OS like FreeRTOS or can run bare-metal.\nOverhead: Minimal, making it highly efficient for real-time applications.\nAdvantages: Direct hardware control, low latency, efficient resource use.\n\n\n\n\n3. Speed of Data Transfer Wirelessly\n\nRaspberry Pi:\n\nWi-Fi: Supports 2.4 GHz and 5 GHz bands, with data transfer rates up to several hundred Mbps.\nUse Cases: Suitable for fast, reliable data transfer, streaming large datasets, frequent updates to a remote server.\nBluetooth: Available for short-range communication.\n\nESP32:\n\nWi-Fi: Primarily operates on the 2.4 GHz band, with data rates up to 150 Mbps.\nUse Cases: Adequate for sensor data transmission, suitable for IoT applications.\nBluetooth: Supports Bluetooth (classic and BLE), useful for short-range data transfer and low-power communication.\n\n\n\n\n4. Programming Language Choice\n\nRaspberry Pi:\n\nLanguages: Supports Python, C, C++, JavaScript, Java, and more.\nDevelopment: Python is favored for its simplicity and extensive library support for hardware interaction.\nFlexibility: Extensive libraries and frameworks for data processing, machine learning, web servers.\n\nESP32:\n\nLanguages: Commonly programmed using Arduino IDE (C/C++) or MicroPython.\nDevelopment: Arduino IDE provides robust sensor libraries; MicroPython offers ease of use and rapid prototyping.\nFlexibility: C/C++ for performance and control; MicroPython for quick development and debugging.\n\n\n\n\n5. Cost\n\nRaspberry Pi:\n\nBoard Cost: $10 (Pi Zero) to $35-$75 (Pi 3 or Pi 4 models).\nAdditional Costs: Power supplies, SD cards, cases, peripherals, potentially exceeding $100.\nValue Proposition: Higher cost but extensive capabilities, suitable for complex applications.\n\nESP32:\n\nBoard Cost: Typically $5-$10.\nAdditional Costs: Fewer peripherals needed, reducing overall system cost.\nValue Proposition: Highly cost-effective for simple, cost-sensitive projects.\n\n\n\n\n6. Power Consumption\n\nRaspberry Pi:\n\nConsumption: Higher due to its full-featured OS and higher processing power.\nPower Supply: Requires a stable 5V power supply, typically 2.5A or more.\nSuitability: Best for applications with reliable power sources.\n\nESP32:\n\nConsumption: Low-power design with deep sleep modes and efficient power management.\nPower Supply: Can operate on battery power for extended periods.\nSuitability: Ideal for battery-powered or solar-powered IoT applications.\n\n\n\n\n7. Complexity of Setup\n\nRaspberry Pi:\n\nSetup: Involves OS installation, Wi-Fi configuration, and additional software setup.\nEase of Use: Broad ecosystem and community support for troubleshooting.\nLearning Curve: Higher due to OS complexity.\n\nESP32:\n\nSetup: Simpler, involving firmware flashing and code development.\nEase of Use: Growing community support, good documentation.\nLearning Curve: Lower, especially with Arduino IDE or MicroPython.\n\n\n\n\n8. Integration with Databases\n\nRaspberry Pi:\n\nLocal Databases: Can run MySQL, PostgreSQL, SQLite for local storage and complex queries.\nRemote Databases: Interfaces with cloud databases via APIs.\nUse Cases: Suitable for extensive data storage, processing, and local analytics.\n\nESP32:\n\nRemote Databases: Interfaces with databases through HTTP/HTTPS, MQTT, or APIs.\nLocal Storage: Limited, suitable for buffering data before transmission.\nUse Cases: Best for periodic data transmission to a central server.\n\n\n\n\n9. Scalability and Expandability\n\nRaspberry Pi:\n\nScalability: High, with multiple USB ports, GPIO pins, and support for I2C, SPI, UART.\nExpandability: Can connect multiple sensors, peripherals, and expansion boards (HATs).\nUse Cases: Ideal for larger, complex projects needing scalability.\n\nESP32:\n\nScalability: Moderate, fewer GPIO pins but sufficient for many IoT projects.\nExpandability: Supports I2C, SPI, UART; multiple sensors can be connected.\nUse Cases: Suitable for compact, efficient IoT solutions.\n\n\n\n\n10. Community and Support\n\nRaspberry Pi:\n\nCommunity: Extensive, with many tutorials, forums, and resources.\nSupport: Strong, especially for educational, hobbyist, and professional uses.\nDocumentation: Comprehensive, with official support from the Raspberry Pi Foundation.\n\nESP32:\n\nCommunity: Growing, with many tutorials and forums.\nSupport: Adequate, focused on embedded systems and IoT.\nDocumentation: Good, provided by Espressif and third-party contributors.\n\n\n\n\n11. Real-Time Operating Capabilities\n\nRaspberry Pi:\n\nRTOS: Not typically used, though real-time kernels (PREEMPT-RT) are available for specific applications.\nSuitability: Best for applications where real-time performance is not critical.\n\nESP32:\n\nRTOS: FreeRTOS support, ideal for real-time applications.\nSuitability: Designed for real-time processing, making it suitable for time-sensitive tasks.\n\n\n\n\n12. Security\n\nRaspberry Pi:\n\nSecurity Features: Depends on OS and software; can use advanced security protocols and encryption.\nUse Cases: Suitable for applications requiring robust security measures.\n\nESP32:\n\nSecurity Features: Built-in hardware security features (e.g., secure boot, flash encryption).\nUse Cases: Adequate for secure IoT applications.\n\n\n\n\nSummary\n\nSampling Speed: Both the Raspberry Pi and ESP32 can handle the sensors’ sampling rates, with the Pi having more processing power for higher-frequency data collection.\nOverhead from OS: The Raspberry Pi has more overhead due to its full OS, while the ESP32 operates with minimal overhead, ideal for real-time applications.\nWireless Data Transfer: The Raspberry Pi achieves higher data transfer speeds, but the ESP32’s capabilities are sufficient for sensor data transmission.\nProgramming Language: The Raspberry Pi offers more flexibility, while the ESP32 focuses on C/C++ and MicroPython.\nCost: The ESP32 is significantly cheaper, making it attractive for cost-sensitive projects.\nPower Consumption: The ESP32 is more power-efficient, suitable for battery-powered applications.\nComplexity of Setup: The Raspberry Pi setup is more complex but has broader support, while the ESP32 setup is simpler and more focused.\nIntegration with Databases: The Raspberry Pi can run full database servers locally, while the ESP32 typically relies on remote databases.\nScalability and Expandability: The Raspberry Pi is more scalable and expandable, suitable for larger projects.\nCommunity and Support: The Raspberry Pi has a larger community and more extensive support.\nReal-Time Operating Capabilities: The ESP32 is better suited for real-time applications with its RTOS support.\nSecurity: Both offer security features, but the ESP32 has built-in hardware security for IoT applications.\n\nIn conclusion, the Raspberry Pi offers more computational power and flexibility, making it suitable for complex applications requiring robust data processing and local storage. The ESP32, with its low cost, low power consumption, and efficient real-time processing capabilities, is ideal for simpler, cost-effective, and portable sensor applications. Additionally, the cost of failure for a raspberry pi is quite large while an ESP32 is negligible. I would like to have a real time setup along with a dynamic plot of temp, humidity, etc. for a plant monitoring project. To do this at scale(someone has 50+ plants….) I need to dramatically reduce cost. Additionally, I need to also begin budgeting for a Pi cluster project to learn distributed/parallel computing, which, funny enough, are two classes in my Master’s at UIUC that I would like to take(CS425 and CS484 respectively). Truth be told much of this IoT Raspberry Pi/Arduino type work recently stems from a desire to take the IoT class at UIUC, but that class’ priority relative to other courses such as Machine Learning, Statistical Learning, etc. is dead last and puts me over the credits I need. Additionally, implementing X/Y/Z with this hardware allows me to learn how things work at a fraction of tuition(1/10).\nIdeally the cluster would serve as a way of actually learning parallel computing and also establishing some local web services to make my life easier. The tentative list of local web service projects include:\n\nPlant Monitoring Dashboard\n\nProject: A centralized dashboard to monitor temp, humidity, soil moisture, and other environmental factors for plants of concern.\nTech: I plan on using Flask/Django for the backend, React for the frontend, and a database like PostgreSQL for the DB.\nBenefits: I also plan on implementing some sort of algorithm to determine when there’s a problem and sending push notifications or texts to the user.\n\nPersonal Cloud Storage\n\nSelf hosted cloud storage solution using something like NextCloud to store and share files securely. I might keep this guy local though as the only thing I plan on exposing to the World Wide Web would be a website which uses Cloudflare for security.\n\nLocal Development Server(Local Git)\n\nProject: Development environment for testing and deploying web applications and other software projects.\nTech: I’d likely use Docker for containerized environments, Jenkins for CI/CD pipelines, and Git for version control. I would also deeply expand my knowledge of what’s what with this tech.\nBenefits: This should actually help standardize and streamline my development workflow, let me perform automated testing/deployment, and provide a stable/cheap platform for experimentation with new tech.\n\nMedia Server(Mostly PS4/Samsung TV)\n\nProject: Local media server to host/stream media content like movies/music/photos\nTech: Plex or Jellyfin for media management and streaming.\nBenefits: Centralized media library with near seamless access to my favorite(repeatedly watched) shows.\n\nIoT Device Management\n\nProject: Platform to manage/monitor/update IoT devices deployed throughout the house. Likely temp/humidity/air quality sensors and plant sensors. Possible intrusion detection sensors if I decide to revisit some of my previous projects.\nTechnology: Mosquitto for MQTT communication, InfluxDB for time-series data, and Grafana(if I’m not using JSON’s…) for visualization.\nBenefits: Centralized control and visualization of all IoT devices, real time monitoring, and simplified firmware updates. I will have to write quite a bit of C++ code now that I think about it to ensure the ESP32 works and works fast.\n\nNetwork Monitoring Tool\n\nProject: Basic system to monitor the health/performance of my network.\nTech: Nagios or Zabbix.\nBenefits: Real time alerts on network issues, detailed performance metrics, and insights into network traffic patterns.\n\nPiHole\n\nProject: Pi hold server to provide network wide ad blocking, improve browsing speed, and improve privacy by blocking unwanted ads/trackers. It can also act as a local DNS resolver boosting network performance.\nTech: Raspberry Pi for running the software.\nBenefits: Ad blocking, boost privacy, reduce data usage, reduce DNS query times and overall help speed up my network.\n\nHPC Pi\n\nProject: Build a scalable Raspberry Pi cluster(at least 3) to learn and implement parallel computing, distributed computing, and high performance computing concepts.\nTech: Multiple raspberry pis, networking equipment, cluster management software, and parallel computing libraries(OpenMP/MPI). Plus Kubernetes.\nBenefits: Hands on experience with distributed systems and parallel processing, cost effective way of exploring high performance computing, and gaining marketable skills in cluster setup, management, and scalability.\n\n\nTentatively something like this tutorialwill be followed and result in a three pi setup vs the 8 pi setup below(from the tutorial):\n\nHopefully I have some time before the summer ends and classes begin to experiment. Fingers crossed and my willpower willing, I will take the distributed systems course this fall alongside an intensive statistical learning course!"
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html",
    "href": "posts/OPTICSinPython/OPTICS.html",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "href": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "posts/ESP32_Post_1/index.html",
    "href": "posts/ESP32_Post_1/index.html",
    "title": "Efficient Data Logging with ESP32",
    "section": "",
    "text": "To begin with, we are using an ESP32-Wroom-32 Expressif board purchased from Microcenter for $10. These boards can be found for as low as $4-$5 on Amazon, and more capable boards are available in the $10-$20 range. This makes the ESP32 a cost-effective alternative to the Raspberry Pi, especially when the task is primarily logging sensor data. Additionally, the ESP32 outperforms the Raspberry Pi Pico significantly; it is approximately five times faster with integer arithmetic and 60-70 times faster with floating-point calculations, as demonstrated in this youtube video: https://www.youtube.com/watch?v=zGog29YNLmk&ab_channel=Tomsvideos\nThe code for this project is written in MicroPython. Although rewriting it in C/C++ could potentially yield a 10x performance improvement, the time and effort required to handle compilation issues are not justifiable for this use case. If this were a production environment, opting for C/C++ would be an obvious choice."
  },
  {
    "objectID": "posts/ESP32_Post_1/index.html#what-is-an-esp32-wroom-32-expressif-board",
    "href": "posts/ESP32_Post_1/index.html#what-is-an-esp32-wroom-32-expressif-board",
    "title": "Efficient Data Logging with ESP32",
    "section": "What is an ESP32-WROOM-32 Expressif Board?",
    "text": "What is an ESP32-WROOM-32 Expressif Board?\nThe ESP32-WROOM-32 is a powerful, low-cost Wi-Fi and Bluetooth microcontroller module developed by Espressif Systems. It is designed for a wide range of applications, from simple IoT projects to complex systems requiring wireless connectivity and advanced processing capabilities. Here are some key features and specifications of the ESP32-WROOM-32 board:\n\nDual-Core Processor: The ESP32-WROOM-32 features a dual-core Tensilica Xtensa LX6 microprocessor, with clock speeds up to 240 MHz. This provides ample processing power for a variety of tasks, including real-time data processing and multitasking.\nWireless Connectivity:\n\nWi-Fi: The board supports 802.11 b/g/n Wi-Fi, making it ideal for IoT applications that require internet connectivity. It can operate in both Station and Access Point modes, allowing it to connect to existing networks or create its own.\nBluetooth: It includes Bluetooth 4.2 (BLE and Classic), enabling communication with other Bluetooth devices, such as sensors, smartphones, and peripherals.\n\nMemory:\n\nFlash Memory: The module typically comes with 4 MB of flash memory, used for storing the firmware and other data.\nSRAM: It has 520 KB of on-chip SRAM, providing sufficient space for running programs and handling data.\n\nGPIO and Peripherals:\n\nThe board features numerous General Purpose Input/Output (GPIO) pins, which can be used for interfacing with various sensors, actuators, and other peripherals.\nIt includes a variety of built-in peripherals, such as UART, SPI, I2C, PWM, ADC, and DAC, making it highly versatile for different types of projects.\n\nPower Management:\n\nThe ESP32-WROOM-32 is designed with power efficiency in mind, offering multiple power-saving modes, such as deep sleep and light sleep. This makes it suitable for battery-powered applications where low power consumption is crucial.\n\nDevelopment Environment:\n\nThe board is compatible with popular development environments like Arduino IDE, PlatformIO, and Espressif’s own ESP-IDF (Espressif IoT Development Framework). This flexibility allows developers to choose their preferred tools and programming languages.\n\n\nThe ESP32-WROOM-32 board is widely used in various applications, including Internet of Things (IoT), industrial automation, home automation, robotics, health monitoring, and educational projects. It enables the creation of smart home devices, environmental monitoring systems, wearable health trackers, and remote industrial monitoring solutions. Additionally, it is suitable for controlling autonomous robots and drones, developing smart appliances, and building voice assistants. Its versatility makes it an excellent choice for learning and prototyping in STEM education, providing a hands-on experience with microcontrollers, IoT, and embedded systems.\nHowever, the ESP32-WROOM-32 has some limitations that need to be considered. Its processing power and memory are limited compared to more powerful systems, which can be a constraint for complex applications. While it offers various power-saving modes, its power consumption is higher than simpler microcontrollers, making it less ideal for ultra-low-power applications. The board’s real-time performance may not meet the needs of highly time-sensitive tasks, and its limited GPIO pins might require additional hardware for larger projects. Additionally, it may not be suitable for extreme environmental conditions without protective measures, and its complexity can present a steep learning curve for beginners.\nThe datasheet for the ESP32 can be found here.\n\nCPU and Internal Memory\nESP32-D0WDQ6 contains two low-power Xtensa® 32-bit LX6 microprocessors. The internal memory includes:\n\n448 KB of ROM for booting and core functions.\n520 KB of on-chip SRAM for data and instructions.\n8 KB of SRAM in RTC, which is called RTC FAST Memory and can be used for data storage; it is accessed by the main CPU during RTC Boot from the Deep-sleep mode.\n8 KB of SRAM in RTC, which is called RTC SLOW Memory and can be accessed by the co-processor during the Deep-sleep mode.\n1 Kbit of eFuse: 256 bits are used for the system (MAC address and chip configuration) and the remaining 768 bits are reserved for customer applications, including flash-encryption and chip-ID.\n\nGet the esptool via pip:\npip install esptool\nSee usage:\nesptool\nNext find your port after plugging in your ESP32 device via USB:\nIn Windows at least its Device Manager -&gt; Ports(Com & LPT) and look for a device named USB-Serial CH340 or Silicon Labs CP210x USB to UART Bridge or similar. I had two devices in my ports so I noted the ports in use, unplugged the board, then plugged it back in to get the port. This of course didn’t work and I had to add the COM port manually…\nAdding COM ports manually.\n\nOpen Device Manager on your computer.\nClick on the Action option from menu bar.\nChoose Add legacy hardware from the menu to open the Add Hardware window.\nClick on the Next button to move on.\nCheck Install the hardware that I manually select from a list (Advanced) and press Next.\nSelect Ports (COM & LPT) from the given list and press the Next button.\nChoose Standard port types option or the manufacturer for the ports; then, click Next.\nClick on the Finish button to complete.\n\nYou’ll note a new COM port, in my case COM4 and that’s what you’ll need for the next step.\nFollow this guide if you’re not seeing things or some other nonsense: https://docs.espressif.com/projects/esp-idf/en/stable/esp32/get-started/establish-serial-connection.html\nIf you’re experiencing driver issues, this resource might help:\nhttps://www.silabs.com/developers/usb-to-uart-bridge-vcp-drivers?tab=overview\nI used the “with serial enumeration” file, and it worked well for me. The device was recognized and assigned to COM4, which I then used for my setup.\nAfter installing PuTTY, everything worked smoothly. Using both the drivers and PuTTY resolved my issues, reminding me to be more patient and consult the documentation before rushing. If you’ve already flashed something, follow the steps to reset while monitoring COM# on PuTTY. You should see the download mode activate. Reset the device by holding the Boot button and pressing the reset button, then holding Boot while flashing."
  },
  {
    "objectID": "posts/ESP32_Post_1/index.html#getting-started-with-micropython",
    "href": "posts/ESP32_Post_1/index.html#getting-started-with-micropython",
    "title": "Efficient Data Logging with ESP32",
    "section": "Getting Started with MicroPython",
    "text": "Getting Started with MicroPython\nFor the most part refer to the instructions at: https://docs.micropython.org/en/latest/esp32/tutorial/intro.html\nDownload firmware for your ESP32 board:\nhttps://micropython.org/download/#esp32\nSpecifically the Microcenter Inland WROOM Board:\nhttps://micropython.org/download/ESP32_GENERIC/\nEnsure your device is erased with:\nesptool - p COM4 erase_flash\nTake care to replace the ‘COM4’ with your port.\nNext flash MicroPython to the board:\nesptool.py --chip esp32 --port COM4 --baud 460800 write_flash -z 0x1000 esp32-20190125-v1.10.bin\nMake sure you replace the .bin file with the file you downloaded and ensure you’re in the correct directory.\nNext I got up and running with Thonny(https://thonny.org/). Its a very lightweight Python IDE that’s ESP32 friendly. Make sure you select your device in the lower right corner and you’ll be up and running.\n\nYou can run a simple print(“Hello World”) to ensure you’re communicating with the device in the shell.\nHere’s a schematic of my board setup, with swappable GPIO pins and an optional LED. I included an LED to provide a quick visual indicator that my board is running the code correctly during stress tests.\n\nAnd the Real Life Version:\n\nPinouts courteousy of https://github.com/natedogg2020/Inland_ESP32-Supplements\nTop:\n\nBottom:\n\nAll pin references are looking from the top and will be either referenced Top Left or Top Right.\n\nDHT11\nGround to Ground Line to Ground at Pin 19(Top Left)\nGPIO15 Power from Pin 4(Top Left)\nGPIO13 Signal from Pin 5(Top Left)\nGPIO Power from Pin 5(Top Right)(I know I should’ve kept it split, but I had to do some nonsense with the power after the wifi cut out due to the power demand)\n\n\nLED\nResistor: 22ohm(Also used 10, but 22 works better to not blind me)\nLED Anode to Ground to Ground Line to Ground at Pin 6(Top Right)\nLED Cathode to Resistor(22 ohm) to GPIO2 Power from Pin 5(Again, I know I should’ve kept it separate. I’m not an EE.)\nNext, you’ll want to test that your device can connect to the internet, read the sensor, and blink the LED. For now, we’ll skip the Google App Script/Vercel PostgreSQL/MongoDB/ThingSpeak integration, and you will encounter errors when transmitting data.\nI’ll provide the code below with explanations of the different parts. To save memory, especially with the Google Apps Script request, I used a lower-level form of POST than standard. This approach is necessary because the Google Response was overloading the memory. Adjust the timeouts according to your needs. Initially, disable the sensors and LED to ensure sufficient power for the wireless connection, then enable the sensors and allow a few seconds for them to register readings.\n\n\nCode\n\nimport network\nimport urequests as requests\nimport time as time_module\nfrom machine import Pin\nimport dht\nimport ntptime\nimport utime\nimport ujson as json\nimport gc\nfrom machine import freq\n\n# Wi-Fi credentials\nSSID = 'YOUR-WIFI-NAME'\nPASSWORD = 'YOUR-WIFI-PASSWORD'\n\n# Google Sheets settings\nSPREADSHEET_ID = 'YOUR-SPREADSHEET-ID'\nRANGE_NAME = 'Sheet1!A1:E1' #NOTE THAT THIS IS FOR DATE TIME HUMIDITY TEMP_F TEMP_C. WILL CHANGE COLUMN DESIGNATION IF YOU ADD/REMOVE DATA.\nSHEET_NAME = 'Sheet1'\nGOOGLE_URL = 'YOUR-GOOGLE-APP-SCRIPT'\n\n# ThingSpeak settings\nTHINGSPEAK_API_KEY = 'THINGSPEAK-API-KEY'\nTHINGSPEAK_URL = 'https://api.thingspeak.com/update'\nTHINGSPEAK_CHANNEL_ID = 0000000 # Replace with your ThingSpeak channel ID\nTHINGSPEAK_BULK_UPDATE_URL = 'https://api.thingspeak.com/channels/'+str(THINGSPEAK_CHANNEL_ID)+'/bulk_update.json'\n\n# MongoDB settings\nMONGODB_API_URL = 'YOUR_MONGO_DB_URL'\n#MONGODB_API_KEY = 'your_mongodb_api_key' #WE ARE NOT USING PYMONGO, DO NOT NEED.\nMONGODB__VERCEL_API_URL = 'YOUR-VERCEL-URL/api/sensorMongoDB'\n# Vercel settings\nVERCEL_API_URL = 'YOUR-VERCEL-URL/api/sensor'\nVERCEL_API_KEY = 'YOUR-VERCEL-API-KEY'\n\n# DHT11 sensor setup\nSENSOR_POWER_PIN = 13  # Change this to the pin connected to the power control of the sensor\nSENSOR_DATA_PIN = 15  # Change this to the pin connected to the data pin of the sensor\nLED_PIN = 2  # GPIO pin for the LED\n\n# Initialize the sensor power pin and LED pin\nsensor_power_pin = Pin(SENSOR_POWER_PIN, Pin.OUT)\nsensor_data_pin = Pin(SENSOR_DATA_PIN)\nled = Pin(LED_PIN, Pin.OUT)\n\n# Buffers to store data\ndata_buffer_vercel = []\ndata_buffer_mongodb = []\nthingspeak_buffer = []  # Buffer for ThingSpeak data\n\n# Control flags\nSEND_TO_VERCEL = True\nSEND_TO_GOOGLE_SHEETS = True\nSEND_TO_THINGSPEAK = True\nSEND_TO_MONGODB = True\n\ndef connect_wifi(ssid, password):\n    wlan = network.WLAN(network.STA_IF)\n    wlan.active(True)\n    wlan.connect(ssid, password)\n    while not wlan.isconnected():\n        time_module.sleep(1)\n        print(\"Connecting to WiFi...\")\n    print(\"Connected to WiFi\")\n    print(wlan.ifconfig())\n\n# Function to disable sensors\ndef disable_sensors():\n    sensor_power_pin.value(0)  # Turn off sensor by setting the power pin low\n\n# Function to enable sensors\ndef enable_sensors():\n    sensor_power_pin.value(1)  # Turn on sensor by setting the power pin high\n    time_module.sleep(2)  # Wait for the sensor to stabilize    \n# Function to get system status\ndef get_system_status(firstRun):\n    free_heap = gc.mem_free()\n    total_heap = gc.mem_alloc() + free_heap\n    free_heap_percent = (free_heap / total_heap) * 100\n    if firstRun == True:\n        print(f\"Total heap memory: {total_heap} bytes\")\n        # Additional information about the system\n        print(f\"Frequency: {freq()} Hz\")\n    print(f\"Free heap memory: {free_heap} bytes ({free_heap_percent:.2f}%)\")\n    \ndef get_time_chicago():\n    max_retries = 100\n    for attempt in range(max_retries):\n        try:\n            ntptime.settime()\n            current_time = utime.localtime()\n            break\n        except OSError as e:\n            print(f\"Failed to get NTP time, attempt {attempt + 1} of {max_retries}. Error: {e}\")\n            time_module.sleep(1)\n    else:\n        print(\"Could not get NTP time, proceeding without time synchronization.\")\n        return utime.localtime()\n\n    # Determine if it is daylight saving time (DST)\n    month = current_time[1]\n    day = current_time[2]\n    hour = current_time[3]\n    if (month &gt; 3 and month &lt; 11) or (month == 3 and day &gt;= 8 and hour &gt;= 2) or (month == 11 and day &lt; 1 and hour &lt; 2):\n        is_dst = True\n    else:\n        is_dst = False\n    \n    offset = -6 * 3600 if not is_dst else -5 * 3600\n    local_time = utime.mktime(current_time) + offset\n    return utime.localtime(local_time)\n\ndef read_sensor():\n    try:\n        led.off()\n        sensor = dht.DHT11(sensor_data_pin)\n        time_module.sleep(1) #the DHT11 sensor takes 1 second\n        sensor.measure()\n        led.on()\n        temp = sensor.temperature()\n        hum = sensor.humidity()\n        return temp, hum\n    except OSError as e:\n        print(\"Failed to read sensor. Exception: \", e)\n        return None, None\n# Read the access token from the file uploaded earlier\ndef read_access_token():\n    with open('access_token.txt', 'r') as token_file:\n        return token_file.read().strip()\n\n# ACCESS_TOKEN = read_access_token()\n# print(ACCESS_TOKEN)\n\nimport usocket as socket\nimport ssl\ndef send_data_to_google_sheets(temp_c, temp_f, humidity, time_str, date_str):\n#     print(time_module.time())\n    url = GOOGLE_URL  # Define your Google URL here\n    headers = {\n        'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    data = {\n        'Date': date_str,\n        'Time': time_str,\n        'Humidity %': humidity,\n        'Temp F': temp_f,\n        'Temp C': temp_c\n    }\n#     print(time_module.time())\n    # Construct the URL-encoded string manually\n    encoded_data = (\n        \"Date=\" + date_str +\n        \"&Time=\" + time_str +\n        \"&Humidity %=\" + str(humidity) +\n        \"&Temp F=\" + str(temp_f) +\n        \"&Temp C=\" + str(temp_c)\n     ) \n    try:\n        # Extract host and path from URL\n        _, _, host, path = url.split('/', 3)\n        \n        # Set up a socket connection\n        addr = socket.getaddrinfo(host, 443)[0][-1]\n        s = socket.socket()\n        s.connect(addr)\n        s = ssl.wrap_socket(s)\n        \n        # Create the HTTP request manually\n        request = f\"POST /{path} HTTP/1.1\\r\\nHost: {host}\\r\\n\"\n        request += \"Content-Type: application/x-www-form-urlencoded\\r\\n\"\n        request += f\"Content-Length: {len(encoded_data)}\\r\\n\\r\\n\"\n        request += encoded_data\n\n        # Send the request\n        s.write(request)\n        \n#         # Read the response\n#         response = s.read(1024)  # Read up to 2048 bytes from the response, THIS TAKES A WHILE, SET TO WHATEVER. 128-1024\n#         print('Data sent to Google Sheets:', response)  \n        # Close the socket\n        s.close()\n        print('Data sent to Google Sheets!')\n#         print(time_module.time())\n    except Exception as e:\n        print('Failed to send data to Google Sheets:', e)\n\n# # Function to send data to Google Sheets\n# def send_data_to_google_sheets(temp_c, temp_f, humidity,time_str,date_str):\n#     print(time_module.time())\n#     url = GOOGLE_URL\n#     headers = {\n#         'Content-Type': 'application/x-www-form-urlencoded'\n#     }\n#     # Construct the URL-encoded string manually\n#     print(time_module.time())\n#     encoded_data = (\n#         \"Date=\" + date_str +\n#         \"&Time=\" + time_str +\n#         \"&Humidity %=\" + str(humidity) +\n#         \"&Temp F=\" + str(temp_f) +\n#         \"&Temp C=\" + str(temp_c)\n#     )\n#     print(encoded_data)\n#     print(time_module.time())\n#     print(\"request\")\n#     try:\n#         # Get initial free memory\n#         # Run garbage collection to get a clean slate\n#         gc.collect()\n#         initial_free = gc.mem_free()\n#         response = requests.post(url, data=encoded_data, headers=headers) #DRAGS... Also\n#         # Run garbage collection again\n#         gc.collect()\n#         # Get final free memory\n#         final_free = gc.mem_free()\n# \n#         # Calculate memory used by the variable\n#         memory_used = initial_free - final_free\n#         print('Data sent to Google Sheets:')\n#         print(time_module.time())\n#         print('Size of response: ', memory_used, 'bytes') # Size of response:  46352 bytes. Crazy.... That's like ~4635 date strings.\n#     except Exception as e:\n#         print('Failed to send data to Google Sheets:', e)\n\ndef send_data_to_thingspeak():\n    \"\"\"Send data to ThingSpeak.\"\"\"\n    if SEND_TO_THINGSPEAK and thingspeak_buffer:\n        if len(thingspeak_buffer) &gt; 1:\n            # Bulk update\n            payload = {\n                    'write_api_key': THINGSPEAK_API_KEY,\n                    'updates': []\n                }\n            for data in thingspeak_buffer:\n                update = {\n                    'created_at': f\"{data['date']} {data['time']} -0500\",\n                    'field1': data['temperature_C'],\n                    'field2': data['temperature_F'],\n                    'field3': data['humidityPercent'],\n                    'field4': data['time'],\n                    'field5': data['date']\n                }\n                payload['updates'].append(update)\n\n            try:\n                # Send the bulk update request to ThingSpeak\n                headers = {'Content-Type': 'application/json'}\n                #print(len(thingspeak_buffer))\n                #print(headers)\n                #print(json.dumps(payload))\n                # Convert the data payload to JSON format\n                json_data = json.dumps(payload)\n                response = requests.post(THINGSPEAK_BULK_UPDATE_URL,headers=headers,data=json_data)\n                if response.status_code == 202:\n                    print('Data posted to ThingSpeak (bulk update):', response.text)\n                    thingspeak_buffer.clear()  # Clear the buffer after successful update\n                else:\n                    print(f'Failed to send data to ThingSpeak (bulk update): {response.status_code}, {response.text}')\n            except Exception as e:\n                print('Failed to send data to ThingSpeak (bulk update):', e)\n        else:\n            data = thingspeak_buffer.pop(0)  # Get the first item in the buffer\n            print(data)\n            payload = {\n            'api_key': THINGSPEAK_API_KEY,\n            'field1': data['temperature_C'],\n            'field2': data['temperature_F'],\n            'field3': data['humidityPercent'],\n            'field4': data['time'],\n            'field5': data['date']\n        }\n            try:\n                response = requests.post(THINGSPEAK_URL, json=payload)\n                print('Data posted to ThingSpeak', response.text)\n                print(payload)\n                thingspeak_buffer.clear()  # Clear the buffer after successful update\n            except Exception as e:\n                print('Failed to send data to ThingSpeak:', e)\n                thingspeak_buffer.clear()\ndef send_data_to_mongodb():\n    url = MONGODB__VERCEL_API_URL\n    headers = {\n        'x-api-key': VERCEL_API_KEY,\n        'Content-Type': 'application/json'\n    }\n\n    try:\n        # Convert the data dictionary to a JSON string\n        json_data = json.dumps(data_buffer_mongodb)\n        \n        # Print the request details for debugging\n#         print(\"Sending data to:\", url)\n#         print(\"Headers:\", headers)\n#         print(\"Payload:\", json_data)\n        \n        response = requests.post(url, data=json_data, headers=headers)\n        \n        # Print the response details for debugging\n        print(\"Status Code:\", response.status_code)\n        print(\"Response Text:\", response.text)\n        data_buffer_mongodb.clear()\n    except Exception as e:\n        print(\"Failed to send data to Vercel MongoDB API:\", e)\n        data_buffer_mongodb.clear()\ndef send_data_to_vercel():\n    url = VERCEL_API_URL\n    headers = {\n        'x-api-key': VERCEL_API_KEY,\n        'Content-Type': 'application/json'\n    }\n\n    try:\n        # Convert the data dictionary to a JSON string\n        json_data = json.dumps(data_buffer_vercel)\n        \n        # Print the request details for debugging\n#         print(\"Sending data to:\", url)\n#         print(\"Headers:\", headers)\n#         print(\"Payload:\", json_data)\n        \n        response = requests.post(url, data=json_data, headers=headers)\n        \n        # Print the response details for debugging\n        print(\"Status Code:\", response.status_code)\n        print(\"Response Text:\", response.text)\n    except Exception as e:\n        print(\"Failed to send data to Vercel:\", e)\n\ndef main():\n    firstRun = True\n    enable_sensors()\n    temp,hum=read_sensor()\n    print(temp,hum)\n    led.off()\n    disable_sensors()\n    connect_wifi(SSID, PASSWORD)\n    enable_sensors()\n    last_google_sheets_update = time_module.time()\n    last_thingspeak_update = time_module.time()\n    last_vercel_update = time_module.time()\n    last_mongodb_update = time_module.time()\n    iter = 0\n    while True:\n        try:\n            led.on()\n            enable_sensors() #enable the sensors via GPIO\n            temp_c, humidity = read_sensor() #log readings\n            led.off()\n            disable_sensors()# disable the sensors so wifi transmission doesn't run into power issues.\n            if temp_c is not None and humidity is not None:\n                temp_f = temp_c * 9 / 5 + 32\n                local_time = get_time_chicago()\n                date_str = f\"{local_time[0]}-{local_time[1]:02d}-{local_time[2]:02d}\"\n                time_str = f\"{local_time[3]:02d}:{local_time[4]:02d}:{local_time[5]:02d}\"\n                print(f'[{iter}]Date: {date_str}, Time: {time_str}, Temperature: {temp_c}°C, Humidity: {hum}%, Temperature: {temp_f}°F ')\n    #             print(date_str)\n    #             print(time_str)\n                # Add data to buffers\n                data = {\n                        'date': date_str,\n                        'time': time_str,\n                        'humidityPercent': humidity,\n                        'temperatureFahrenheit': temp_f,\n                        'temperatureCelsius': temp_c\n                    }\n                data_buffer_vercel={\n                    'date': date_str,\n                    'time': time_str,\n                    'humidityPercent': humidity,\n                    'temperatureFahrenheit': temp_f,\n                    'temperatureCelsius': temp_c\n                }\n                thingspeak_buffer.append({\n                    'temperature_C': temp_c,\n                    'temperature_F': temp_f,\n                    'humidityPercent': humidity,\n                    'time': time_str,\n                    'date': date_str\n                })\n                data_buffer_mongodb.append({\n                    'date': date_str,\n                    'time': time_str,\n                    'humidityPercent': humidity,\n                    'temperatureFahrenheit': temp_f,\n                    'temperatureCelsius': temp,\n                })\n                print(\"ThingSpeak Buffer:\",len(thingspeak_buffer),\"|Vercel Buffer:\",len(data_buffer_vercel),\"|MongoDB Buffer:\",len(data_buffer_mongodb))\n                 # Check if it's time to send data to Google Sheets \n                if time_module.time() - last_google_sheets_update &gt;= 5:\n                    \n                    send_data_to_google_sheets(temp_c, temp_f, humidity, time_str, date_str)\n                    last_google_sheets_update = time_module.time()\n\n                # Check if it's time to send data to ThingSpeak\n                if time_module.time() - last_thingspeak_update &gt;= 15:\n                    send_data_to_thingspeak()\n                    last_thingspeak_update = time_module.time()\n\n    #             # Check if it's time to send data to Vercel\n    #Vercel DB no good for this low level stuff. Overflow error and out of memory.\n    #No append operation as a result.\n                if time_module.time() - last_vercel_update &gt;= 3600:\n                    send_data_to_vercel()\n                    last_vercel_update = time_module.time()\n\n                # Check if it's time to send data to MongoDB\n                if time_module.time() - last_mongodb_update &gt;= 15:\n                    send_data_to_mongodb()\n                    last_mongodb_update = time_module.time()\n                led.on()    \n#                 time_module.sleep(1)  # Wait for 1 seconds before logging the next reading. Note sensor sampling times!\n                led.off()\n                get_system_status(firstRun)\n                print(\" \")\n                firstRun = False\n                iter = iter+1\n        except Exception as e:\n            print(f\"Error in main loop: {e}\") # usually some one off memory error. It'll reset while still connected to wifi and everyone will be happy.\n            \nif __name__ == '__main__':\n    main()\n\nAnd a breakdown:\n\n\nCode breakdown\n\n\nImport Necessary Libraries\nFirst, we import the necessary libraries required for the project:\n\nnetwork for managing Wi-Fi connectivity.\nurequests for making HTTP requests to various APIs.\ntime and utime for handling time-related functions.\ndht for interacting with the DHT11 sensor.\nntptime for synchronizing time with an NTP server.\nujson for handling JSON data.\ngc for garbage collection to manage memory.\nmachine for controlling hardware components like GPIO pins.\n\n\n\nWi-Fi and API Credentials\nWe define constants to store Wi-Fi credentials and API details:\n\nSSID and PASSWORD for Wi-Fi network credentials.\nSPREADSHEET_ID, RANGE_NAME, SHEET_NAME, and GOOGLE_URL for Google Sheets integration.\nTHINGSPEAK_API_KEY, THINGSPEAK_URL, THINGSPEAK_CHANNEL_ID, and THINGSPEAK_BULK_UPDATE_URL for ThingSpeak integration.\nMONGODB_API_URL, MONGODB_VERCEL_API_URL for MongoDB integration.\nVERCEL_API_URL and VERCEL_API_KEY for Vercel integration.\n\n\n\nSetting Up GPIO Pins\nWe configure the GPIO pins on the ESP32:\n\nSENSOR_POWER_PIN to control the power to the DHT11 sensor.\nSENSOR_DATA_PIN to read data from the DHT11 sensor.\nLED_PIN to control an LED used for indicating status.\n\n\n\nData Buffers and Control Flags\nBuffers are initialized to temporarily store data before sending it to the respective services:\n\ndata_buffer_vercel, data_buffer_mongodb, and thingspeak_buffer store data for Vercel, MongoDB, and ThingSpeak, respectively.\n\nControl flags (SEND_TO_VERCEL, SEND_TO_GOOGLE_SHEETS, SEND_TO_THINGSPEAK, SEND_TO_MONGODB) determine whether data should be sent to each service.\n\n\nConnecting to Wi-Fi\nThe connect_wifi function manages the connection to the Wi-Fi network:\n\nActivates the WLAN interface.\nConnects to the specified Wi-Fi network using the provided SSID and password.\nContinuously checks the connection status and prints the IP configuration once connected.\n\n\n\nSensor Control Functions\nTwo functions manage the power state of the DHT11 sensor:\n\ndisable_sensors sets the power pin low to turn off the sensor.\nenable_sensors sets the power pin high and waits for the sensor to stabilize.\n\n\n\nSystem Status Function\nThe get_system_status function provides insights into the system’s memory usage and CPU frequency:\n\nCalculates the total and free heap memory.\nPrints the memory statistics and CPU frequency.\n\n\n\nTime Synchronization\nThe get_time_chicago function synchronizes the ESP32’s clock with an NTP server:\n\nAttempts to set the time using NTP up to a maximum number of retries.\nAdjusts the time based on whether daylight saving time (DST) is in effect for the Chicago timezone.\n\n\n\nReading Sensor Data\nThe read_sensor function reads temperature and humidity data from the DHT11 sensor:\n\nMeasures the temperature and humidity.\nReturns the values or None if the reading fails.\n\n\n\nSending Data to Google Sheets\nThe send_data_to_google_sheets function sends sensor data to Google Sheets:\n\nConstructs the data payload and URL-encodes it.\nSends the data using an HTTP POST request.\nHandles errors during the data sending process.\n\n\n\nSending Data to ThingSpeak\nThe send_data_to_thingspeak function sends data to ThingSpeak:\n\nSupports both single data point updates and bulk updates.\nConstructs the payload and sends it using an HTTP POST request.\nHandles errors and clears the buffer after successful updates.\n\n\n\nSending Data to MongoDB via Vercel API\nThe send_data_to_mongodb function sends data to a MongoDB instance via a Vercel API:\n\nConverts the data buffer to JSON.\nSends the data using an HTTP POST request.\nHandles errors and clears the buffer after successful updates.\n\n\n\nSending Data to Vercel API\nThe send_data_to_vercel function sends data to a Vercel API endpoint:\n\nConverts the data buffer to JSON.\nSends the data using an HTTP POST request.\nHandles errors during the data sending process(NO BUFFER DUE TO MEMORY LIMITATIONS AND VERCEL LIMITS).\n\n\n\nMain Function\nThe main function orchestrates the entire process:\n\nInitializes the sensor and connects to Wi-Fi.\nEnters an infinite loop where it periodically reads sensor data, stores it in buffers, and sends it to the configured services.\nControls the LED to indicate the status of operations.\nManages the timing of data sending to ensure that each service receives data at the specified intervals.\nLogs system status and handles errors in the main loop.\nUpon encountering an error, most likely memory related, begins the loop again.\n\n\nTest the code and verify that your circuit is functioning correctly. After that, configure Vercel or another API endpoint and Google App Script. Configuration details for Vercel can be found at: https://jesse-anderson.github.io/Blog/_site/posts/Pi-Sensor-Proj-May-2024/, so I’ll skip that part. Create the JavaScript file(sensorMongoDB.js) below and place it in your /api folder. Ensure all changes are pushed to GitHub.\n\n\nCode\n\nconst { MongoClient } = require('mongodb');\n\nconst API_KEY = process.env.API_KEY; // Retrieve the API key from environment variables\nconst MONGO_URI = process.env.MONGODB_URI; // MongoDB connection string from environment variables\n\nconst MONGODB_DB_NAME = 'Raspberry_Pi'; // Database name\nconst MONGODB_COLLECTION_NAME = 'Readings'; // Collection name\n\nlet client;\n\nconst connectToMongo = async () =&gt; {\n    if (!client) {\n        client = new MongoClient(MONGO_URI, {\n            useNewUrlParser: true,\n            useUnifiedTopology: true,\n        });\n        await client.connect();\n    }\n    return client.db(MONGODB_DB_NAME);\n};\n\nconst handleSensorData = async (req, res) =&gt; {\n    if (req.method !== 'POST') {\n        return res.status(405).json({ error: 'Method not allowed' });\n    }\n\n    try {\n        console.log('Request received');  // For debugging purposes\n\n        // Extract API key from request headers\n        const providedApiKey = req.headers['x-api-key'];\n        console.log('Provided API Key:', providedApiKey);  // For debugging purposes\n\n        // Check if API key is provided and matches the expected API key\n        if (!providedApiKey || providedApiKey !== API_KEY) {\n            return res.status(401).json({ error: 'Unauthorized' });\n        }\n\n        // Extract data from request body\n        const data = req.body;\n\n        // Log the data received to console for verification\n        console.log('Received data:', JSON.stringify(data, null, 2));\n\n        const db = await connectToMongo();\n        const collection = db.collection(MONGODB_COLLECTION_NAME);\n\n        let result;\n\n        if (Array.isArray(data)) {\n            // Insert multiple readings\n            result = await collection.insertMany(data.map(entry =&gt; ({\n                ...entry,\n                temperatureCelsius: entry.temperatureCelsius !== null && entry.temperatureCelsius !== undefined ? entry.temperatureCelsius : 0,\n                temperatureFahrenheit: entry.temperatureFahrenheit !== null && entry.temperatureFahrenheit !== undefined ? entry.temperatureFahrenheit : 0,\n                humidityPercent: entry.humidityPercent !== null && entry.humidityPercent !== undefined ? entry.humidityPercent : 0,\n                date: entry.date !== null && entry.date !== undefined ? entry.date : new Date().toISOString().split('T')[0],\n                time: entry.time !== null && entry.time !== undefined ? entry.time : new Date().toISOString().split('T')[1].split('.')[0]\n            })));\n        } else {\n            // Insert a single reading\n            result = await collection.insertOne({\n                temperatureCelsius: data.temperatureCelsius !== null && data.temperatureCelsius !== undefined ? data.temperatureCelsius : 0,\n                temperatureFahrenheit: data.temperatureFahrenheit !== null && data.temperatureFahrenheit !== undefined ? data.temperatureFahrenheit : 0,\n                humidityPercent: data.humidityPercent !== null && data.humidityPercent !== undefined ? data.humidityPercent : 0,\n                date: data.date !== null && data.date !== undefined ? data.date : new Date().toISOString().split('T')[0],\n                time: data.time !== null && data.time !== undefined ? data.time : new Date().toISOString().split('T')[1].split('.')[0]\n            });\n        }\n\n        console.log('Data stored in MongoDB:', result);\n\n        // Send a successful response back to the client\n        res.status(200).json({ message: 'Data received and stored successfully!', data: result });\n    } catch (e) {\n        // Handle errors and send an error response\n        console.error(\"Error connecting to MongoDB or inserting data:\", e);\n        res.status(500).json({ error: 'Failed to connect to database or insert data', details: e.message });\n    }\n};\n\n// Export the function for Vercel\nmodule.exports = handleSensorData;\n\nI’ll omit breaking down the code as it is similar enough to the code described in the earlier post."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engineering/Science/Tech Blog",
    "section": "",
    "text": "Oak Park Crime Reporting\n\n\n\n\n\n\nData Science\n\n\nData Analytics\n\n\nData Engineering\n\n\n\n\n\n\n\n\n\nJan 9, 2025\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nSCD41: On Demand CO2 Sensor\n\n\n\n\n\n\nESP32\n\n\nIoT\n\n\n\n\n\n\n\n\n\nSep 2, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nESP32 Project: Sensor Reliability/Power Efficiency\n\n\nBatteries/BuckBoost/OLEDs\n\n\n\nESP32\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nEfficient Data Logging with ESP32\n\n\nA Guide to Using Google Sheets/MongoDB/PostgreSQL/ThingSpeak for IoT\n\n\n\nESP32\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nRaspberry Pi Sensor Server Project Part 2\n\n\n\n\n\n\nRaspberry Pi\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nRaspberry Pi Sensor Server Project\n\n\n\n\n\n\nRaspberry Pi\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nOPTICS in Python\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\nML\n\n\nGenerative AI\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to OPTICS\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN Intro\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nDBSCAN\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I launched this blog in April of 2024 as part of a unique project aimed at creating a space for my thoughts and analyses, free from the constraints and subscriptions of platforms like Medium, and distinct from the algorithm-driven feeds of LinkedIn. While there’s certainly more content on the way, I hope this introduction serves as a more engaging placeholder than a simple “This is my blog.”"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n\n\n\nJan 9, 2025\n\n\nOak Park Crime Reporting\n\n\n\n\n\n\n\nSep 2, 2024\n\n\nSCD41: On Demand CO2 Sensor\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nESP32 Project: Sensor Reliability/Power Efficiency\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nEfficient Data Logging with ESP32\n\n\n\n\n\n\n\nMay 18, 2024\n\n\nRaspberry Pi Sensor Server Project Part 2\n\n\n\n\n\n\n\nMay 16, 2024\n\n\nRaspberry Pi Sensor Server Project\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nOPTICS in Python\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nIntro to OPTICS\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nDBSCAN Intro\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nWelcome To My Blog\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/DBSCAN/index.html",
    "href": "posts/DBSCAN/index.html",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm notable for its proficiency in identifying clusters of varying shapes and sizes in large spatial datasets. This algorithm is especially useful in the field of spatiotemporal data analysis, where the goal is often to group similar data points that are in close proximity over time and space. In this blog post, we’ll delve into the mechanics of DBSCAN, discuss its critical parameters, and provide guidance on adjusting these parameters to achieve optimal clustering results for spatiotemporal data.\n\n\nDBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform.\n\n\n\nThe effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n\nCore, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here\n\n\n\n\nSpatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here\n\n\n\n\nScale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data.\n\n\n\n\nDBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/DBSCAN/index.html#what-is-dbscan",
    "href": "posts/DBSCAN/index.html#what-is-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform."
  },
  {
    "objectID": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "href": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "The effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n\nCore, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "href": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Spatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "href": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Scale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data."
  },
  {
    "objectID": "posts/DBSCAN/index.html#conclusion",
    "href": "posts/DBSCAN/index.html#conclusion",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html",
    "href": "posts/OP-Crime-Documentation/index.html",
    "title": "Oak Park Crime Reporting",
    "section": "",
    "text": "Motivation\nHow to Use the Tool\n\nUsing the Live Streamlit Dashboard\nAccessing the Static HTML Maps\nSigning Up for Weekly Email Updates\n\nDocumentation Introduction\nData Parsing\n\nOverview\nKey Components\n\n1. OakPark_Crime_Reporting_Web.py\n2. utils.py\n\nDetailed Code Breakdown\n\nLive Streamlit Dashboard\n\nOverview\nKey Components\n\n1. streamlit_app.py\n\nDetailed Code Breakdown\n\nStatic HTML Generation\n\nOverview\nKey Components\n\n1. weekly_crime_report.py\n\nDetailed Code Breakdown\n\nConclusion\nFuture Enhancements"
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#oak-park-crime-reporting-documentation",
    "href": "posts/OP-Crime-Documentation/index.html#oak-park-crime-reporting-documentation",
    "title": "Oak Park Crime Reporting",
    "section": "",
    "text": "Motivation\nHow to Use the Tool\n\nUsing the Live Streamlit Dashboard\nAccessing the Static HTML Maps\nSigning Up for Weekly Email Updates\n\nDocumentation Introduction\nData Parsing\n\nOverview\nKey Components\n\n1. OakPark_Crime_Reporting_Web.py\n2. utils.py\n\nDetailed Code Breakdown\n\nLive Streamlit Dashboard\n\nOverview\nKey Components\n\n1. streamlit_app.py\n\nDetailed Code Breakdown\n\nStatic HTML Generation\n\nOverview\nKey Components\n\n1. weekly_crime_report.py\n\nDetailed Code Breakdown\n\nConclusion\nFuture Enhancements"
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#Motivation",
    "href": "posts/OP-Crime-Documentation/index.html#Motivation",
    "title": "Oak Park Crime Reporting",
    "section": "Motivation",
    "text": "Motivation\nTracking and analyzing crime data is crucial for fostering safer communities and enabling informed decision-making. While this tool is not designed nor should not be used in any informal or informal decision making per the disclaimers presented within the application it is my hope that it triggers discussion regarding using analytics to enable data driven decision making per my other projects both personal and professional. My journey to develop a comprehensive crime tracking tool for Oak Park was driven by the challenges and inefficiencies I encountered while navigating the Oak Park Police Department’s (OPPD) publicly available resources.\n\nInitial Challenges\nNavigating the OPPD website presented a significant hurdle. The process involved manually accessing PDF files for specific dates, meticulously copying and pasting individual crime reports, and then attempting to locate each incident on a map. This method was not only time-consuming but also prone to errors, making it difficult to:\n\nQuickly Assess Crime Trends: Without an aggregated view, understanding the frequency and distribution of crimes over a given period was arduous.\nIdentify Crime Hotspots: Pinpointing areas with high concentrations of criminal activity lacked precision and was labor-intensive.\nMonitor Crime Progression: Determining whether crime rates were escalating or declining within Oak Park required extensive manual effort.\n\n\n\nEmbarking on a Solution\nDetermined to streamline this process, I embarked on automating data extraction and visualization. The primary objective was to transform unstructured PDF data into a structured format that could be easily analyzed and visualized.\n\n\nOvercoming Technical Hurdles\nThe transition from PDFs to actionable data was fraught with challenges:\n\nData Extraction Complexity:\n\nInitial Approach: I leveraged Regular Expressions (Regex) to parse and extract relevant crime data from the PDFs.\nError Rates: The initial extraction efforts yielded error rates between 20-30%, compromising data reliability.\n\nOptimizing Data Accuracy:\n\nRefining Regex Patterns: Through iterative testing and refinement of Regex patterns, I significantly reduced error rates to 10% and eventually to 5%.\nAdditional Optimizations: Implementing data validation checks and leveraging Python’s robust data processing libraries further enhanced extraction accuracy.\n\n\n\n\nCreating Insightful Visualizations\nWith accurate data in hand, the next step was to visualize it in a meaningful way:\n\nInteractive Maps: Utilizing Folium, I created dynamic maps that plot each crime incident, providing a spatial understanding of criminal activity.\nIdentifying Hotspots: By incorporating Marker Clustering, the maps highlight areas with high crime densities, enabling quick identification of hotspots.\nFiltering Capabilities: Implementing filtering options allows users to view specific types of crimes or incidents within selected time frames, enhancing the map’s utility.\n\n\n\nExpanding Analytical Capabilities\nThe success of the initial visualization opened avenues for further enhancements:\n\nNatural Language Processing (NLP): I recognized the potential to analyze crime narratives using NLP techniques to uncover commonalities and emerging patterns across different incidents.\nTrend Analysis: Incorporating statistical analyses and trend graphs could provide deeper insights into the progression of crime rates over time.\n\nNote: While NLP and advanced trend analyses offer substantial benefits, they are beyond the scope of this documentation and are earmarked for future development.\n\n\nPersonal Satisfaction and Practical Usage\nThe culmination of these efforts resulted in a robust, user-friendly tool that not only alleviates the tedious manual processes but also empowers users with actionable insights into Oak Park’s crime landscape. The tool’s ability to:\n\nStreamline Data Processing: Automates the extraction and visualization of crime data, saving valuable time.\nEnhance Decision-Making: Provides clear visual representations of crime trends and hotspots, facilitating informed community and law enforcement decisions.\nPromote Community Safety: By making crime data more accessible and understandable, it fosters a proactive approach to community safety initiatives.\n\nI am immensely satisfied with the outcome of this project. The tool has become an integral part of my routine, allowing me to stay informed about the evolving crime dynamics in Oak Park efficiently and effectively."
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#how-to-use-the-tool",
    "href": "posts/OP-Crime-Documentation/index.html#how-to-use-the-tool",
    "title": "Oak Park Crime Reporting",
    "section": "How to Use the Tool",
    "text": "How to Use the Tool\nThe Oak Park Crime Reporting tool is designed to provide comprehensive insights into crime data within Oak Park through interactive dashboards, static maps, and automated email updates. This section guides you through the various functionalities and how to effectively utilize them.\n\nUsing the Live Streamlit Dashboard\nThe Live Streamlit Dashboard offers an interactive platform to explore and visualize crime data in real-time. Here’s how to make the most of it:\n\nAccessing the Dashboard:\n\nLocal Deployment: If you’re running the dashboard locally, navigate to the project directory in your terminal and execute:\nstreamlit run streamlit_app.py\nThis command will launch the dashboard in your default web browser.\nHosted Deployment: To access the current free deployment I am using as I don’t particularly want to pay for an AWS instance without funds coming in… access it via the provided URL (e.g., https://op-crime.streamlit.app/).\n\nNavigating the Dashboard:\n\nDisclaimer Agreement: Upon first access, you’ll be presented with a legal disclaimer. Read through the terms and check the agreement box to proceed.\nInteractive Filters:\n\nDate Range: Select the desired date range using the date pickers to filter crime data accordingly.\nOffense Type: Use the multiselect dropdown to filter crimes based on specific offense categories.\n\nVisualizing Data:\n\nMap Display: The right panel displays a dynamic map highlighting crime incidents based on your filters. Click on markers to view detailed information about each incident.\nData Table: Optionally, a table listing all filtered crime records may be available for reference.\n\n\nAdditional Features:\n\nNavigation Links: Access related resources such as the author’s portfolio, blog, and documentation through the top navigation links.\nEmail Subscription: There’s an option to subscribe or unsubscribe from weekly email updates for the latest crime reports. Currently the script is run automatically daily to capture new incidents and a report is emailed to myself and a few other people once weekly.\n\n\n\n\nAccessing the Static HTML Maps\nFor users who prefer or require static reports, the tool generates HTML maps that can be accessed independently of the Streamlit dashboard. Here’s how to access and utilize them:\n\nGenerated Maps:\n\nWeekly Crime Map: Provides a snapshot of crime incidents from the past week.\nCumulative Crime Map: Displays all recorded crime incidents to date.\n\nAccess Methods:\n\nGitHub Pages: The static maps are hosted on GitHub Pages for easy access. Navigate to the respective URLs:\n\nWeekly Map: https://jesse-anderson.github.io/OP-Crime-Maps/crime_map_weekly_YYYY-MM-DD.html\nCumulative Map: https://jesse-anderson.github.io/OP-Crime-Maps/crime_map_cumulative.html\n\nDirect Access: If hosting elsewhere, ensure the HTML files are uploaded to a web-accessible directory and navigate to their URLs.\n\nInteracting with the Maps:\n\nZoom and Pan: Use your browser’s native controls to zoom in/out and pan across the map.\nMarker Details: Click on individual markers to view detailed information about each crime incident, including links to original PDF reports.\n\nEmbedding Maps:\n\nWeb Integration: Embed the HTML maps into other websites or internal dashboards using &lt;iframe&gt; tags or direct links for seamless integration.\n\n\n\n\nSigning Up for Weekly Email Updates\nStay informed with the latest crime reports by subscribing to our weekly email updates. Follow these steps to sign up:\n\nAccess the Subscription Form:\n\nStreamlit Dashboard: Navigate to the “📧 Email Updates” section within the dashboard.\nDirect Link: Alternatively, use the provided Google Forms link: Add me to Weekly Updates\n\nSubmitting Your Email:\n\nSubscription:\n\nForm Fill: Enter your valid email address in the subscription form.\nConfirmation: Upon successful submission, you’ll receive a confirmation email verifying your subscription.\n\nUnsubscription:\n\nForm Fill: To unsubscribe, provide your email address in the unsubscription section of the form.\nConfirmation: A confirmation email will notify you of the successful removal from the mailing list.\n\n\nManaging Preferences:\n\nFrequency: Currently, the tool sends out weekly updates. Future enhancements may include customizable frequencies such as daily or weekly.\nContent: Receive links to the latest weekly and cumulative maps, along with attached CSV reports detailing the most recent crime data.\n\nPrivacy Assurance:\n\nData Handling: Your email address is securely handled and only used for sending crime report updates.\nOpt-Out Anytime: You can unsubscribe at any time without any hassle through the provided unsubscription process."
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#documentation-introduction",
    "href": "posts/OP-Crime-Documentation/index.html#documentation-introduction",
    "title": "Oak Park Crime Reporting",
    "section": "Documentation Introduction",
    "text": "Documentation Introduction\nThe Oak Park Crime Reporting project is designed to streamline the process of tracking, analyzing, and visualizing crime data within Oak Park. The project automates data extraction from the Oak Park Police Department’s (OPPD) reports, processes and cleans the data, visualizes it on interactive maps, and disseminates the information through various channels such as a Streamlit dashboard and static HTML pages. This documentation provides an in-depth look into the project’s components, focusing initially on data parsing."
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#data-parsing",
    "href": "posts/OP-Crime-Documentation/index.html#data-parsing",
    "title": "Oak Park Crime Reporting",
    "section": "Data Parsing",
    "text": "Data Parsing\n\nOverview\nData parsing is the foundational step in the Oak Park Crime Reporting pipeline. It involves extracting relevant information from the OPPD’s PDF reports, cleaning and structuring the data, and preparing it for visualization and analysis. The primary scripts responsible for this phase are:\n\nOakPark_Crime_Reporting_Web.py\nutils.py\n\nThese scripts work in tandem to automate the cumbersome manual process of navigating through PDF files, extracting crime details, and geocoding locations for mapping.\n\n\nKey Components\n\n1. OakPark_Crime_Reporting_Web.py\nPurpose: This script orchestrates the data parsing workflow. It handles downloading PDF reports from the OPPD website, extracting crime data, geocoding locations, managing caches to optimize performance, and committing the processed data to a GitHub repository.\nImports and Dependencies:\n\n\nCode\n\nimport os\nimport re\nimport json\nimport pandas as pd\nfrom pathlib import Path\nimport logging\nfrom collections import defaultdict\nimport time\nfrom datetime import datetime\nimport string\nimport googlemaps\nimport zipfile\n\nfrom utils import (\n    load_env_vars,\n    normalize_location,\n    load_json_cache,\n    save_json_cache,\n    fetch_pdf_links,\n    download_pdf,\n    extract_data_from_pdf,\n    clean_narrative_basic,\n    process_narrative_nlp,\n    parse_date,\n    clean_text,\n    get_lat_long,\n    get_api_call_count,\n    extract_year,\n    git_commit_and_push\n)\n\n\nStandard Libraries: os, re, json, pandas, pathlib, logging, collections, time, datetime, string, zipfile\nThird-Party Libraries: googlemaps\nLocal Utilities: Functions imported from utils.py\n\nMain Functionality: The main() function serves as the entry point, executing the following steps:\n\nEnvironment Setup:\n\nDetermines the script’s directory.\nLoads environment variables from env_vars.txt.\nInitializes the Google Maps client using the API key.\n\nDirectory and Path Configuration:\n\nSets up directories for downloading PDFs, storing data, and caching.\nDefines paths for output CSV and ZIP files.\n\nCache Management:\n\nLoads existing caches to avoid redundant processing.\nTracks already processed complaint numbers to prevent duplicates.\n\nPDF Processing Loop:\n\nFetches PDF links from the OPPD website.\nDownloads each PDF unless it’s already processed and unchanged.\nExtracts crime data from PDFs, handling errors and logging.\n\nData Aggregation and Storage:\n\nCombines new data with existing data, removes duplicates, and sorts by date.\nCompresses the aggregated data into a ZIP file.\n\nLogging and Reporting:\n\nRecords processing statistics and errors.\nSaves logs to a dated log file.\nCalculates and records the error rate in error_rate.txt for the current run.\n\nGitHub Integration:\n\nCommits and pushes the updated data to a GitHub repository.\n\n\nSample Code Snippet:\n\n\nCode\n\ndef main():\n    start_time = time.time()\n    start_time_str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Determine the directory where this script resides\n    script_dir = Path(__file__).parent.resolve()\n\n    # Path to the environment variables file relative to script directory\n    env_file_path = script_dir / \"env_vars.txt\"\n\n    # Load environment variables\n    try:\n        load_env_vars(env_file_path)\n    except FileNotFoundError as e:\n        print(e)\n        return\n    \n    googlemaps_api_key = os.getenv(\"GOOGLEMAPS_API_KEY\")\n    if not googlemaps_api_key:\n        raise ValueError(\"Google Maps API key not found in environment variables.\")\n\n    # Initialize Google Maps client\n    try:\n        gmaps_client = googlemaps.Client(key=googlemaps_api_key)\n    except Exception as e:\n        logging.critical(f\"Failed to initialize Google Maps client: {e}\")\n        print(f\"Failed to initialize Google Maps client: {e}\")\n        return\n\n    # Define paths relative to script directory\n    base_url = 'https://www.oak-park.us/village-services/police-department/police-activity-summary-reports'\n    download_dir = script_dir / 'downloaded_pdfs'\n    data_dir = script_dir / 'data'\n    cache_dir = script_dir / 'cache'\n    data_dir.mkdir(parents=True, exist_ok=True)\n    cache_dir.mkdir(parents=True, exist_ok=True)\n\n    # ...\n\nExplanation:\n\nEnvironment Variables: Critical for configuring API keys and repository paths without hardcoding sensitive information.\nGoogle Maps Client: Essential for geocoding addresses to latitude and longitude coordinates.\nDirectory Setup: Organizes downloaded PDFs, processed data, and caches systematically.\n\n\n\n2. utils.py\nPurpose:\nutils.py contains a collection of helper functions that support the main data parsing operations. These functions handle tasks such as loading environment variables, normalizing location strings, caching mechanisms, PDF link fetching, PDF downloading, text cleaning, date parsing, geocoding, and GitHub file uploads.\nKey Functions:\n\nEnvironment Management:\n\nload_env_vars(file_path): Loads environment variables from a specified file.\n\nData Normalization and Cleaning:\n\nnormalize_location(loc_str): Standardizes location strings for consistency.\nclean_text(text): Cleans raw text extracted from PDFs.\nparse_date(date_str): Parses various date formats into a standardized YYYY-MM-DD format.\n\nCaching Mechanisms:\n\nload_json_cache(cache_path): Loads cache data from a JSON file.\nsave_json_cache(cache_path, data): Saves cache data to a JSON file.\n\nPDF Handling:\n\nfetch_pdf_links(base_url): Scrapes the OPPD website to retrieve PDF report links.\ndownload_pdf(url, download_dir, redownload=False): Downloads PDFs from given URLs.\n\nData Extraction and Processing:\n\nextract_data_from_pdf(file_path, gmaps_client, location_cache, reprocess_locs, existing_complaint_numbers): Extracts structured crime data from PDFs.\nclean_narrative_basic(narrative): Performs basic cleaning on narrative text.\nprocess_narrative_nlp(narrative): Applies NLP techniques to process narrative text.\n\nGeocoding:\n\nget_lat_long(location_string, gmaps_client): Converts location strings to geographical coordinates using the Google Maps API.\nget_api_call_count(): Retrieves the count of API calls made (useful for monitoring rate limits).\n\nGitHub Integration:\n\nupload_file_to_github(file_path, github_repo_path, target_subfolder): Uploads a single file to a specified GitHub repository subfolder.\nupload_files_to_github_batch(file_paths, github_repo_path, target_subfolder): Facilitates batch uploading of multiple files.\ngit_commit_and_push(repo_path, commit_message): Commits and pushes changes to the GitHub repository, handling authentication and potential conflicts.\n\n\nSample Code Snippet:\n\n\nCode\n\ndef extract_data_from_pdf(file_path, gmaps_client, location_cache, reprocess_locs, existing_complaint_numbers):\n    \"\"\"\n    Extract data from PDF, returning (report, log_entries).\n    Only processes complaints not already in existing_complaint_numbers.\n\n    Args:\n        file_path (str or Path): Path to the PDF file.\n        gmaps_client (googlemaps.Client): Initialized Google Maps client.\n        location_cache (dict): Cache of normalized locations to (lat, lng).\n        reprocess_locs (bool): Flag to force reprocessing of locations.\n        existing_complaint_numbers (set): Set of complaint numbers already processed.\n\n    Returns:\n        tuple: (list of report entries, list of log entries)\n    \"\"\"\n    try:\n        reader = PdfReader(file_path)\n        raw_text = \" \".join([page.extract_text() or \"\" for page in reader.pages])\n    except Exception as e:\n        logging.error(f\"Failed to read PDF '{file_path}': {e}\")\n        return [], [f\"Failed to read PDF '{file_path}': {e}\"]\n    text = clean_text(raw_text)\n    base_url_static = 'https://www.oak-park.us/sites/default/files/police/summaries/'\n    # Log a preview of the cleaned text\n    logging.debug(f\"Cleaned Text Preview (first 500 chars): {text[:500]}...\")\n    \n    complaint_pattern = r\"COMPLAINT NUMBER:\\s*(\\d{2}-\\d{5})\"\n    offense_pattern   = r\"OFFENSE:\\s+([A-Z\\s]+)\"\n    date_pattern      = r\"DATE\\(S\\)\\s*:?\\s+([A-Za-z0-9\\s&\\-–—/]+?)(?=\\s+TIME\\(S\\)|\\s+$)\"\n    time_pattern      = r\"TIME\\(S\\):\\s+([\\d:HRS\\s\\-–—]+)\"\n    location_pattern  = r\"LOCATION:\\s+(.+?)(?=\\s+(?:VICTIM/ADDRESS|NARRATIVE|NARRITIVE|NARRTIVE))\"\n    victim_pattern    = r\"VICTIM/ADDRESS:\\s+(.+?)(?=\\s+NARRATIVE|NARRITIVE|NARRTIVE)\"\n    narrative_pattern = r\"NARR(?:ATIVE|ITIVE|TIVE)\\s*:\\s+(.+?)(?=COMPLAINT NUMBER|$)\"\n\n    complaints = re.findall(complaint_pattern, text)\n    offenses   = re.findall(offense_pattern, text)\n    dates      = re.findall(date_pattern, text)\n    times      = re.findall(time_pattern, text)\n    locations  = re.findall(location_pattern, text)\n    victims    = re.findall(victim_pattern, text)\n    narratives = re.findall(narrative_pattern, text)\n    \n    # Clean offenses\n    offenses = [o.replace(\"DATE\", \"\").strip() for o in offenses]\n    \n    report = []\n    log_entries = []\n    time.sleep(0.2)  # Respectful pause for API calls\n    \n    num_entries = len(complaints)\n    logging.debug(f\"Number of complaints found: {num_entries}\")\n    \n    for i in range(num_entries):\n        try:\n            # Safe indexing\n            comp_num = complaints[i] if i &lt; len(complaints) else \"N/A\"\n            \n            # Skip already processed complaints\n            if comp_num in existing_complaint_numbers:\n                logging.info(f\"Skipping already processed Complaint # {comp_num}\")\n                continue\n\n            # Extract other fields\n            offense  = offenses[i].strip() if i &lt; len(offenses) else \"N/A\"\n            time_str = times[i].strip() if i &lt; len(times) else \"N/A\"\n            loc_str  = locations[i].strip() if i &lt; len(locations) else \"N/A\"\n            victim   = victims[i].strip() if i &lt; len(victims) else \"N/A\"\n            narr_raw = narratives[i].strip() if i &lt; len(narratives) else \"N/A\"\n\n            # ...\n\nExplanation:\n\nPDF Reading: Utilizes PyPDF2.PdfReader to extract text from each page of the PDF.\nRegex Patterns: Defined to capture specific sections like Complaint Number, Offense, Date, Time, Location, Victim Address, and Narrative. Of note is capturing different variations of date and of capturing various spellings.\nData Extraction: Uses re.findall to extract relevant data based on the defined patterns.\nData Cleaning: Processes extracted data to ensure consistency and accuracy.\nDuplicate Handling: Skips complaints that have already been processed to prevent duplication.\n\n\n\nDetailed Code Breakdown\nLet’s delve deeper into some of the critical functions within utils.py to understand their roles and implementations.\n\na. Environment Variables Loading\n\n\nCode\n\ndef load_env_vars(file_path):\n    \"\"\"\n    Load environment variables from a file and set them in os.environ.\n\n    Args:\n        file_path (str or Path): The path to the environment variables file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n    \"\"\"\n    env_file = Path(file_path)\n    if not env_file.exists():\n        raise FileNotFoundError(f\"Environment file '{file_path}' not found.\")\n    \n    with env_file.open('r') as f:\n        for line in f:\n            # Remove leading/trailing whitespace\n            line = line.strip()\n            # Skip empty lines and comments\n            if not line or line.startswith('#'):\n                continue\n            # Split into key and value\n            if '=' in line:\n                key, value = line.split('=', 1)\n                key = key.strip()\n                value = value.strip()\n                os.environ[key] = value\n                print(f\"Loaded environment variable\")  \n            else:\n                print(f\"Ignoring invalid line in env file\")  \n\nFunctionality:\n\nPurpose: Reads a file containing environment variables and sets them in the os.environ dictionary for use throughout the application.\nError Handling: Raises a FileNotFoundError if the specified environment file does not exist.\nParsing Logic:\n\nIgnores empty lines and lines starting with # (comments).\nSplits each valid line into a key-value pair based on the = delimiter.\nTrims whitespace and sets the environment variable.\n\n\nUsage: This function ensures that sensitive information like API keys and repository paths are not hardcoded into the scripts but are instead loaded securely from an external file.\n\n\nb. Location Normalization\n\n\nCode\n\ndef normalize_location(loc_str):\n    \"\"\"\n    Normalize the location string to ensure consistency in caching.\n    \n    Steps:\n    - Convert to lowercase.\n    - Remove leading/trailing whitespace.\n    - Remove punctuation.\n    - Replace multiple spaces with a single space.\n    - Standardize common street suffixes.\n    \n    Args:\n        loc_str (str or float): The original location string.\n\n    Returns:\n        str: The normalized location string.\n    \"\"\"\n    if not isinstance(loc_str, str):\n        if pd.isna(loc_str):\n            loc_str = \"\"\n        else:\n            loc_str = str(loc_str)\n    \n    if not loc_str:\n        return \"\"\n    \n    # Convert to lowercase\n    loc_str = loc_str.lower()\n    # Remove leading/trailing whitespace\n    loc_str = loc_str.strip()\n    # Remove punctuation\n    loc_str = loc_str.translate(str.maketrans('', '', string.punctuation))\n    # Replace multiple spaces with a single space\n    loc_str = re.sub(r'\\s+', ' ', loc_str)\n    # Standardize suffixes\n    loc_str = standardize_suffix(loc_str)\n    return loc_str\n\nFunctionality:\n\nPurpose: Ensures that location strings are consistently formatted to improve caching efficiency and reduce redundancy.\nNormalization Steps:\n\nType Handling: Converts non-string inputs to strings, handling missing values gracefully.\nCase Conversion: Transforms the string to lowercase to ensure case-insensitive matching.\nWhitespace Trimming: Removes unnecessary leading and trailing spaces.\nPunctuation Removal: Strips out all punctuation to avoid discrepancies caused by different punctuation marks.\nWhitespace Reduction: Collapses multiple spaces into a single space for uniformity.\nSuffix Standardization: Converts common street suffixes (e.g., “st” to “street”) to a standardized form.\n\n\n\n\nc. PDF Link Fetching\n\n\nCode\n\ndef fetch_pdf_links(base_url):\n    \"\"\"\n    Fetch PDF links from Oak Park site.\n\n    Args:\n        base_url (str): The base URL to fetch PDFs from.\n\n    Returns:\n        list: List of PDF URLs.\n    \"\"\"\n    try:\n        response = requests.get(base_url)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        logging.error(f\"Failed to fetch PDF links from '{base_url}': {e}\")\n        return []\n\n    soup = BeautifulSoup(response.content, 'html.parser')\n    pdf_links = []\n    for link in soup.find_all('a', href=True):\n        href = link['href']\n        if href.lower().endswith('.pdf'):\n            if href.startswith('http'):\n                pdf_links.append(href)\n            else:\n                pdf_links.append('https://www.oak-park.us' + href)\n    return pdf_links\n\nFunctionality:\n\nPurpose: Scrapes the OPPD website to retrieve all available PDF report links.\nProcess:\n\nHTTP Request: Sends a GET request to the specified base_url.\nError Handling: Logs and returns an empty list if the request fails.\nHTML Parsing: Utilizes BeautifulSoup to parse the HTML content.\nLink Extraction: Iterates through all &lt;a&gt; tags with href attributes, filtering those that end with .pdf.\nURL Formation: Ensures that all PDF links are absolute URLs by prefixing relative paths with the base domain.\n\n\nUsage: This function dynamically gathers all relevant PDF reports, ensuring that the data parsing pipeline remains up-to-date with the latest reports published by the OPPD.\n\n\n\n\nConclusion of Data Parsing Section\nThe Data Parsing component is meticulously crafted to automate the extraction and preparation of crime data from PDF reports. By leveraging robust libraries like PyPDF2, googlemaps, and BeautifulSoup, alongside custom utility functions, the scripts ensure data accuracy, consistency, and efficiency. Caching mechanisms play a pivotal role in optimizing performance by preventing redundant downloading, processing and minimizing API calls. Additionally, seamless integration with GitHub facilitates version control and data dissemination."
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#live-streamlit-dashboard",
    "href": "posts/OP-Crime-Documentation/index.html#live-streamlit-dashboard",
    "title": "Oak Park Crime Reporting",
    "section": "Live Streamlit Dashboard",
    "text": "Live Streamlit Dashboard\n\nOverview\nThe Live Streamlit Dashboard serves as the interactive frontend of the Oak Park Crime Reporting project. It provides users with a dynamic interface to explore, filter, and visualize crime data within Oak Park. Leveraging Streamlit’s capabilities, the dashboard offers:\n\nInteractive Maps: Visual representation of crime incidents using Folium.\nDynamic Filters: Date range and offense type filters to customize data views.\nEmail Subscription: Integration with Mailchimp for users to subscribe or unsubscribe from updates.\nNavigation Links: Quick access to related resources like Portfolio, Blog, Documentation, and signing up for Email Updates.\n\nThe primary script responsible for this component is streamlit_app.py.\n\n\nKey Components\n\n1. streamlit_app.py\nPurpose:\nThis script builds the interactive Streamlit dashboard, enabling users to filter crime data by date and offense type, visualize the data on a map, and manage email subscriptions for updates.\nImports and Dependencies:\n\n\nCode\n\nimport streamlit as st\nimport pandas as pd\nimport folium\nfrom streamlit_folium import st_folium\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport re\nimport hashlib\nimport requests\n\n# Define Mailchimp API details from secrets\nMAILCHIMP_API_KEY = st.secrets[\"mailchimp\"][\"api_key\"]\nMAILCHIMP_AUDIENCE_ID = st.secrets[\"mailchimp\"][\"audience_id\"]\nMAILCHIMP_DATA_CENTER = st.secrets[\"mailchimp\"][\"data_center\"]\n\n# Mailchimp API endpoint\nMAILCHIMP_API_URL = f\"https://{MAILCHIMP_DATA_CENTER}.api.mailchimp.com/3.0\"\n\n\nStandard Libraries: datetime, timedelta, re, hashlib, requests\nThird-Party Libraries: streamlit, pandas, folium, streamlit_folium, numpy\nMailchimp Integration: Accesses Mailchimp API credentials securely via st.secrets\n\nMain Functionalities:\n\nData Loading: Reads and caches the processed crime data from a ZIP file.\nDisclaimer Enforcement: Presents a legal disclaimer that users must agree to before accessing the dashboard.\nEmail Subscription Management: Allows users to subscribe or unsubscribe from email updates via Mailchimp.\nNavigation Links: Provides quick access to Portfolio, Blog, Documentation, Contact, Email Updates, and a Cumulative Map.\nInteractive Filters: Users can filter crime data by date range and offense type.\nData Visualization: Displays filtered crime incidents on an interactive Folium map with detailed popups.\n\nSample Code Snippets and Explanations:\n\na. Safe Field Handling\n\n\nCode\n\ndef safe_field(value):\n    \"\"\"\n    Return the string version of a field or 'Not found' if it's missing/NaN/empty.\n    \"\"\"\n    if pd.isnull(value) or value == \"\":\n        return \"Not found\"\n    return str(value)\n\nExplanation:\n\nPurpose: Ensures that all fields displayed in the dashboard are present and readable. If a field is missing (NaN) or empty, it returns “Not found” to maintain consistency in the UI.\n\n\n\nb. Data Loading with Caching\n\n\nCode\n\n@st.cache_data\ndef load_data():\n    \"\"\"\n    Reads 'summary_report.zip' once, caching the DataFrame in memory.\n    This prevents re-reading the file on every app rerun.\n    \"\"\"\n    df = pd.read_csv(\"data/summary_report.zip\", compression=\"zip\", encoding=\"cp1252\")\n    return df\n\nExplanation:\n\nFunctionality: Loads the crime data from a compressed ZIP file and caches it using Streamlit’s @st.cache_data decorator.\nBenefits:\n\nPerformance: Reduces load times by preventing redundant reads.\nEfficiency: Ensures that the dashboard remains responsive, especially with large datasets.\n\n\n\n\nc. Disclaimer Gate\n\n\nCode\n\ndef show_disclaimer():\n    \"\"\"\n    Show a disclaimer 'gate' that the user must agree to in order to proceed.\n    \"\"\"\n    st.markdown(\n        \"\"\"\n        # Important Legal Disclaimer\n        \n        **By using this demonstrative research tool, you acknowledge and agree**:\n        \n        - This tool is for **demonstration purposes only**.\n        - The data originated from publicly available Oak Park Police Department PDF files.\n          View the official site here: [Oak Park Police Department](https://www.oak-park.us/village-services/police-department).\n        - During parsing, **~10%** of complaints were **omitted** due to parsing issues; \n          thus the data is **incomplete**.\n        - The **official** and **complete** PDF files remain with the Oak Park Police Department.\n        - You **will not hold** the author **liable** for **any** decisions—formal or informal—based on this tool.\n        - This tool **should not** be used in **any** official or unofficial **decision-making**.\n        \n        By continuing, you indicate your acceptance of these terms and disclaim all liability. \n        \"\"\"\n    )\n    agree = st.checkbox(\"I have read the disclaimer and I agree to continue.\")\n    if agree:\n        st.session_state[\"user_agreed\"] = True\n        st.rerun()\n    else:\n        st.stop()\n\nExplanation:\n\nPurpose: Presents a mandatory disclaimer to users, ensuring they acknowledge the tool’s limitations and liabilities before accessing the dashboard.\nMechanism:\n\nCheckbox: Users must check the box indicating their agreement to proceed.\nSession State: Utilizes st.session_state to remember the user’s agreement across interactions.\nFlow Control: If the user does not agree, the app stops rendering further content.\n\n\n\n\nd. Email Validation and Subscription Functions\n\n\nCode\n\ndef validate_email(email):\n    \"\"\"\n    Validates the email format using regex.\n    \"\"\"\n    email_regex = r\"(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)\"\n    return re.match(email_regex, email) is not None\n\ndef subscribe_email(email):\n    \"\"\"\n    Subscribes an email to the Mailchimp audience.\n    \"\"\"\n    # Mailchimp requires the subscriber hash, which is the MD5 hash of the lowercase version of the email\n    email_lower = email.lower().encode()\n    subscriber_hash = hashlib.md5(email_lower).hexdigest()\n\n    url = f\"{MAILCHIMP_API_URL}/lists/{MAILCHIMP_AUDIENCE_ID}/members/{subscriber_hash}\"\n\n    data = {\n        \"email_address\": email,\n        \"status\": \"subscribed\",  # Explicitly set status to 'subscribed'\n        \"status_if_new\": \"subscribed\"  # Ensure new members are subscribed\n    }\n\n    response = requests.put(\n        url,\n        auth=(\"anystring\", MAILCHIMP_API_KEY),\n        json=data\n    )\n\n    return response\n\ndef unsubscribe_email(email):\n    \"\"\"\n    Unsubscribes an email from the Mailchimp audience.\n    \"\"\"\n    # Mailchimp requires the subscriber hash, which is the MD5 hash of the lowercase version of the email\n    email_lower = email.lower().encode()\n    subscriber_hash = hashlib.md5(email_lower).hexdigest()\n\n    url = f\"{MAILCHIMP_API_URL}/lists/{MAILCHIMP_AUDIENCE_ID}/members/{subscriber_hash}\"\n\n    data = {\n        \"status\": \"unsubscribed\"\n    }\n\n    response = requests.patch(\n        url,\n        auth=(\"anystring\", MAILCHIMP_API_KEY),\n        json=data\n    )\n\n    return response\n\nExplanation:\n\nEmail Validation:\n\nPurpose: Ensures that users enter a correctly formatted email address before attempting subscription or unsubscription.\nMethod: Uses a regular expression to match standard email formats.\n\nSubscription Functions:\n\nsubscribe_email:\n\nFunctionality: Subscribes a user to the Mailchimp audience list.\nProcess:\n\nHashing: Generates an MD5 hash of the email to comply with Mailchimp’s API requirements.\nAPI Request: Sends a PUT request to Mailchimp to add or update the subscriber’s status to “subscribed”.\n\n\nunsubscribe_email:\n\nFunctionality: Removes a user from the Mailchimp audience list.\nProcess:\n\nHashing: Similar to the subscription function.\nAPI Request: Sends a PATCH request to update the subscriber’s status to “unsubscribed”.\n\n\n\n\n\n\ne. Navigation Links\n\n\nCode\n\ndef add_top_links():\n    \"\"\"\n    Adds horizontal navigation links: Portfolio, Blog, and Email Updates.\n    \"\"\"\n    # Create six equal columns for the links\n    col1, col2, col3, col4, col5, col6 = st.columns(6)\n    with col1:\n        st.markdown('[**Portfolio**](https://jesse-anderson.net/)', unsafe_allow_html=True)\n    with col2:\n        st.markdown('[**Blog**](https://blog.jesse-anderson.net/)', unsafe_allow_html=True)\n    with col3:\n        st.markdown('[**Documentation**](https://blog.jesse-anderson.net/)', unsafe_allow_html=True)\n    with col4:\n        st.markdown('[**Contact**](mailto:jesse@jesse-anderson.net?subject=Inquiry%20from%20Oak%20Park%20Crime%20Map%20App&body=Hello%20Jesse,%0A%0A)', unsafe_allow_html=True)\n    with col5:\n        # Email Updates link pointing to the email updates section\n        #Google forms is better. .-.\n        st.markdown('[**📧 Email Updates**](https://forms.gle/GnyaVwo1Vzm8nBH6A)')\n    with col6:\n        # Cumulative static map with all crimes\n        st.markdown('[**Comp. Map**](https://jesse-anderson.net/OP-Crime-Maps/crime_map_cumulative.html)')\n\nExplanation:\n\nPurpose: Provides users with quick access to related resources and sections of the project.\nLayout: Utilizes Streamlit’s st.columns to arrange six links horizontally.\nLinks Included:\n\nPortfolio: Directs to the author’s personal portfolio.\nBlog: Links to the author’s blog.\nDocumentation: Accesses the project’s documentation.\nContact: Opens the user’s default email client to send an inquiry.\nEmail Updates: Directs to a Google Forms page for email subscriptions.\nComp. Map: Links to a cumulative static map of all crimes.\n\n\n\n\nf. Main Application Logic\n\n\nCode\n\ndef main_app():\n    \"\"\"\n    The main body of the application: date filters, offense filter, map, etc.\n    \"\"\"\n\n    st.title(\"Oak Park Crime Map\")\n\n    # 1) Load data from the ZIP (cached)\n    df = load_data()\n\n    # Convert 'Date' to datetime, remove rows with date=1900 or missing lat/long\n    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n    df = df[(df['Date'].notna()) & (df['Date'] != pd.Timestamp(\"1900-01-01\"))]\n    df = df.dropna(subset=['Lat', 'Long'])\n\n    # Determine data's min/max date\n    min_date_in_data = df['Date'].min().date()\n    max_date_in_data = df['Date'].max().date()\n\n    # We'll still allow up to 3 months, but default to just the last 1 month for faster display\n    today = datetime.now().date()\n    \n    # Default = last 1 month\n    # e.g. 31 days; you can do 30 if you prefer\n    default_start = max(min_date_in_data, today - timedelta(days=31))\n    default_end   = min(today, max_date_in_data)\n\n    # Create columns with ratio [1, 2]: left filter (narrow), right map (wider)\n    col_filter, col_map = st.columns([1, 2], gap=\"small\")\n\n    with col_filter:\n        st.subheader(\"Date Range (up to 3 months)\")\n\n        # Start & End date pickers\n        start_date = st.date_input(\n            \"Start Date\",\n            value=default_start,\n            min_value=min_date_in_data,\n            max_value=max_date_in_data\n        )\n        end_date = st.date_input(\n            \"End Date\",\n            value=default_end,\n            min_value=min_date_in_data,\n            max_value=max_date_in_data\n        )\n\n        # Validate date logic\n        if end_date &lt; start_date:\n            st.warning(\"End date cannot be before start date. Please adjust.\")\n            st.stop()\n\n        date_diff = (end_date - start_date).days\n        # Still enforce up to 3 months (~92 days)\n        if date_diff &gt; 92:\n            st.warning(\"Time range cannot exceed ~3 months (92 days). Please shorten.\")\n            st.stop()\n\n        # Filter by date\n        start_dt = pd.to_datetime(start_date)\n        end_dt   = pd.to_datetime(end_date) + pd.Timedelta(days=1)  # inclusive\n        date_mask = (df['Date'] &gt;= start_dt) & (df['Date'] &lt; end_dt)\n        partial_df = df[date_mask]\n\n        if partial_df.empty:\n            st.info(\"No records found for the selected date range.\")\n            st.stop()\n\n        # Dynamically gather offenses from partial_df\n        unique_offenses = sorted(partial_df['Offense'].dropna().unique())\n\n        st.subheader(\"Offense Filter\")\n        with st.expander(\"Select Offenses (scrollable)\", expanded=False):\n            if not unique_offenses:\n                st.write(\"No offenses found for this date range.\")\n                selected_offenses = []\n            else:\n                # By default, no offenses =&gt; show all\n                selected_offenses = st.multiselect(\n                    \"Offense(s)\",\n                    options=unique_offenses,\n                    default=[],  # empty =&gt; show all\n                    help=\"Scroll to find more offenses. If empty =&gt; show all.\"\n                )\n\n    # If user picks no offense =&gt; show all\n    if selected_offenses:\n        final_df = partial_df[partial_df['Offense'].isin(selected_offenses)]\n    else:\n        final_df = partial_df\n\n    if final_df.empty:\n        st.info(\"No records found for the selected offense(s).\")\n        st.stop()\n\n    # Truncate to 2,000\n    total_recs = len(final_df)\n    if total_recs &gt; 2000:\n        st.info(f\"There are {total_recs} matching records. Showing only the first 2,000.\")\n        final_df = final_df.iloc[:2000]\n    \n    base_url_static = 'https://www.oak-park.us/sites/default/files/police/summaries/'\n\n    with col_map:\n        st.write(f\"**Displaying {len(final_df)} records on the map**.\")\n\n        # Create Folium map\n        oak_park_center = [41.885, -87.78]\n        crime_map = folium.Map(location=oak_park_center, zoom_start=13)\n\n        for _, row in final_df.iterrows():\n            complaint   = safe_field(row.get('Complaint #'))\n            offense_val = safe_field(row.get('Offense'))\n            date_val    = row.get('Date')\n            date_str    = safe_field(date_val.strftime('%Y-%m-%d') if pd.notnull(date_val) else np.nan)\n            time_val    = safe_field(row.get('Time'))\n            location    = safe_field(row.get('Location'))\n            victim      = safe_field(row.get('Victim/Address'))\n            narrative   = safe_field(row.get('Narrative'))\n            filename = safe_field(row.get('File Name'))\n            # Extract the year from the filename\n            year = extract_year(filename)\n            if year:\n                base_url = f\"{base_url_static}{year}/\"\n                link = f\"{base_url}{filename}\"\n            else:\n                # Handle cases where the year isn't found or is out of range\n                link = \"#\"\n                st.warning(f\"Year not found or out of range in filename: {filename}\")\n\n            popup_html = f\"\"\"\n            &lt;b&gt;Complaint #:&lt;/b&gt; {complaint}&lt;br/&gt;\n            &lt;b&gt;Offense:&lt;/b&gt; {offense_val}&lt;br/&gt;\n            &lt;b&gt;Date:&lt;/b&gt; {date_str}&lt;br/&gt;\n            &lt;details&gt;\n              &lt;summary&gt;&lt;b&gt;View Details&lt;/b&gt;&lt;/summary&gt;\n              &lt;b&gt;Time:&lt;/b&gt; {time_val}&lt;br/&gt;\n              &lt;b&gt;Location:&lt;/b&gt; {location}&lt;br/&gt;\n              &lt;b&gt;Victim:&lt;/b&gt; {victim}&lt;br/&gt;\n              &lt;b&gt;Narrative:&lt;/b&gt; {narrative}&lt;br/&gt;\n              &lt;b&gt;URL:&lt;/b&gt; &lt;a href=\"{link}\" target=\"_blank\"&gt;PDF Link&lt;/a&gt;\n            &lt;/details&gt;\n        \"\"\"\n\n            folium.Marker(\n                location=[row['Lat'], row['Long']],\n                popup=folium.Popup(popup_html, max_width=400),\n                tooltip=f\"Complaint # {complaint}\",\n                icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n            ).add_to(crime_map)\n\n        st_folium(crime_map, width=1000, height=1000, use_container_width=True)\n\nExplanation:\n\nTitle: Sets the dashboard’s title to “Oak Park Crime Map”.\nData Filtering:\n\nDate Conversion: Converts the ‘Date’ column to datetime objects, removing entries with invalid dates (e.g., “1900-01-01”) or missing geographical coordinates.\nDate Range Determination: Identifies the earliest and latest dates in the dataset to set the boundaries for date selection.\nDefault Dates: Sets the default date range to the last 31 days, ensuring that the data displayed is recent and relevant.\n\nLayout Setup:\n\nColumns: Utilizes Streamlit’s st.columns to create a two-column layout:\n\nLeft Column (col_filter): Contains filters for date range and offense type.\nRight Column (col_map): Displays the interactive Folium map.\n\n\nInteractive Filters:\n\nDate Range Picker: Allows users to select a start and end date within the permissible range.\n\nValidation: Ensures that the end date is not before the start date and that the selected range does not exceed three months (92 days).\n\nOffense Type Multiselect:\n\nDynamic Options: Populates the multiselect options based on the offenses present in the filtered date range.\nDefault Behavior: If no offenses are selected, all offenses are displayed.\n\n\nData Truncation:\n\nLimitation: Caps the number of records displayed on the map to 2,000 to maintain performance and usability.\n\nMap Generation:\n\nFolium Map: Centers the map on Oak Park’s geographical coordinates.\nMarkers:\n\nCustomization: Each crime incident is represented by a blue marker with an info-sign icon.\nPopups: Clicking on a marker reveals detailed information about the incident, including a link to the original PDF report.\n\n\nStreamlit Folium Integration:\n\nst_folium: Embeds the Folium map within the Streamlit app, ensuring seamless interactivity.\n\n\n\n\ng. Main Function to Control App Flow\n\n\nCode\n\ndef main():\n    # Check if user has agreed to disclaimer\n    if \"user_agreed\" not in st.session_state:\n        st.session_state[\"user_agreed\"] = False\n\n    if not st.session_state[\"user_agreed\"]:\n        show_disclaimer()\n    else:\n        # Set the page layout to wide\n        st.set_page_config(page_title=\"Oak Park Crime\", layout=\"wide\")\n\n        # Add horizontal navigation links at the top\n        add_top_links()\n\n        # Proceed with the main application\n        main_app()\n\n        # # Add Email Updates section at the bottom\n        # add_email_subscription()\n\nif __name__ == \"__main__\":\n    main()\n\nExplanation:\n\nDisclaimer Check:\n\nSession State: Utilizes st.session_state to track whether the user has agreed to the disclaimer.\nFlow Control: If the user hasn’t agreed, the disclaimer is displayed; otherwise, the main application proceeds.\n\nPage Configuration:\n\nLayout: Sets the Streamlit app layout to “wide” for better use of screen real estate.\nPage Title: Labels the browser tab as “Oak Park Crime”.\n\nNavigation Links: Invokes add_top_links() to display navigation links at the top of the dashboard.\nMain Application Execution: Calls main_app() to render the interactive filters and map.\nEmail Subscription: The add_email_subscription() function is present but commented out, now instead of potentially exposing the site to some sort of injection type attack we redirect users to a simple google forms sheet to subscribe/unsubscribe.\n\n\n\n\n\nDetailed Code Breakdown\nLet’s delve deeper into specific functions and sections of streamlit_app.py to understand their roles and implementations.\n\na. Email Subscription Management\nAlthough the add_email_subscription() function is commented out, it’s essential to understand its intended functionality, as it may be incorporated in the future in some one off project of mine and this documentation would greatly assist..\n\n\nCode\n\ndef add_email_subscription():\n    \"\"\"\n    Displays subscription and unsubscription forms at the bottom of the page.\n    The forms are within a collapsed expander that the user can expand manually.\n    \"\"\"\n    # Add an anchor to scroll to\n    st.markdown('&lt;a id=\"email-updates\"&gt;&lt;/a&gt;', unsafe_allow_html=True)\n    \n    # **3. Implement Email Updates within a Collapsed Expander**\n    with st.expander(\"📧 Email Updates\", expanded=False):\n        st.markdown(\"### Subscribe to Email Updates\")\n        with st.form(\"email_subscription_form\"):\n            subscribe_email_input = st.text_input(\"Enter your email address to subscribe:\")\n            subscribe_submit = st.form_submit_button(\"Subscribe\")\n\n            if subscribe_submit:\n                if validate_email(subscribe_email_input):\n                    response = subscribe_email(subscribe_email_input)\n                    if response.status_code == 200:\n                        # Check if the email was already subscribed\n                        response_data = response.json()\n                        status = response_data.get(\"status\")\n                        if status == \"subscribed\":\n                            # Check if the 'previous_status' was 'unsubscribed' to provide accurate feedback\n                            previous_status = response_data.get(\"status_if_new\")\n                            if previous_status == \"subscribed\":\n                                st.success(\"Subscription successful! You've been resubscribed to the email list.\")\n                            else:\n                                st.success(\"Subscription successful! You've been added to the email list.\")\n                        else:\n                            st.info(\"You are already subscribed.\")\n                    else:\n                        # Handle errors\n                        error_message = response.json().get('detail', 'An error occurred.')\n                        st.error(f\"Subscription failed: {error_message}\")\n                else:\n                    st.error(\"Please enter a valid email address.\")\n\n        st.markdown(\"---\")  # Separator\n\n        st.markdown(\"### Unsubscribe from Email Updates\")\n        with st.form(\"email_unsubscription_form\"):\n            unsubscribe_email_input = st.text_input(\"Enter your email address to unsubscribe:\")\n            unsubscribe_submit = st.form_submit_button(\"Unsubscribe\")\n\n            if unsubscribe_submit:\n                if validate_email(unsubscribe_email_input):\n                    response = unsubscribe_email(unsubscribe_email_input)\n                    if response.status_code == 200:\n                        response_data = response.json()\n                        status = response_data.get(\"status\")\n                        if status == \"unsubscribed\":\n                            st.success(\"You have been unsubscribed successfully.\")\n                        else:\n                            st.info(\"Your email was not found in our list.\")\n                    else:\n                        # Handle errors\n                        error_message = response.json().get('detail', 'An error occurred.')\n                        st.error(f\"Unsubscription failed: {error_message}\")\n                else:\n                    st.error(\"Please enter a valid email address.\")\n\nExplanation:\n\nPurpose: Provides users with forms to subscribe or unsubscribe from email updates.\nStructure:\n\nExpander: Collapses the email update section to keep the dashboard clean.\nSubscription Form:\n\nInput: Email address field.\nSubmission: Validates and processes the subscription request via Mailchimp.\nFeedback: Displays success or error messages based on the API response.\n\nUnsubscription Form:\n\nInput: Email address field.\nSubmission: Validates and processes the unsubscription request via Mailchimp.\nFeedback: Displays success or error messages based on the API response.\n\n\nCurrent Status: The entire function is commented out, indicating it’s not active(obvious).\n\n\n\nb. Year Extraction from Filename\n\n\nCode\n\ndef extract_year(filename, start_year=2017, end_year=2030):\n    \"\"\"\n    Extracts a four-digit year from the filename.\n    Returns the year as a string if found and within the range.\n    Returns None otherwise.\n    \"\"\"\n    match = re.search(r'(20[1][7-9]|20[2][0-9]|2030)', filename)\n    if match:\n        return match.group(0)\n    return None\n\nExplanation:\n\nPurpose: Retrieves the year from the PDF filename to construct accurate URLs linking back to the original reports.\nLogic:\n\nRegex Pattern: Searches for years ranging from 2017 to 2030. If this tool is still in use by 2030 and/or crime records pre 2018 are on the oak park website or somewhere else then this should be fixed.\nReturn Value: Provides the matched year as a string or None if no valid year is found.\n\n\nUsage in main_app():\n\nURL Construction: Utilizes the extracted year to build the base URL for the PDF link.\nFallback Handling: If the year isn’t found, the PDF link defaults to #, and a warning is displayed.\n\n\n\nc. Interactive Map Rendering\n\n\nCode\n\nwith col_map:\n    st.write(f\"**Displaying {len(final_df)} records on the map**.\")\n\n    # Create Folium map\n    oak_park_center = [41.885, -87.78]\n    crime_map = folium.Map(location=oak_park_center, zoom_start=13)\n\n    for _, row in final_df.iterrows():\n        complaint   = safe_field(row.get('Complaint #'))\n        offense_val = safe_field(row.get('Offense'))\n        date_val    = row.get('Date')\n        date_str    = safe_field(date_val.strftime('%Y-%m-%d') if pd.notnull(date_val) else np.nan)\n        time_val    = safe_field(row.get('Time'))\n        location    = safe_field(row.get('Location'))\n        victim      = safe_field(row.get('Victim/Address'))\n        narrative   = safe_field(row.get('Narrative'))\n        filename = safe_field(row.get('File Name'))\n        # Extract the year from the filename\n        year = extract_year(filename)\n        if year:\n            base_url = f\"{base_url_static}{year}/\"\n            link = f\"{base_url}{filename}\"\n        else:\n            # Handle cases where the year isn't found or is out of range\n            link = \"#\"\n            st.warning(f\"Year not found or out of range in filename: {filename}\")\n\n        popup_html = f\"\"\"\n        &lt;b&gt;Complaint #:&lt;/b&gt; {complaint}&lt;br/&gt;\n        &lt;b&gt;Offense:&lt;/b&gt; {offense_val}&lt;br/&gt;\n        &lt;b&gt;Date:&lt;/b&gt; {date_str}&lt;br/&gt;\n        &lt;details&gt;\n          &lt;summary&gt;&lt;b&gt;View Details&lt;/b&gt;&lt;/summary&gt;\n          &lt;b&gt;Time:&lt;/b&gt; {time_val}&lt;br/&gt;\n          &lt;b&gt;Location:&lt;/b&gt; {location}&lt;br/&gt;\n          &lt;b&gt;Victim:&lt;/b&gt; {victim}&lt;br/&gt;\n          &lt;b&gt;Narrative:&lt;/b&gt; {narrative}&lt;br/&gt;\n          &lt;b&gt;URL:&lt;/b&gt; &lt;a href=\"{link}\" target=\"_blank\"&gt;PDF Link&lt;/a&gt;\n        &lt;/details&gt;\n    \"\"\"\n\n        folium.Marker(\n            location=[row['Lat'], row['Long']],\n            popup=folium.Popup(popup_html, max_width=400),\n            tooltip=f\"Complaint # {complaint}\",\n            icon=folium.Icon(color=\"blue\", icon=\"info-sign\")\n        ).add_to(crime_map)\n\n    st_folium(crime_map, width=1000, height=1000, use_container_width=True)\n\nExplanation:\n\nMap Initialization:\n\nCentering: The map is centered on Oak Park’s geographical coordinates with a zoom level of 13 for optimal visibility.\n\nMarker Creation:\n\nLooping Through Data: Iterates over each record in the filtered DataFrame (final_df).\nData Extraction: Retrieves necessary fields like Complaint Number, Offense, Date, Time, Location, Victim Address, Narrative, and File Name.\nURL Formation: Constructs a direct link to the original PDF report using the extracted year. If the year isn’t found, the link defaults to #, and a warning is displayed.\nPopup HTML: Formats the incident details into an HTML structure that includes expandable sections (&lt;details&gt;) for additional information and the PDF link.\n\nFolium Marker Customization:\n\nLocation: Plots the marker based on latitude and longitude.\nPopup: Attaches the formatted HTML popup to the marker.\nTooltip: Displays the Complaint Number when hovering over the marker.\nIcon: Uses a blue info-sign icon for consistency and visibility.\n\nMap Embedding:\n\nst_folium: Renders the Folium map within the Streamlit dashboard, allowing for interactivity like zooming and panning.\n\n\n\n\n\nConclusion of Live Streamlit Dashboard Section\nThe Live Streamlit Dashboard is a pivotal component of the Oak Park Crime Reporting project, offering users an intuitive and interactive means to explore crime data. By integrating dynamic filters, interactive maps, and email subscription management, the dashboard enhances user engagement and data accessibility. The modular design, leveraging Streamlit’s capabilities and third-party integrations like Mailchimp, ensures scalability and maintainability."
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#static-html-generation",
    "href": "posts/OP-Crime-Documentation/index.html#static-html-generation",
    "title": "Oak Park Crime Reporting",
    "section": "Static HTML Generation",
    "text": "Static HTML Generation\n\nOverview\nThe Static HTML Generation component automates the creation of static HTML reports and maps based on the latest crime data. This ensures that the information remains accessible even outside the interactive Streamlit dashboard environment. The component performs the following tasks:\n\nData Aggregation: Compiles crime data for the past week.\nMap Creation: Generates interactive and cumulative Folium maps with detailed popups.\nGitHub Integration: Uploads the generated HTML reports and CSV files to a GitHub repository, facilitating easy sharing and hosting via GitHub Pages.\nEmail Dissemination: Sends automated emails containing the latest reports and links to subscribers.\n\nThe primary script responsible for this component is weekly_crime_report.py.\n\n\nKey Components\n\n1. weekly_crime_report.py\nPurpose:\nThis script automates the generation of weekly crime reports, including interactive maps and CSV data files. It handles data filtering, map creation with disclaimers, uploading to GitHub, and sending out emails to subscribers with the latest reports.\nImports and Dependencies:\n\n\nCode\n\nimport os\nimport pandas as pd\nimport folium\nimport logging\nimport zipfile\nimport time\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\nfrom email.mime.application import MIMEApplication\nimport base64\nimport sys\n\nimport numpy as np\nimport re\nimport hashlib\nimport requests\n\nfrom google.auth.transport.requests import Request\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\n# Local utility to load env vars\nfrom utils import load_env_vars, extract_year, upload_file_to_github, upload_files_to_github_batch\n\n# Folium plugins\nfrom folium.plugins import MarkerCluster\n\n# Define Gmail API scope\nSCOPES = ['https://www.googleapis.com/auth/gmail.send']\n\n\nStandard Libraries: os, re, json, pandas, pathlib, logging, collections, time, datetime, string, zipfile, email, base64, sys\nThird-Party Libraries: numpy, folium, google-auth, google-auth-oauthlib, google-auth-httplib2, google-api-python-client, requests\nLocal Utilities: Functions imported from utils.py\n\nMain Functionalities:\n\nEnvironment Setup:\n\nLoads environment variables from env_vars.txt.\nConfigures logging.\nSets up directories for data, maps, CSVs, and GitHub integration.\n\nData Processing:\n\nLoads all crime data from a compressed ZIP file.\nFilters data for the past week.\nWrites the filtered data to a CSV file.\n\nMap Creation:\n\nGenerates an interactive Folium map with markers for each crime incident.\nGenerates a cumulative Folium map displaying all crime incidents.\nAdds disclaimers as overlaying HTML elements on the maps.\n\nGitHub Integration:\n\nUploads the generated HTML maps and CSV files to specified GitHub repository subfolders.\n\nEmail Dissemination:\n\nAuthenticates with the Gmail API.\nPrepares and sends an email to subscribers with links to the latest reports and attached CSV files.\n\nLogging and Reporting:\n\nLogs processing steps, errors, and summary statistics.\nMeasures and records the execution time of the script.\n\n\nSample Code Snippet:\n\n\nCode\n\ndef create_folium_map_filtered_data(\n    df,\n    lat_col='Lat',\n    lng_col='Long',\n    offense_col='Offense',\n    date_col='Date',\n    output_html_path='weekly_map.html'\n):\n    \"\"\"\n    Creates a Folium map plotting each record in df with disclaimers overlay (JS + HTML) \n    and saves it to 'output_html_path'.\n    \"\"\"\n    oak_park_center = [41.885, -87.78]\n    crime_map = folium.Map(location=oak_park_center, zoom_start=13)\n\n    marker_cluster = MarkerCluster().add_to(crime_map)\n    # Define the static part of the base URL\n    base_url_static = 'https://www.oak-park.us/sites/default/files/police/summaries/'\n\n    for _, row in df.iterrows():\n        lat = row[lat_col]\n        lng = row[lng_col]\n        offense = row.get(offense_col, \"Unknown\")\n        complaint = safe_field(row.get('Complaint #'))\n        offense_val = safe_field(offense)\n        date_str = safe_field(row['Date'].strftime('%Y-%m-%d') if pd.notnull(row['Date']) else np.nan)\n        time_val = safe_field(row.get('Time'))\n        location = safe_field(row.get('Location'))\n        victim = safe_field(row.get('Victim/Address'))\n        narrative = safe_field(row.get('Narrative'))\n        filename = safe_field(row.get('File Name'))\n\n        popup_html = f\"\"\"\n            &lt;b&gt;Complaint #:&lt;/b&gt; {complaint}&lt;br/&gt;\n            &lt;b&gt;Offense:&lt;/b&gt; {offense_val}&lt;br/&gt;\n            &lt;b&gt;Date:&lt;/b&gt; {date_str}&lt;br/&gt;\n            &lt;details&gt;\n              &lt;summary&gt;&lt;b&gt;View Details&lt;/b&gt;&lt;/summary&gt;\n              &lt;b&gt;Time:&lt;/b&gt; {time_val}&lt;br/&gt;\n              &lt;b&gt;Location:&lt;/b&gt; {location}&lt;br/&gt;\n              &lt;b&gt;Victim:&lt;/b&gt; {victim}&lt;br/&gt;\n              &lt;b&gt;Narrative:&lt;/b&gt; {narrative}&lt;br/&gt;\n              &lt;b&gt;URL:&lt;/b&gt; &lt;a href=\"{filename}\" target=\"_blank\"&gt;PDF Link&lt;/a&gt;\n            &lt;/details&gt;\n        \"\"\"\n\n        folium.Marker(\n            location=[lat, lng],\n            popup=folium.Popup(popup_html, max_width=300),\n            icon=folium.Icon(color='red', icon='info-sign')\n        ).add_to(marker_cluster)\n\n    # Basic map title\n    title_html = '''\n    &lt;h3 align=\"center\" style=\"font-size:20px\"&gt;&lt;b&gt;Oak Park Crime Map&lt;/b&gt;&lt;/h3&gt;\n    &lt;br&gt;\n    &lt;h3 align=\"center\" style=\"font-size:10px\"&gt;\n    &lt;a href=\"https://jesse-anderson.net/\"&gt;My Portfolio&lt;/a&gt; |\n    &lt;a href=\"https://blog.jesse-anderson.net/\"&gt;My Blog&lt;/a&gt; |\n    &lt;a href=\"https://blog.jesse-anderson.net/\"&gt;Documentation&lt;/a&gt; |\n    &lt;a href=\"mailto:jesse@jesse-anderson.net\"&gt;Contact&lt;/a&gt; |\n    &lt;a href=\"https://forms.gle/GnyaVwo1Vzm8nBH6A\"&gt;\n        Add me to Weekly Updates\n    &lt;/a&gt;\n    &lt;/h3&gt;\n'''\n    crime_map.get_root().html.add_child(folium.Element(title_html))\n\n    # 1) Overlays disclaimers in a \"splash screen\" with JavaScript:\n    disclaimers_overlay = \"\"\"\n    &lt;style&gt;\n    /* Full-page overlay styling */\n    #disclaimerOverlay {\n      position: fixed;\n      z-index: 9999; /* On top of everything */\n      left: 0;\n      top: 0;\n      width: 100%;\n      height: 100%;\n      background-color: rgba(255, 255, 255, 0.95);\n      color: #333;\n      display: block; /* Visible by default */\n      overflow: auto;\n      text-align: center;\n      padding-top: 100px;\n      font-family: Arial, sans-serif;\n    }\n    #disclaimerContent {\n      background: #f9f9f9;\n      border: 1px solid #ccc;\n      display: inline-block;\n      padding: 20px;\n      max-width: 800px;\n      text-align: left;\n    }\n    #acceptButton {\n      margin-top: 20px;\n      padding: 10px 20px;\n      font-size: 16px;\n      cursor: pointer;\n    }\n    &lt;/style&gt;\n\n    &lt;div id=\"disclaimerOverlay\"&gt;\n      &lt;div id=\"disclaimerContent\"&gt;\n        &lt;h2&gt;Important Legal Disclaimer&lt;/h2&gt;\n        &lt;p&gt;&lt;strong&gt;By using this demonstrative research tool, you acknowledge and agree:&lt;/strong&gt;&lt;/p&gt;\n        &lt;ul&gt;\n            &lt;li&gt;This tool is for &lt;strong&gt;demonstration purposes only&lt;/strong&gt;.&lt;/li&gt;\n            &lt;li&gt;The data originated from publicly available Oak Park Police Department PDF files.\n                View the official site here: \n                &lt;a href=\"https://www.oak-park.us/village-services/police-department\"\n                   target=\"_blank\"&gt;Oak Park Police Department&lt;/a&gt;.&lt;/li&gt;\n            &lt;li&gt;During parsing, &lt;strong&gt;~10%&lt;/strong&gt; of complaints were &lt;strong&gt;omitted&lt;/strong&gt; \n                due to parsing issues; thus the data is &lt;strong&gt;incomplete&lt;/strong&gt;.&lt;/li&gt;\n            &lt;li&gt;The &lt;strong&gt;official&lt;/strong&gt; and &lt;strong&gt;complete&lt;/strong&gt; PDF files remain \n                with the Oak Park Police Department.&lt;/li&gt;\n            &lt;li&gt;You &lt;strong&gt;will not hold&lt;/strong&gt; the author &lt;strong&gt;liable&lt;/strong&gt; for &lt;strong&gt;any&lt;/strong&gt; \n                decisions—formal or informal—based on this tool.&lt;/li&gt;\n            &lt;li&gt;This tool &lt;strong&gt;should not&lt;/strong&gt; be used in &lt;strong&gt;any&lt;/strong&gt; official or unofficial \n                &lt;strong&gt;decision-making&lt;/strong&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n        &lt;p&gt;&lt;strong&gt;By continuing, you indicate your acceptance of these terms \n           and disclaim all liability.&lt;/strong&gt;&lt;/p&gt;\n        &lt;hr/&gt;\n        &lt;button id=\"acceptButton\" onclick=\"hideOverlay()\"&gt;I Accept&lt;/button&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n    function hideOverlay() {\n      var overlay = document.getElementById('disclaimerOverlay');\n      overlay.style.display = 'none'; \n    }\n    &lt;/script&gt;\n    \"\"\"\n\n    disclaimers_element = folium.Element(disclaimers_overlay)\n    crime_map.get_root().html.add_child(disclaimers_element)\n\n    # 2) Save final HTML\n    crime_map.save(str(output_html_path))\n\nExplanation:\n\nMap Initialization:\n\nCentering: Centers the Folium map on Oak Park’s geographical coordinates with a zoom level of 13 for optimal visibility.\n\nMarker Creation:\n\nCustomization: Each crime incident is represented by a red marker with an info-sign icon.\nPopups: Clicking on a marker reveals detailed information about the incident, including a link to the original PDF report.\n\nTitle Addition:\n\nHTML Styling: Adds a title and navigation links directly onto the Folium map using HTML and inline CSS.\n\nDisclaimer Overlay:\n\nPurpose: Overlays a full-page disclaimer that users must accept before interacting with the map.\nImplementation:\n\nCSS: Styles the overlay to cover the entire page with semi-transparent background.\nHTML: Structures the disclaimer content within a styled div.\nJavaScript: Provides a function to hide the overlay when the user clicks the “I Accept” button.\n\n\nMap Saving:\n\nOutput: Saves the generated map with overlays to the specified HTML file path.\n\n\n\nd. Gmail API Service Setup\n\n\nCode\n\ndef get_gmail_service():\n    \"\"\"\n    Authenticates the user and returns the Gmail API service.\n    \"\"\"\n    creds = None\n    token_path = Path('token.json')\n\n    if token_path.exists():\n        creds = Credentials.from_authorized_user_file(str(token_path), SCOPES)\n\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            try:\n                creds.refresh(Request())\n                logging.info(\"Credentials refreshed successfully.\")\n            except Exception as e:\n                logging.error(f\"Error refreshing credentials: {e}\")\n                print(f\"[ERROR] Could not refresh credentials: {e}\")\n                return None\n        else:\n            try:\n                flow = InstalledAppFlow.from_client_secrets_file(\n                    'credentials.json', SCOPES\n                )\n                creds = flow.run_local_server(port=0)\n                logging.info(\"Authentication flow completed successfully.\")\n            except Exception as e:\n                logging.error(f\"Error during OAuth flow: {e}\")\n                print(f\"[ERROR] Could not complete OAuth flow: {e}\")\n                return None\n\n        try:\n            with open(token_path, 'w') as token:\n                token.write(creds.to_json())\n                logging.info(\"Credentials saved to token.json.\")\n        except Exception as e:\n            logging.error(f\"Failed to save credentials: {e}\")\n            print(f\"[ERROR] Could not save credentials: {e}\")\n\n    try:\n        service = build('gmail', 'v1', credentials=creds)\n        logging.info(\"Gmail service created successfully.\")\n        return service\n    except HttpError as error:\n        logging.error(f\"An error occurred while building Gmail service: {error}\")\n        print(f\"[ERROR] An error occurred while building Gmail service: {error}\")\n        return None\n\nExplanation:\n\nPurpose: Authenticates the user and establishes a connection with the Gmail API to send emails.\nAuthentication Flow:\n\nToken Check: Looks for existing credentials in token.json.\nToken Refresh: If credentials are expired but have a refresh token, it attempts to refresh them.\nOAuth Flow: If no valid credentials exist, initiates the OAuth flow using credentials.json.\nCredential Saving: Saves the new or refreshed credentials back to token.json for future use.\n\nError Handling: Logs and prints errors encountered during authentication or service creation.\n\nUsage: This function ensures secure and authenticated access to the Gmail API, enabling the script to send automated emails containing the latest crime reports.\n\n\ne. Email Sending Function\n\n\nCode\n\ndef send_email_with_disclaimer_and_links(\n    service,\n    sender_email,\n    to_emails,\n    subject,\n    body_text,\n    attachments\n):\n    \"\"\"\n    Sends an email with disclaimers and links using the Gmail API.\n    \"\"\"\n    try:\n        message = MIMEMultipart()\n        message['to'] = \"Undisclosed Recipients &lt;jesse@jesse-anderson.net&gt;\"\n        message['subject'] = subject\n        message['from'] = sender_email\n\n        # disclaimer = \"\"\"\n        # &lt;p&gt;&lt;strong&gt;Important Legal Disclaimer&lt;/strong&gt;&lt;/p&gt;\n        # &lt;p&gt;&lt;strong&gt;By using this demonstrative research tool, you acknowledge and agree:&lt;/strong&gt;&lt;/p&gt;\n        # &lt;ul&gt;\n        #     &lt;li&gt;This tool is for &lt;strong&gt;demonstration purposes only&lt;/strong&gt;.&lt;/li&gt;\n        #     &lt;li&gt;The data originated from publicly available Oak Park Police Department PDF files.\n        #         &lt;a href=\"https://www.oak-park.us/village-services/police-department\"&gt;Official site&lt;/a&gt;.&lt;/li&gt;\n        #     &lt;li&gt;During parsing, &lt;strong&gt;~10%&lt;/strong&gt; of complaints were &lt;strong&gt;omitted&lt;/strong&gt;.&lt;/li&gt;\n        #     &lt;li&gt;The &lt;strong&gt;official&lt;/strong&gt; and &lt;strong&gt;complete&lt;/strong&gt; PDF files remain with the Oak Park Police Department.&lt;/li&gt;\n        #     &lt;li&gt;You &lt;strong&gt;will not hold&lt;/strong&gt; the author &lt;strong&gt;liable&lt;/strong&gt; for any decisions\n        #         based on this tool.&lt;/li&gt;\n        #     &lt;li&gt;This tool &lt;strong&gt;should not&lt;/strong&gt; be used in any official or unofficial &lt;strong&gt;decision-making&lt;/strong&gt;.&lt;/li&gt;\n        # &lt;/ul&gt;\n        # &lt;p&gt;&lt;strong&gt;By continuing, you disclaim all liability.&lt;/strong&gt;&lt;/p&gt;\n        # &lt;hr&gt;\n        # \"\"\"\n\n        links = \"\"\"\n        &lt;p&gt;\n            &lt;a href=\"https://jesse-anderson.net/\"&gt;My Portfolio&lt;/a&gt; | \n            &lt;a href=\"https://blog.jesse-anderson.net/\"&gt;My Blog&lt;/a&gt;\n        &lt;/p&gt;\n        &lt;hr&gt;\n        \"\"\"\n\n        # Define the plain text content\n        plain_text = f\"\"\"\n        Important Legal Disclaimer\n        \n        By using this demonstrative research tool, you acknowledge and agree:\n\n        - This tool is for demonstration purposes only.\n        - The data originated from publicly available Oak Park Police Department PDF files.\n          View the official site here: https://www.oak-park.us/village-services/police-department.\n        - During parsing, ~10% of total complaints since 2018 were omitted due to parsing issues; \n          thus the data is incomplete.\n        - The official and complete PDF files remain with the Oak Park Police Department.\n        - You will not hold the author liable for any decisions—formal or informal—based on this tool.\n        - This tool should not be used in any official or unofficial decision-making.\n\n        By continuing, you indicate your acceptance of these terms and disclaim all liability.\n\n        ------------\n\n        Hello,\n        The crime report from {body_text['start_date']} to {body_text['end_date']} is attached as a .csv file.\n\n        Interactive map:\n        {body_text['weekly_map_url']}\n\n        Cumulative map:\n        {body_text['cumulative_map_url']}\n        \n        Last week's data:\n        {body_text['csv_url']}\n        \"\"\"\n\n        part1 = MIMEText(plain_text, 'plain')\n        message.attach(part1)\n\n        bcc_emails = \", \".join(to_emails)\n        message['bcc'] = bcc_emails\n\n        # Attach the CSV\n        for file_path in attachments:\n            file_path = Path(file_path)\n            if not file_path.exists():\n                logging.warning(f\"Attachment '{file_path}' not found, skipping.\")\n                continue\n            with open(file_path, 'rb') as f:\n                mime_application = MIMEApplication(f.read(), Name=file_path.name)\n            mime_application['Content-Disposition'] = f'attachment; filename=\"{file_path.name}\"'\n            message.attach(mime_application)\n\n        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n        body = {'raw': raw_message}\n\n        sent_message = service.users().messages().send(userId=\"me\", body=body).execute()\n        logging.info(f\"Email sent successfully. Message ID: {sent_message['id']}\")\n        print(\"Crime report email sent successfully.\")\n    except Exception as e:\n        logging.error(f\"Failed to send email: {e}\")\n        print(f\"[ERROR] Failed to send email: {e}\")\n\nExplanation:\n\nPurpose: Composes and sends an email containing the latest crime reports and links to interactive maps.\nStructure:\n\nEmail Composition:\n\nPlain Text: Provides a disclaimer and details about the latest crime reports.\nAttachments: Attaches the filtered CSV report.\n\nBCC: Sends the email to undisclosed recipients to maintain privacy.\n\nEncoding: Encodes the email content in base64 to comply with Gmail API requirements.\nSending: Utilizes the authenticated Gmail service to send the email.\nError Handling: Logs and prints errors encountered during the email sending process.\n\n\n\nf. Main Report Generation Workflow\n\n\nCode\n\ndef main_report_generation():\n    \"\"\"\n    Executes the full pipeline:\n    1. Load environment variables\n    2. Load & filter data for the last 7 days\n    3. Create & save Folium map with disclaimers overlay\n    4. Upload HTML to GitHub\n    5. Send email with CSV attached\n    \"\"\"\n    start_time = time.time()\n    script_dir = Path(__file__).parent.resolve()\n\n    env_file_path = script_dir / \"env_vars.txt\"\n    try:\n        load_env_vars(env_file_path)\n    except FileNotFoundError as e:\n        print(f\"[ERROR] {e}\")\n        return\n\n    sender_email = os.getenv(\"SENDER_EMAIL\")\n    if not sender_email:\n        raise ValueError(\"Missing SENDER_EMAIL in env_vars.txt\")\n\n    logging.basicConfig(\n        filename=script_dir / 'full_crime_report.log',\n        level=logging.DEBUG,\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    data_dir = script_dir / 'data'\n    map_dir = script_dir / 'generated_maps'\n    # recipients_csv = script_dir / 'recipients.csv'\n    csv_dir = script_dir / 'generated_csvs'  # New directory for CSVs\n    map_dir.mkdir(parents=True, exist_ok=True)\n    csv_dir.mkdir(parents=True, exist_ok=True)\n\n    zip_file_path = data_dir / 'summary_report.zip'\n\n    execution_date = datetime.now().date()\n    date_str = execution_date.strftime('%Y-%m-%d')\n\n    # CSV & HTML output\n    filtered_subset_filename = f'filtered_subset_{date_str}.csv'\n    filtered_subset_path = csv_dir / filtered_subset_filename\n    weekly_map_output_filename = f'crime_map_weekly_{date_str}.html'\n    weekly_map_output_path = map_dir / weekly_map_output_filename\n    cumulative_map_output_filename = f'crime_map_cumulative.html'\n    cumulative_map_output_path = map_dir / cumulative_map_output_filename\n\n    # Local GitHub Pages folder\n    github_repo_path = os.getenv(\"GITHUB_REPO\")\n    github_repo_path = Path(github_repo_path)\n\n    # (C) Load data\n    try:\n        df_full = load_all_crimes(zip_file_path)\n    except Exception as e:\n        logging.error(f\"Failed to load all crimes: {e}\")\n        print(f\"[ERROR] Could not load all crimes: {e}\")\n        return\n\n    if df_full.empty:\n        logging.info(\"No crime data—no map or CSV generated.\")\n        print(\"No crime data—no map or CSV generated.\")\n        return\n\n    # (D) Filter last 7 days\n    try:\n        start_date, end_date = determine_date_range(df_full, execution_date)\n        df_filtered = filter_crime_data(df_full, start_date, end_date)\n    except Exception as e:\n        logging.error(f\"Error determining date range: {e}\")\n        print(f\"[ERROR] Could not determine date range: {e}\")\n        return\n\n    if df_filtered.empty:\n        logging.info(\"No crimes found—no map or CSV generated.\")\n        print(\"No crimes found in the determined date range—no map or CSV generated.\")\n        return\n\n    # (E) Write filtered CSV\n    try:\n        df_filtered.to_csv(filtered_subset_path, index=False, encoding=\"cp1252\")\n        logging.info(f\"Filtered data written to {filtered_subset_path}\")\n    except Exception as e:\n        logging.error(f\"Failed to write filtered data to CSV: {e}\")\n        print(f\"[ERROR] Could not write filtered data to CSV: {e}\")\n        return\n\n    # (F) Create Folium map with disclaimers overlay\n    try:\n        create_folium_map_filtered_data(\n            df=df_filtered,\n            lat_col='Lat',\n            lng_col='Long',\n            offense_col='Offense',\n            date_col='Date',\n            output_html_path=weekly_map_output_path\n        )\n        logging.info(f\"Folium map created at {weekly_map_output_path}\")\n    except Exception as e:\n        logging.error(f\"Error creating Folium map: {e}\")\n        print(f\"[ERROR] Could not create Folium map: {e}\")\n        return\n\n    try:\n        create_folium_map_cumulative(\n            df=df_full,  # Use the full dataset for cumulative map\n            lat_col='Lat',\n            lng_col='Long',\n            offense_col='Offense',\n            date_col='Date',\n            output_html_path=cumulative_map_output_path\n        )\n        logging.info(f\"Cumulative Folium map created at {cumulative_map_output_path}\")\n    except Exception as e:\n        logging.error(f\"Error creating cumulative Folium map: {e}\")\n        print(f\"[ERROR] Could not create cumulative Folium map: {e}\")\n        return\n    \n    test = False\n    if not test:\n        # (G) Upload Map and csv\n        try:\n            # Upload Maps\n            files_to_upload_maps = [weekly_map_output_path, cumulative_map_output_path]\n            upload_files_to_github_batch(\n                file_paths=files_to_upload_maps,\n                github_repo_path=github_repo_path,\n                target_subfolder='OP-Crime-Maps'\n            )\n            time.sleep(5) #paranoia\n            # Upload CSV\n            files_to_upload_csv = filtered_subset_path\n            upload_file_to_github(\n                file_path=files_to_upload_csv,\n                github_repo_path=github_repo_path,\n                target_subfolder='OP-Crime-Data'\n            )\n        except Exception as e:\n            logging.error(f\"Failed to upload files to GitHub: {e}\")\n            print(f\"[ERROR] Could not upload files to GitHub: {e}\")\n            return\n        # (I) Generate GitHub URLs for the uploaded files\n        # Assuming GitHub Pages are served from the root of the repository\n        github_base_url = \"https://jesse-anderson.github.io\"\n\n        weekly_map_url = f\"{github_base_url}/OP-Crime-Maps/{weekly_map_output_filename}\"\n        cumulative_map_url = f\"{github_base_url}/OP-Crime-Maps/{cumulative_map_output_filename}\"\n        csv_url = f\"{github_base_url}/OP-Crime-Data/{filtered_subset_filename}\"\n\n        # (J) Gmail API & Email\n        try:\n            service = get_gmail_service()\n            if not service:\n                raise Exception(\"Failed to create Gmail service.\")\n        except Exception as e:\n            logging.error(f\"Authentication failed: {e}\")\n            print(f\"[ERROR] Authentication failed: {e}\")\n            return\n        time.sleep(60) #time to build github pages....\n        # (K) Load Recipients\n        try:\n            # to_list = load_recipients_list(recipients_csv)\n            # to_list = get_mailchimp_subscribers()\n            to_list = [\"myemail@gmail.com\"]\n        except FileNotFoundError as e:\n            logging.error(f\"Error loading recipients: {e}\")\n            print(f\"[ERROR] {e}\")\n            return\n\n        if not to_list:\n            logging.warning(\"No recipients found—cannot send email.\")\n            print(\"No recipients found in recipients.csv—cannot send email.\")\n            return\n\n        subject = f\"Crime Report from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\"\n        body_text = {\n            'start_date': start_date.strftime('%Y-%m-%d'),\n            'end_date': end_date.strftime('%Y-%m-%d'),\n            'weekly_map_url': weekly_map_url,\n            'cumulative_map_url': cumulative_map_url,\n            'csv_url': csv_url\n        }\n        attachments = []\n\n        try:\n            send_email_with_disclaimer_and_links(\n                service=service,\n                sender_email=sender_email,\n                to_emails=to_list,\n                subject=subject,\n                body_text=body_text,\n                attachments=attachments\n            )\n        except Exception as e:\n            logging.error(f\"Failed to send email: {e}\")\n            print(f\"[ERROR] Could not send email: {e}\")\n\n    end_time = time.time()\n    elapsed_sec = end_time - start_time\n    logging.info(f\"Finished full_crime_report in {elapsed_sec:.2f} seconds.\")\n    print(f\"Finished full_crime_report in {elapsed_sec:.2f} seconds.\")\n\nExplanation:\n\nPipeline Steps:\n\nEnvironment Variables: Loads necessary configurations like API keys and GitHub repository paths.\nData Loading: Retrieves the complete set of crime data from summary_report.zip.\nData Filtering: Selects records from the past seven days to focus the weekly report.\nCSV Generation: Exports the filtered data to a CSV file for easy access and dissemination.\nMap Creation:\n\nWeekly Map: Generates an interactive map highlighting crimes from the past week.\nCumulative Map: Generates a comprehensive map showcasing all recorded crimes.\n\nGitHub Upload: Pushes the newly generated HTML maps and CSV reports to designated GitHub repository subfolders.\nEmail Preparation and Sending:\n\nGmail API Authentication: Ensures secure access to the Gmail API for sending emails.\nRecipient Loading: Retrieves the list of subscribers.\nEmail Composition: Crafts an email containing links to the latest reports and attaches the CSV file.\nEmail Dispatch: Sends the email to all subscribers.\n\nLogging: Records the entire process’s execution details, including any errors and execution time.\n\nTesting Mode:\n\nFlag: The test variable allows toggling between testing and production modes. When test = False, the script proceeds with uploading and emailing. This is useful for development and debugging without affecting live data or subscribers.\n\nError Handling:\n\nTry-Except Blocks: Enclose critical operations to catch and log exceptions, ensuring the script fails gracefully and provides informative error messages.\n\n\n\n\ng. Main Entry Point\n\n\nCode\n\ndef main():\n    \"\"\"\n    Simply runs the report generation from the command line; no Streamlit involved.\n    \"\"\"\n    main_report_generation()\n\nif __name__ == \"__main__\":\n    main()\n\nExplanation:\n\nPurpose: Defines the script’s entry point, initiating the entire report generation process when the script is executed.\nFunction Call: Invokes main_report_generation() to start the pipeline.\n\n\n\n\n\nDetailed Code Breakdown\nLet’s delve deeper into specific functions and sections of weekly_crime_report.py to understand their roles and implementations.\n\na. Mailchimp Subscribers Fetching\n\n\nCode\n\ndef get_mailchimp_subscribers():\n    \"\"\"\n    Fetches all subscribed members from the Mailchimp audience.\n    Handles pagination to retrieve all subscribers.\n    \"\"\"\n    MAILCHIMP_API_KEY = os.getenv(\"MAILCHIMP_API_KEY\")\n    MAILCHIMP_AUDIENCE_ID = os.getenv(\"MAILCHIMP_AUDIENCE_ID\")\n    MAILCHIMP_DATA_CENTER = os.getenv(\"MAILCHIMP_DATA_CENTER\")  # e.g., 'us1', 'us2'\n\n    if not all([MAILCHIMP_API_KEY, MAILCHIMP_AUDIENCE_ID, MAILCHIMP_DATA_CENTER]):\n        raise ValueError(\"Missing Mailchimp API configuration in environment variables.\")\n\n    MAILCHIMP_API_URL = f\"https://{MAILCHIMP_DATA_CENTER}.api.mailchimp.com/3.0\"\n    endpoint = f\"/lists/{MAILCHIMP_AUDIENCE_ID}/members\"\n    params = {\n        \"status\": \"subscribed\",\n        \"count\": 1000,  # Max allowed by Mailchimp\n        \"offset\": 0\n    }\n    subscribers = []\n\n    while True:\n        response = requests.get(\n            MAILCHIMP_API_URL + endpoint,\n            auth=(\"anystring\", MAILCHIMP_API_KEY),\n            params=params\n        )\n\n        if response.status_code != 200:\n            logging.error(f\"Failed to fetch subscribers: {response.status_code} - {response.text}\")\n            raise Exception(f\"Mailchimp API Error: {response.status_code} - {response.text}\")\n\n        data = response.json()\n        members = data.get('members', [])\n        subscribers.extend([member['email_address'] for member in members])\n\n        total_items = data.get('total_items', 0)\n        if len(subscribers) &gt;= total_items:\n            break\n        params['offset'] += params['count']\n        time.sleep(1)  # To respect API rate limits\n\n    logging.info(f\"Fetched {len(subscribers)} subscribers from Mailchimp.\")\n    return subscribers\n\nExplanation:\n\nPurpose: Retrieves all email subscribers from the specified Mailchimp audience list.\nProcess:\n\nAPI Configuration: Fetches Mailchimp API credentials from environment variables.\nPagination Handling: Mailchimp’s API returns a maximum of 1,000 subscribers per request. This function loops through pages until all subscribers are retrieved.\nAPI Requests: Sends GET requests to the Mailchimp API, authenticating with the API key.\nData Extraction: Extracts email addresses from the response and appends them to the subscribers list.\nRate Limiting: Introduces a 1-second pause between requests to comply with Mailchimp’s rate limits.\n\nError Handling: Logs and raises exceptions for failed API requests.\n\nUsage: This function ensures that the email dissemination process targets all current subscribers, keeping them updated with the latest crime reports.\n\n\nb. Date Range Determination\n\n\nCode\n\ndef determine_date_range(df, execution_date):\n    \"\"\"\n    Determines the date range for the report based on the execution_date (7 days).\n    \"\"\"\n    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n    df_valid = df[df['Date'].notna()].copy()\n    if df_valid.empty:\n        raise ValueError(\"No valid dates found in the data.\")\n\n    latest_date = df_valid[df_valid['Date'].dt.date &lt;= execution_date]['Date'].max().date()\n    if pd.isna(latest_date):\n        raise ValueError(\"No records found on or before today's date.\")\n\n    # last 7 days\n    end_date = latest_date\n    start_date = end_date - timedelta(days=6)\n\n    return start_date, end_date\n\nExplanation:\n\nPurpose: Establishes the start and end dates for the weekly report based on the current execution date.\nLogic:\n\nDate Conversion: Ensures that the ‘Date’ column is in datetime format, removing invalid entries.\nLatest Date Identification: Finds the most recent date in the dataset that is on or before the execution date.\nDate Range Calculation: Sets the end date as the latest date and the start date as six days prior, encompassing a full week.\n\nError Handling: Raises exceptions if no valid dates are found or if no records exist up to the execution date.\n\nUsage: This function ensures that the weekly report accurately reflects the most recent week’s data, maintaining the report’s relevance and timeliness.\n\n\nc. Crime Data Filtering\n\n\nCode\n\ndef filter_crime_data(df, start_date, end_date):\n    \"\"\"\n    Filters the DataFrame to entries between start_date and end_date, inclusive.\n    \"\"\"\n    start_dt = pd.to_datetime(start_date)\n    end_dt   = pd.to_datetime(end_date) + timedelta(days=1) - timedelta(seconds=1)\n    mask = (df['Date'] &gt;= start_dt) & (df['Date'] &lt;= end_dt)\n    return df.loc[mask].copy()\n\nExplanation:\n\nPurpose: Selects crime records that fall within the specified date range.\nProcess:\n\nDate Conversion: Converts start_date and end_date to datetime objects.\nMask Creation: Creates a boolean mask to filter records where the ‘Date’ falls within the range.\nData Selection: Applies the mask to the DataFrame and returns a copy of the filtered data.\n\n\nUsage: This function isolates the relevant crime data for the weekly report, ensuring that only pertinent records are included in the generated maps and CSV reports.\n\n\nd. Crime Data Loading\n\n\nCode\n\ndef load_all_crimes(zip_file_path):\n    \"\"\"\n    Reads summary_report.zip containing your full crime data (summary_report.csv).\n    \"\"\"\n    if not zip_file_path.exists():\n        raise FileNotFoundError(f\"Could not find zip file '{zip_file_path}'\")\n\n    with zipfile.ZipFile(zip_file_path, 'r') as z:\n        with z.open('summary_report.csv') as csvfile:\n            df = pd.read_csv(csvfile, encoding='cp1252', on_bad_lines='skip')\n\n    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n    df = df[df['Date'].notna()]\n    df.sort_values(by='Date', ascending=False, inplace=True)\n    return df\n\nExplanation:\n\nPurpose: Loads the complete set of crime data from a compressed ZIP file containing summary_report.csv.\nProcess:\n\nZIP Extraction: Opens the ZIP file and extracts the CSV file.\nCSV Reading: Reads the CSV data into a Pandas DataFrame, skipping bad lines to ensure data integrity.\nDate Validation: Converts the ‘Date’ column to datetime format and removes records with invalid dates.\nData Sorting: Sorts the DataFrame in descending order based on the ‘Date’ to prioritize recent records.\n\nError Handling: Raises a FileNotFoundError if the ZIP file does not exist.\n\nUsage: This function provides a reliable method to access the entire crime dataset, forming the basis for generating both weekly and cumulative reports.\n\n\ne. Safe Field Handling\n\n\nCode\n\ndef safe_field(value):\n    \"\"\"\n    Return the string version of a field or 'Not found' if it's missing/NaN/empty.\n    \"\"\"\n    if pd.isnull(value) or value == \"\":\n        return \"Not found\"\n    return str(value)\n\nExplanation:\n\nPurpose: Ensures that all fields included in the reports are present and readable. If a field is missing (NaN) or empty, it returns “Not found” to maintain consistency in the reports.\n\n\n\nf. Cumulative Map Creation\n\n\nCode\n\ndef create_folium_map_cumulative(\n    df,\n    lat_col='Lat',\n    lng_col='Long',\n    offense_col='Offense',\n    date_col='Date',\n    output_html_path='cumulative_map.html'\n):\n    \"\"\"\n    Creates a Folium map plotting all records in df up to a certain date with disclaimers overlay (JS + HTML) \n    and saves it to 'output_html_path'.\n    \"\"\"\n    oak_park_center = [41.885, -87.78]\n    crime_map = folium.Map(location=oak_park_center, zoom_start=11)  # Zoomed out for cumulative view\n\n    marker_cluster = MarkerCluster().add_to(crime_map)\n    # Define the static part of the base URL\n    base_url_static = 'https://www.oak-park.us/sites/default/files/police/summaries/'\n\n    for _, row in df.iterrows():\n        lat = row[lat_col]\n        lng = row[lng_col]\n        offense = row.get(offense_col, \"Unknown\")\n        complaint = safe_field(row.get('Complaint #'))\n        offense_val = safe_field(offense)\n        date_str = safe_field(row['Date'].strftime('%Y-%m-%d') if pd.notnull(row['Date']) else np.nan)\n        time_val = safe_field(row.get('Time'))\n        location = safe_field(row.get('Location'))\n        victim = safe_field(row.get('Victim/Address'))\n        narrative = safe_field(row.get('Narrative'))\n        filename = safe_field(row.get('File Name'))\n\n        popup_html = f\"\"\"\n            &lt;b&gt;Complaint #:&lt;/b&gt; {complaint}&lt;br/&gt;\n            &lt;b&gt;Offense:&lt;/b&gt; {offense_val}&lt;br/&gt;\n            &lt;b&gt;Date:&lt;/b&gt; {date_str}&lt;br/&gt;\n            &lt;details&gt;\n              &lt;summary&gt;&lt;b&gt;View Details&lt;/b&gt;&lt;/summary&gt;\n              &lt;b&gt;Time:&lt;/b&gt; {time_val}&lt;br/&gt;\n              &lt;b&gt;Location:&lt;/b&gt; {location}&lt;br/&gt;\n              &lt;b&gt;Victim:&lt;/b&gt; {victim}&lt;br/&gt;\n              &lt;b&gt;Narrative:&lt;/b&gt; {narrative}&lt;br/&gt;\n              &lt;b&gt;URL:&lt;/b&gt; &lt;a href=\"{filename}\" target=\"_blank\"&gt;PDF Link&lt;/a&gt;\n            &lt;/details&gt;\n        \"\"\"\n\n        folium.Marker(\n            location=[lat, lng],\n            popup=folium.Popup(popup_html, max_width=300),\n            icon=folium.Icon(color='blue', icon='info-sign')  # Different color for distinction\n        ).add_to(marker_cluster)\n\n    # Basic map title\n    title_html = '''\n    &lt;h3 align=\"center\" style=\"font-size:20px\"&gt;&lt;b&gt;Oak Park Cumulative Crime Map&lt;/b&gt;&lt;/h3&gt;\n    &lt;br&gt;\n    &lt;h3 align=\"center\" style=\"font-size:10px\"&gt;\n    &lt;a href=\"https://jesse-anderson.net/\"&gt;My Portfolio&lt;/a&gt; |\n    &lt;a href=\"https://blog.jesse-anderson.net/\"&gt;My Blog&lt;/a&gt; |\n    &lt;a href=\"https://blog.jesse-anderson.net/\"&gt;Documentation&lt;/a&gt; |\n    &lt;a href=\"mailto:jesse@jesse-anderson.net\"&gt;Contact&lt;/a&gt; |\n    &lt;a href=\"https://forms.gle/GnyaVwo1Vzm8nBH6A\"&gt;\n        Add me to Weekly Updates\n    &lt;/a&gt;\n    &lt;/h3&gt;\n'''\n    crime_map.get_root().html.add_child(folium.Element(title_html))\n\n    # Overlays disclaimers in a \"splash screen\" with JavaScript (reuse existing disclaimer)\n    disclaimers_overlay = \"\"\"\n    &lt;style&gt;\n    /* Full-page overlay styling */\n    #disclaimerOverlay {\n      position: fixed;\n      z-index: 9999; /* On top of everything */\n      left: 0;\n      top: 0;\n      width: 100%;\n      height: 100%;\n      background-color: rgba(255, 255, 255, 0.95);\n      color: #333;\n      display: block; /* Visible by default */\n      overflow: auto;\n      text-align: center;\n      padding-top: 100px;\n      font-family: Arial, sans-serif;\n    }\n    #disclaimerContent {\n      background: #f9f9f9;\n      border: 1px solid #ccc;\n      display: inline-block;\n      padding: 20px;\n      max-width: 800px;\n      text-align: left;\n    }\n    #acceptButton {\n      margin-top: 20px;\n      padding: 10px 20px;\n      font-size: 16px;\n      cursor: pointer;\n    }\n    &lt;/style&gt;\n\n    &lt;div id=\"disclaimerOverlay\"&gt;\n      &lt;div id=\"disclaimerContent\"&gt;\n        &lt;h2&gt;Important Legal Disclaimer&lt;/h2&gt;\n        &lt;p&gt;&lt;strong&gt;By using this demonstrative research tool, you acknowledge and agree:&lt;/strong&gt;&lt;/p&gt;\n        &lt;ul&gt;\n            &lt;li&gt;This tool is for &lt;strong&gt;demonstration purposes only&lt;/strong&gt;.&lt;/li&gt;\n            &lt;li&gt;The data originated from publicly available Oak Park Police Department PDF files.\n                View the official site here: \n                &lt;a href=\"https://www.oak-park.us/village-services/police-department\"\n                   target=\"_blank\"&gt;Oak Park Police Department&lt;/a&gt;.&lt;/li&gt;\n            &lt;li&gt;During parsing, &lt;strong&gt;~10%&lt;/strong&gt; of complaints were &lt;strong&gt;omitted&lt;/strong&gt; \n                due to parsing issues; thus the data is &lt;strong&gt;incomplete&lt;/strong&gt;.&lt;/li&gt;\n            &lt;li&gt;The &lt;strong&gt;official&lt;/strong&gt; and &lt;strong&gt;complete&lt;/strong&gt; PDF files remain \n                with the Oak Park Police Department.&lt;/li&gt;\n            &lt;li&gt;You &lt;strong&gt;will not hold&lt;/strong&gt; the author &lt;strong&gt;liable&lt;/strong&gt; for &lt;strong&gt;any&lt;/strong&gt; \n                decisions—formal or informal—based on this tool.&lt;/li&gt;\n            &lt;li&gt;This tool &lt;strong&gt;should not&lt;/strong&gt; be used in &lt;strong&gt;any&lt;/strong&gt; official or unofficial \n                &lt;strong&gt;decision-making&lt;/strong&gt;.&lt;/li&gt;\n        &lt;/ul&gt;\n        &lt;p&gt;&lt;strong&gt;By continuing, you indicate your acceptance of these terms \n           and disclaim all liability.&lt;/strong&gt;&lt;/p&gt;\n        &lt;hr/&gt;\n        &lt;button id=\"acceptButton\" onclick=\"hideOverlay()\"&gt;I Accept&lt;/button&gt;\n      &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;script&gt;\n    function hideOverlay() {\n      var overlay = document.getElementById('disclaimerOverlay');\n      overlay.style.display = 'none'; \n    }\n    &lt;/script&gt;\n    \"\"\"\n\n    disclaimers_element = folium.Element(disclaimers_overlay)\n    crime_map.get_root().html.add_child(disclaimers_element)\n\n    # Save final HTML\n    crime_map.save(str(output_html_path))\n\nExplanation:\n\nMap Initialization:\n\nCentering: Centers the Folium map on Oak Park’s geographical coordinates with a zoom level of 11 for a broader view in the cumulative map.\n\nMarker Creation:\n\nCustomization: Each crime incident is represented by a blue marker with an info-sign icon for distinction from the weekly map’s red markers.\nPopups: Similar to the weekly map, but tailored for cumulative data.\n\nTitle Addition:\n\nHTML Styling: Adds a title and navigation links directly onto the Folium map using HTML and inline CSS.\n\nDisclaimer Overlay:\n\nReuse: Reuses the same disclaimer overlay mechanism as in the weekly map to ensure consistency.\n\nMap Saving:\n\nOutput: Saves the generated cumulative map with overlays to the specified HTML file path.\n\n\n\n\ng. Gmail API Service Setup\n\n\nCode\n\ndef get_gmail_service():\n    \"\"\"\n    Authenticates the user and returns the Gmail API service.\n    \"\"\"\n    creds = None\n    token_path = Path('token.json')\n\n    if token_path.exists():\n        creds = Credentials.from_authorized_user_file(str(token_path), SCOPES)\n\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            try:\n                creds.refresh(Request())\n                logging.info(\"Credentials refreshed successfully.\")\n            except Exception as e:\n                logging.error(f\"Error refreshing credentials: {e}\")\n                print(f\"[ERROR] Could not refresh credentials: {e}\")\n                return None\n        else:\n            try:\n                flow = InstalledAppFlow.from_client_secrets_file(\n                    'credentials.json', SCOPES\n                )\n                creds = flow.run_local_server(port=0)\n                logging.info(\"Authentication flow completed successfully.\")\n            except Exception as e:\n                logging.error(f\"Error during OAuth flow: {e}\")\n                print(f\"[ERROR] Could not complete OAuth flow: {e}\")\n                return None\n\n        try:\n            with open(token_path, 'w') as token:\n                token.write(creds.to_json())\n                logging.info(\"Credentials saved to token.json.\")\n        except Exception as e:\n            logging.error(f\"Failed to save credentials: {e}\")\n            print(f\"[ERROR] Could not save credentials: {e}\")\n\n    try:\n        service = build('gmail', 'v1', credentials=creds)\n        logging.info(\"Gmail service created successfully.\")\n        return service\n    except HttpError as error:\n        logging.error(f\"An error occurred while building Gmail service: {error}\")\n        print(f\"[ERROR] An error occurred while building Gmail service: {error}\")\n        return None\n\nExplanation:\n\nPurpose: Authenticates the user and establishes a connection with the Gmail API to send emails.\nAuthentication Flow:\n\nToken Check: Looks for existing credentials in token.json.\nToken Refresh: If credentials are expired but have a refresh token, it attempts to refresh them.\nOAuth Flow: If no valid credentials exist, initiates the OAuth flow using credentials.json.\nCredential Saving: Saves the new or refreshed credentials back to token.json for future use.\n\nError Handling: Logs and prints errors encountered during authentication or service creation.\n\nUsage: This function ensures secure and authenticated access to the Gmail API, enabling the script to send automated emails containing the latest crime reports.\n\n\nh. Email Sending Function\n\n\nCode\n\ndef send_email_with_disclaimer_and_links(\n    service,\n    sender_email,\n    to_emails,\n    subject,\n    body_text,\n    attachments\n):\n    \"\"\"\n    Sends an email with disclaimers and links using the Gmail API.\n    \"\"\"\n    try:\n        message = MIMEMultipart()\n        message['to'] = \"Undisclosed Recipients &lt;jesse@jesse-anderson.net&gt;\"\n        message['subject'] = subject\n        message['from'] = sender_email\n\n        # Define the plain text content\n        plain_text = f\"\"\"\n        Important Legal Disclaimer\n        \n        By using this demonstrative research tool, you acknowledge and agree:\n\n        - This tool is for demonstration purposes only.\n        - The data originated from publicly available Oak Park Police Department PDF files.\n          View the official site here: https://www.oak-park.us/village-services/police-department.\n        - During parsing, ~10% of total complaints since 2018 were omitted due to parsing issues; \n          thus the data is incomplete.\n        - The official and complete PDF files remain with the Oak Park Police Department.\n        - You will not hold the author liable for any decisions—formal or informal—based on this tool.\n        - This tool should not be used in any official or unofficial decision-making.\n\n        By continuing, you indicate your acceptance of these terms and disclaim all liability.\n\n        ------------\n\n        Hello,\n        The crime report from {body_text['start_date']} to {body_text['end_date']} is attached as a .csv file.\n\n        Interactive map:\n        {body_text['weekly_map_url']}\n\n        Cumulative map:\n        {body_text['cumulative_map_url']}\n        \n        Last week's data:\n        {body_text['csv_url']}\n        \"\"\"\n\n        part1 = MIMEText(plain_text, 'plain')\n        message.attach(part1)\n\n        bcc_emails = \", \".join(to_emails)\n        message['bcc'] = bcc_emails\n\n        # Attach the CSV\n        for file_path in attachments:\n            file_path = Path(file_path)\n            if not file_path.exists():\n                logging.warning(f\"Attachment '{file_path}' not found, skipping.\")\n                continue\n            with open(file_path, 'rb') as f:\n                mime_application = MIMEApplication(f.read(), Name=file_path.name)\n            mime_application['Content-Disposition'] = f'attachment; filename=\"{file_path.name}\"'\n            message.attach(mime_application)\n\n        raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n        body = {'raw': raw_message}\n\n        sent_message = service.users().messages().send(userId=\"me\", body=body).execute()\n        logging.info(f\"Email sent successfully. Message ID: {sent_message['id']}\")\n        print(\"Crime report email sent successfully.\")\n    except Exception as e:\n        logging.error(f\"Failed to send email: {e}\")\n        print(f\"[ERROR] Failed to send email: {e}\")\n\nExplanation:\n\nPurpose: Composes and sends an email containing the latest crime reports and links to interactive maps.\nStructure:\n\nEmail Composition:\n\nPlain Text: Provides a disclaimer and details about the latest crime reports.\nAttachments: Attaches the filtered CSV report.\n\nBCC: Sends the email to undisclosed recipients to maintain privacy.\n\nEncoding: Encodes the email content in base64 to comply with Gmail API requirements.\nSending: Utilizes the authenticated Gmail service to send the email.\nError Handling: Logs and prints errors encountered during the email sending process.\n\n\n\ni. Main Report Generation Workflow\n\n\nCode\n\ndef main_report_generation():\n    \"\"\"\n    Executes the full pipeline:\n    1. Load environment variables\n    2. Load & filter data for the last 7 days\n    3. Create & save Folium map with disclaimers overlay\n    4. Upload HTML to GitHub\n    5. Send email with CSV attached\n    \"\"\"\n    start_time = time.time()\n    script_dir = Path(__file__).parent.resolve()\n\n    env_file_path = script_dir / \"env_vars.txt\"\n    try:\n        load_env_vars(env_file_path)\n    except FileNotFoundError as e:\n        print(f\"[ERROR] {e}\")\n        return\n\n    sender_email = os.getenv(\"SENDER_EMAIL\")\n    if not sender_email:\n        raise ValueError(\"Missing SENDER_EMAIL in env_vars.txt\")\n\n    logging.basicConfig(\n        filename=script_dir / 'full_crime_report.log',\n        level=logging.DEBUG,\n        format='%(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    data_dir = script_dir / 'data'\n    map_dir = script_dir / 'generated_maps'\n    # recipients_csv = script_dir / 'recipients.csv'\n    csv_dir = script_dir / 'generated_csvs'  # New directory for CSVs\n    map_dir.mkdir(parents=True, exist_ok=True)\n    csv_dir.mkdir(parents=True, exist_ok=True)\n\n    zip_file_path = data_dir / 'summary_report.zip'\n\n    execution_date = datetime.now().date()\n    date_str = execution_date.strftime('%Y-%m-%d')\n\n    # CSV & HTML output\n    filtered_subset_filename = f'filtered_subset_{date_str}.csv'\n    filtered_subset_path = csv_dir / filtered_subset_filename\n    weekly_map_output_filename = f'crime_map_weekly_{date_str}.html'\n    weekly_map_output_path = map_dir / weekly_map_output_filename\n    cumulative_map_output_filename = f'crime_map_cumulative.html'\n    cumulative_map_output_path = map_dir / cumulative_map_output_filename\n\n    # Local GitHub Pages folder\n    github_repo_path = os.getenv(\"GITHUB_REPO\")\n    github_repo_path = Path(github_repo_path)\n\n    # (C) Load data\n    try:\n        df_full = load_all_crimes(zip_file_path)\n    except Exception as e:\n        logging.error(f\"Failed to load all crimes: {e}\")\n        print(f\"[ERROR] Could not load all crimes: {e}\")\n        return\n\n    if df_full.empty:\n        logging.info(\"No crime data—no map or CSV generated.\")\n        print(\"No crime data—no map or CSV generated.\")\n        return\n\n    # (D) Filter last 7 days\n    try:\n        start_date, end_date = determine_date_range(df_full, execution_date)\n        df_filtered = filter_crime_data(df_full, start_date, end_date)\n    except Exception as e:\n        logging.error(f\"Error determining date range: {e}\")\n        print(f\"[ERROR] Could not determine date range: {e}\")\n        return\n\n    if df_filtered.empty:\n        logging.info(\"No crimes found—no map or CSV generated.\")\n        print(\"No crimes found in the determined date range—no map or CSV generated.\")\n        return\n\n    # (E) Write filtered CSV\n    try:\n        df_filtered.to_csv(filtered_subset_path, index=False, encoding=\"cp1252\")\n        logging.info(f\"Filtered data written to {filtered_subset_path}\")\n    except Exception as e:\n        logging.error(f\"Failed to write filtered data to CSV: {e}\")\n        print(f\"[ERROR] Could not write filtered data to CSV: {e}\")\n        return\n\n    # (F) Create Folium map with disclaimers overlay\n    try:\n        create_folium_map_filtered_data(\n            df=df_filtered,\n            lat_col='Lat',\n            lng_col='Long',\n            offense_col='Offense',\n            date_col='Date',\n            output_html_path=weekly_map_output_path\n        )\n        logging.info(f\"Folium map created at {weekly_map_output_path}\")\n    except Exception as e:\n        logging.error(f\"Error creating Folium map: {e}\")\n        print(f\"[ERROR] Could not create Folium map: {e}\")\n        return\n\n    try:\n        create_folium_map_cumulative(\n            df=df_full,  # Use the full dataset for cumulative map\n            lat_col='Lat',\n            lng_col='Long',\n            offense_col='Offense',\n            date_col='Date',\n            output_html_path=cumulative_map_output_path\n        )\n        logging.info(f\"Cumulative Folium map created at {cumulative_map_output_path}\")\n    except Exception as e:\n        logging.error(f\"Error creating cumulative Folium map: {e}\")\n        print(f\"[ERROR] Could not create cumulative Folium map: {e}\")\n        return\n    \n    test = False\n    if not test:\n        # (G) Upload Map and csv\n        try:\n            # Upload Maps\n            files_to_upload_maps = [weekly_map_output_path, cumulative_map_output_path]\n            upload_files_to_github_batch(\n                file_paths=files_to_upload_maps,\n                github_repo_path=github_repo_path,\n                target_subfolder='OP-Crime-Maps'\n            )\n            time.sleep(5) #paranoia\n            # Upload CSV\n            files_to_upload_csv = filtered_subset_path\n            upload_file_to_github(\n                file_path=files_to_upload_csv,\n                github_repo_path=github_repo_path,\n                target_subfolder='OP-Crime-Data'\n            )\n        except Exception as e:\n            logging.error(f\"Failed to upload files to GitHub: {e}\")\n            print(f\"[ERROR] Could not upload files to GitHub: {e}\")\n            return\n        # (I) Generate GitHub URLs for the uploaded files\n        # Assuming GitHub Pages are served from the root of the repository\n        github_base_url = \"https://jesse-anderson.github.io\"\n\n        weekly_map_url = f\"{github_base_url}/OP-Crime-Maps/{weekly_map_output_filename}\"\n        cumulative_map_url = f\"{github_base_url}/OP-Crime-Maps/{cumulative_map_output_filename}\"\n        csv_url = f\"{github_base_url}/OP-Crime-Data/{filtered_subset_filename}\"\n\n        # (J) Gmail API & Email\n        try:\n            service = get_gmail_service()\n            if not service:\n                raise Exception(\"Failed to create Gmail service.\")\n        except Exception as e:\n            logging.error(f\"Authentication failed: {e}\")\n            print(f\"[ERROR] Authentication failed: {e}\")\n            return\n        time.sleep(60) #time to build github pages....\n        # (K) Load Recipients\n        try:\n            # to_list = load_recipients_list(recipients_csv)\n            # to_list = get_mailchimp_subscribers()\n            to_list = [\"myemail@gmail.com\"]\n        except FileNotFoundError as e:\n            logging.error(f\"Error loading recipients: {e}\")\n            print(f\"[ERROR] {e}\")\n            return\n\n        if not to_list:\n            logging.warning(\"No recipients found—cannot send email.\")\n            print(\"No recipients found in recipients.csv—cannot send email.\")\n            return\n\n        subject = f\"Crime Report from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\"\n        body_text = {\n            'start_date': start_date.strftime('%Y-%m-%d'),\n            'end_date': end_date.strftime('%Y-%m-%d'),\n            'weekly_map_url': weekly_map_url,\n            'cumulative_map_url': cumulative_map_url,\n            'csv_url': csv_url\n        }\n        attachments = []\n\n        try:\n            send_email_with_disclaimer_and_links(\n                service=service,\n                sender_email=sender_email,\n                to_emails=to_list,\n                subject=subject,\n                body_text=body_text,\n                attachments=attachments\n            )\n        except Exception as e:\n            logging.error(f\"Failed to send email: {e}\")\n            print(f\"[ERROR] Could not send email: {e}\")\n\n    end_time = time.time()\n    elapsed_sec = end_time - start_time\n    logging.info(f\"Finished full_crime_report in {elapsed_sec:.2f} seconds.\")\n    print(f\"Finished full_crime_report in {elapsed_sec:.2f} seconds.\")\n\nExplanation:\n\nPipeline Steps:\n\nEnvironment Variables: Loads necessary configurations like API keys and GitHub repository paths.\nData Loading: Retrieves the complete set of crime data from summary_report.zip.\nData Filtering: Selects records from the past seven days to focus the weekly report.\nCSV Generation: Exports the filtered data to a CSV file for easy access and dissemination.\nMap Creation:\n\nWeekly Map: Generates an interactive map highlighting crimes from the past week.\nCumulative Map: Generates a comprehensive map showcasing all recorded crimes.\n\nGitHub Upload: Pushes the newly generated HTML maps and CSV reports to designated GitHub repository subfolders.\nEmail Preparation and Sending:\n\nGmail API Authentication: Ensures secure access to the Gmail API for sending emails.\nRecipient Loading: Retrieves the list of subscribers.\nEmail Composition: Crafts an email containing links to the latest reports and attaches the CSV file.\nEmail Dispatch: Sends the email to all subscribers.\n\nLogging: Records the entire process’s execution details, including any errors and execution time.\n\nTesting Mode:\n\nFlag: The test variable allows toggling between testing and production modes. When test = False, the script proceeds with uploading and emailing. This is useful for development and debugging without affecting live data or subscribers.\n\nError Handling:\n\nTry-Except Blocks: Enclose critical operations to catch and log exceptions, ensuring the script fails gracefully and provides informative error messages.\n\n\n\n\n\nConclusion of Static HTML Generation Section\nThe Static HTML Generation component plays a crucial role in ensuring that the Oak Park Crime Reporting project delivers timely and accessible information. By automating the creation of detailed reports and maps, integrating seamlessly with GitHub for hosting, and utilizing the Gmail API for dissemination, the script ensures that stakeholders and interested parties receive up-to-date crime data efficiently. Robust error handling and logging mechanisms further enhance the reliability and maintainability of the reporting pipeline."
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#conclusion",
    "href": "posts/OP-Crime-Documentation/index.html#conclusion",
    "title": "Oak Park Crime Reporting",
    "section": "Conclusion",
    "text": "Conclusion\nThe Oak Park Crime Reporting project is a comprehensive solution that automates the extraction, processing, visualization, and dissemination of crime data within Oak Park. By leveraging powerful Python libraries and integrations with platforms like GitHub and Gmail, the project ensures that crime data is both accessible and actionable. The meticulous documentation of each component—Data Parsing, Live Streamlit Dashboard, and Static HTML Generation—provides a clear roadmap for understanding, maintaining, and potentially expanding the project’s capabilities."
  },
  {
    "objectID": "posts/OP-Crime-Documentation/index.html#future-enhancements",
    "href": "posts/OP-Crime-Documentation/index.html#future-enhancements",
    "title": "Oak Park Crime Reporting",
    "section": "Future Enhancements",
    "text": "Future Enhancements\nWhile the current implementation of the Oak Park Crime Reporting project is robust, there are several avenues for future enhancements:\n\nEnhanced Data Parsing:\n\nMachine Learning Integration: Implement machine learning models to improve the accuracy of data extraction from PDFs, especially for complex narratives. Currently the narratives have random spaces which does not lend itself to easy reading, but manually processing 7,000+ records and/or spending more time coding does not seem logical.\nAutomated Data Validation: Introduce automated checks to validate the integrity and consistency of the parsed data.\n\nDashboard Improvements:\n\nAdvanced Visualization: Incorporate additional visualization tools like heatmaps, trend graphs, and statistical summaries.\nUser Authentication: Add authentication layers to restrict access to sensitive data and functionalities.\nReal-Time Updates: Enable real-time data streaming to keep the dashboard updated without manual interventions.\n\nStatic Report Enhancements:\n\nInteractive Elements: Introduce interactive components within the static HTML reports for better user engagement such as advanced filtering.\nResponsive Design: Ensure that the static reports are mobile-friendly and adapt seamlessly to various screen sizes.\n\nEmail System Upgrades:\n\nPersonalization: Personalize emails based on user preferences and subscription tiers. such as daily, weekly, monthly, quarterly, and yearly.\nEmail Scheduling: Implement scheduled email dispatches to send reports at predefined intervals automatically beyond simple Cronjobs.\n\nScalability and Performance:\n\nCloud Deployment: Migrate components to cloud platforms for better scalability, reliability, and performance. This is currently not implemented as that would cost money and the balance is such that this project is not bringing in any funds.\nDatabase Integration: Utilize databases like PostgreSQL or MongoDB for more efficient data storage and querying once the storage records begin to exceed 10,000.\n\nSecurity Enhancements:\n\nData Encryption: Encrypt sensitive data both at rest and in transit to ensure data privacy. While this is already done, it can always be improved upon.\nSecure Authentication: Strengthen authentication mechanisms for API integrations and user access. Already done, but improvement is relentless.\n\nUser Feedback Mechanism:\n\nFeedback Forms: Incorporate feedback forms within the dashboard and reports to gather user insights and suggestions.\nAnalytics: Implement analytics to monitor user interactions and improve the platform based on usage patterns.\n\n\nBy pursuing these enhancements, the Oak Park Crime Reporting project can evolve into an even more powerful tool, providing deeper insights and greater value to its users.\n\nI hope you have enjoyed reading this documentation and sincerely hope you got something out of it.\nJesse"
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html",
    "href": "posts/OPTICSWriteup/index.html",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Within the realm of unsupervised machine learning, clustering methods play a critical role in unraveling patterns and groupings within datasets where the labels are unknown. Among these clustering techniques, OPTICS(Ordering Points To Identify the Clustering Structure) stands out as a particularly powerful tool when dealing with complex datasets with varying densities or non-globular shapes. This exploration of OPTICS should get you up to speed on the basics of what OPTICS does and will help you understand its mechanics, benefits, and practical applications.\n\n\nDeveloped in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures.\n\n\n\nThe core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it.\n\n\n\n\n\n\nOPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management.\n\n\n\n\n\nTo determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets.\n\n\n\n\n\nXi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually.\n\n\n\n\nAs datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Developed in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "href": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "The core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "href": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "OPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "To determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Xi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "href": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "As datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/Pi-Sensor-Proj-May-2024/index.html",
    "href": "posts/Pi-Sensor-Proj-May-2024/index.html",
    "title": "Raspberry Pi Sensor Server Project",
    "section": "",
    "text": "I finally decided to use the Raspberry Pi 4 Model B+ 8gb I had lying around to play around with some sensors. Luckily I took an electrical engineering course in circuits[ECE210 at UIC], which made it pretty straight forward to wire things up. I had also already flashed an OS to the SD card and only encountered a few issues with booting up with the pi in its case(with fan!) and the temperature/humidity sensor plugged in. Please note that setting up VNC Server(RPI) and VNC Viewer(Desktop) will speed this up dramatically. Below is a pretty simple mockup of the connection I used with the Fan’s Power on pin 4[+,5.0VDC] and pin 14[-] and the DHT11 sensor on Pin 2[+,5.0VDC], Pin 6[-], and Pin 7[GPIO7].\nImage source\nAnd here’s the setup:\nAs a side note, Fritzing worked really well to generate the image above and I used the build at: https://github.com/Move2win/Fritzing-0.9.9.64.pc-Compiled-Build\nIt is a compiled .exe on a random github repository and one should take care….. but it was definitely faster than trying to build Fritzing from source.\nNext I got a basic python script working on my Raspberry Pi after installing the Adafruit_DHT library.\nInstallation was pretty straight forward. Enter this into the command prompt\nNext:\nChange directories:\nNow:\nFinally….:\nNow create a .py file and enter the following:\nThis script will grab the current date, time, Percent Humidity, Temp in Fahrenheit/Celsius and display it to the user. Note that we are appending it to a buffer which will become important later.\nNext I tried various server/serverless options to get realtime data and decided on Vercel. I also tried out ThingSpeak and really liked its interface, but the fact that I would have to pay(if I wasn’t a student) made me consider other options. To implement a basic realtime logging of sensor data in ThingSpeak one would sign up for an account, create a channel, populate a channel and add field labels such as Temperature and Humidity, and save the channel to receive a unique Channel ID and API key. The code for thingspeak is pretty straightforward and one can implement the code below to populate a ThingSpeak channel.\nThe resulting channel is functional enough:\nLocation: https://thingspeak.com/channels/2545447\nRealistically, I may incorporate sending the data to ThingSpeak as well as the other option I chose for monitoring.\nI got my account up and running with Vercel, installing it on a private Github repo. I then installed Node.js and npm. Then I navigated to the repo and opened up a command prompt:\nI then created a ‘/api’ directory and created a file for my sensor data handling called ‘/api/sensor.js’. Note the addition of the API_KEY variable that you should add to your Vercel global environment variables to make sure there’s some added security.\nFrom here I pushed the changes to github which caused the Vercel site to redeploy and changed the code on the Raspberry Pi to the following:\nI will omit the fact that I spent forever trying to also get MongoDB to work within Vercel and later found out that I needed to perform some sort of installation to get it to work. I did however find out that Vercel offered a PostgreSQL implementation so I could store my data as it came in. I navigated to Storage and found it was a few pretty simple clicks to get it going.\nI created a table using:\nAnd extended sensor.js a bit…Namely I edited it so it can handle a data_buffer of multiple points as well as singular points to cut down on server connection overhead. I also added some logging for the sake of sanity on the off chance anything ever goes wrong.\nNote, you need to install pg on your github directory for PostgreSQL to work.\nFrom there what my final product looks like is an html page which displays the latest sensor readings, a javascript function to pull the entire dataset, a javascript function which pushes the data into the database, and the python script on the raspberry pi sending the data. I actually bunch up the data before I push it to save on the overhead costs of establishing a connection. Realistically the data isn’t too time sensitive and a window of 1-5 minutes is perfectly acceptable for readings. They are below:\nHtml:\nJavascript to pull entire dataset:\nJavascript to display latest sensor readings:\nPython script on the raspberry pi:\nI intend to add more sensors such as a VOC sensor, CO2 sensor, PM2.5/PM10 sensor to have real time air quality data. That should be plug and play and a few lines of code. Getting the raspberry pi, server, and database to get along is a lot more work than wiring up a few sensors. I will also likely throw the data into a mongoDB and also push the data to ThingSpeak regularly once I have figured out what the best storage medium is. Unfortunately server uptime is counted as compute time for the purpose of using PostgreSQL in Vercel, so its great for testing, but definitely won’t be my long term solution. I might just do the unhinged option and use Google Sheets as a database. It should be possible to have up to 10 million cells which, when coupled with the data being logged at Date, Time, Humidity, TempF, and TempC that means I can have roughly 2 million rows before I need to think of pushing to another sheet. With a safety factor of 2 I have 1 million and that means I have 1,000,000/60 = 16,666 seconds/60 = 277 hours/24 = 11.5 days of data before I need to consider using another sheet. I can likely shorten this to 7 days and dynamically generate a new sheet every week.\n**Edit**: I actually went ahead and tried the google sheets option. Created a Google Cloud Project, enabled the Google Sheets API, and created credentials/downloaded the resultant JSON.\nI also had to make sure some Google Python libraries were installed:\nNext I implemented a pretty basic program to send a few values to a Google Sheet:\nOnce I was assured that the Google Sheets API was functioning correctly I tweaked the existing Python code to also send data to a Google Sheet. The sheet was set to read only globally so I could later on access it via my github.io site or similar via javascript. I also added a *.txt file for persistence across runs where that sheet contains my spreadsheet_id, workbook_name, and sheet_name so I can start and stop the Python script whenever I wanted. I also added back in the ThingSpeak code from before with two additional parameters, date and time. That way I can bypass the 15second update limit of ThingSpeak by sending bulk data every 15 seconds and I can also use the date/time parameters to generate a plot on the off chance that the datasent uses the timestamp of receipt as the X axis when plotting. The final result is the plot below:\nThe Matlab code used:\nNow as far as the Python code goes, it is pretty lengthy at this point so buyer beware:\nThat’s 3 services down, with one more crack at MongoDB. The tentative plan is to push everything from the Python script then use some form of authentication to eventually be able to grab the data directly from MongoDB but use the Vercel site to “host” the csv/data. A rough sketch of the setup is below:\nThis was fairly straightforward and with one function and a couple of lines of Python I was all set up. Note that I changed the timings, notably Vercel. As the server spins up and stays active for 5 minutes after it receives data. The limit for Compute time is 60 hrs/month, so at the basic vCPU of 0.6 I will be at 0.6*(5minutes/60minutes)*24hour*30days=36 hours if I update it hourly. This is just safe enough for me to do and as a bonus its not like this is an AWS instance with my Credit Card linked.\nTimings:\nCurrently, sending to all 4 servers, I experience a max “drop” of the sensor data of 3 seconds and that is running a .py file with a desktop running in the background.\nTo set up the .py script to start up at boot I did the following:\nAnd within that file:\nThis sets it up to run at boot and also to log the output.\nEnable and start the service:\nCheck the status:\nI made a separate service file to make sure I can see the log by firing up my pi and leaving it be:\nAnd within that file:\nThis will rotate the log file daily, keep 7 rotations, compress the old logs, and restart the service after every log rotation.\nNext set up a script that checks for specific keywords including errors and sends a notification:\nMake it executable:\nAccess Cron to schedule jobs:\nSet the job to run * * * * *, or every minute of every hour of every day of every month.\nCheck that it worked:\nFrom here I also wanted to display the output at boot as well. This is optional, but I wanna see a running log of what’s going on:\nEnable on boot:\nDisabling the desktop[Boot into Command Line Interface]:\nFinally reboot and you’re done!\nAfter setting up the python script to run at bootup and disabling the desktop I experience a drop of “1-2” seconds with a larger amount of 1 second drops than 2.\nI must have made some sort of mistake in getting the logging to work, but the data is being sent and that’s good enough for now.\nRunning this command after the pi boots up works well enough:\nAnd don’t forget that if you want to connect via SSH(if you didn’t already)”\nIf you want to get the desktop back simply type into the Pi console:\nNow for the results:\nTo get the Google Script working I deployed a Google Aps Script:\nGo to script.google.com and create a new project\nWrite a function to fetch data from my Google Sheets:\nAnd then finally use the web app URL in my client-side code. Note that I tried fetching it manually, but got a:\nLatest sensor reading Google Sheets(button loads latest data):\nLatest Sensor Reading ThingSpeak(press button):\nConfiguring MongoDB to grab the sensor readings and the CSV required me to do quite a bit of configuring of Vercel(timeout increase) and making sure that the MongoDB was no longer using fixed IP addresses(I’m not implementing some cursed dynamic IP allocation). I also implement a 3 second wait just in case there’s a lot of requests/readers(unlikely).\nLatest Sensor Reading MongoDB(Read-Only User):\nLatest Sensor Reading Vercel PostgreSQL(Read-Only User):\nFinal Python Script on the off chance something changed:\n*Note you can run with this pretty easily, change the flags to True for whatever “database” you’re using.*\nAnd there you have it. 4 different databases and the ability to write/read to them in near real time via a simple web portal. I could have used any of these individually but I wanted to explore the strengths/weaknesses of different configurations. I like the idea of just using a pretty long Google Sheet for most projects and completely ignoring actual databases for simple home automation. Using MongoDB was pretty straightforward via VSCode and it is a strong second choice. Tying for second would be ThingSpeak with its cool online interface, but I’m wary of Mathworks in general with their pay to play scheme. Last place is definitely Vercel’s own implementation of PostgreSQL mostly because of the compute/size limitations.\nI would have implemented multiple sensors, likely in the form of PM2.5/10, CO2, a better DHT22 sensor, and a VOC sensor. Here’s my Amazon wish list for future projects:\nPM2.5/10[PMS5003]: https://www.adafruit.com/product/3686\nCO2[MH-Z19]: https://www.amazon.com/EC-Buying-Monitoring-Concentration-Detection/dp/B0CRKH5XVX/\nTemperature/Humidity[DHT22]: https://www.amazon.com/SHILLEHTEK-Digital-Temperature-Humidity-Sensor/dp/B0CN5PN225/\nTemp/Humidity/Pressure/VOC[BME680]: https://www.amazon.com/CJMCU-680-Temperature-Humidity-Ultra-Small-Development/dp/B07K1CGQTJ/\nAir Quality/VOC[MQ135]: https://www.amazon.com/Ximimark-Quality-Hazardous-Detection-Arduino/dp/B07L73VTTY/\nWhile a description of extract, transform, and load(ETL) probably belongs up top I feel like it fits the narrative better to define it here and describe how this project is one giant homegrown ETL pipeline…"
  },
  {
    "objectID": "posts/Pi-Sensor-Proj-May-2024/index.html#what-is-an-etl-pipeline",
    "href": "posts/Pi-Sensor-Proj-May-2024/index.html#what-is-an-etl-pipeline",
    "title": "Raspberry Pi Sensor Server Project",
    "section": "What is an ETL Pipeline?",
    "text": "What is an ETL Pipeline?\n\nDefinition\nETL stands for Extract, Transform, Load. It’s a process used in data management to:\n\nExtract data from various sources.\nTransform the data into a suitable format or structure for analysis and reporting.\nLoad the transformed data into a target database, data warehouse, or data lake.\n\n\n\nHow it Works\n\nExtract:\n\nData is collected from different sources like databases, APIs, files, or sensors.\nThis step involves connecting to the source, querying or retrieving the data, and pulling it into the pipeline.\n\nTransform:\n\nThe extracted data is cleaned, validated, and transformed to fit the schema of the target system.\nTransformations might include filtering, aggregating, joining, sorting, or converting data types.\nThis step ensures data quality and prepares it for efficient loading and querying in the target system.\n\nLoad:\n\nThe transformed data is loaded into the target database, data warehouse, or data lake.\nThis involves writing the data into the storage system, ensuring it’s available for querying and analysis.\nThe loading process can be a full load (overwriting existing data) or an incremental load (updating or adding new data).\n\n\n\n\nApplications\nETL pipelines are used in various scenarios, including:\n\nData Warehousing: Collecting and consolidating data from multiple sources into a central repository for reporting and analysis.\nBusiness Intelligence (BI): Providing clean and structured data for BI tools to generate insights and reports.\nData Integration: Integrating data from different systems to provide a unified view.\nData Migration: Moving data from one system to another, often during system upgrades or cloud migrations.\nData Analytics: Preparing data for analysis and machine learning models.\n\n\n\nExample of a Homegrown ETL Pipeline in the Project\nThe project described above is a great example of a homegrown ETL pipeline, using a Raspberry Pi and various data storage solutions.\n\nExtract\n\nData Sources: Environmental data is extracted from various sensors (e.g., DHT11 for temperature and humidity).\nData Collection: The sensors are connected to the Raspberry Pi, which reads data at regular intervals.\n\n\n\nTransform\n\nData Processing: The raw data from the sensors is processed to convert it into a readable format (e.g., converting temperature from Celsius to Fahrenheit).\nData Formatting: The data is formatted into JSON objects for a consistent structure across different storage solutions.\n\n\n\nLoad\n\nData Storage: The transformed data is loaded into multiple storage solutions:\n\nGoogle Sheets: For easy access and sharing.\nThingSpeak: For real-time visualization and monitoring.\nMongoDB: For flexible, document-based storage.\nVercel PostgreSQL: For relational database storage.\n\n\n\n\n\nHow the Project Implements ETL\n\nExtract:\n\nThe Raspberry Pi reads data from the DHT11 sensor every second using the Adafruit_DHT library.\n\nTransform:\n\nThe raw data (humidity and temperature) is processed to convert temperature to Fahrenheit and format the date and time.\nThe data is structured into JSON objects for consistency.\n\nLoad:\n\nThe data is sent to multiple endpoints:\n\nGoogle Sheets: Using the Google Sheets API to append new rows.\nThingSpeak: Using the ThingSpeak API for both simple and bulk updates.\nMongoDB: Using the MongoDB Python client to insert data into the collection.\nVercel PostgreSQL: Sending data to a Vercel serverless function which writes to a PostgreSQL database.\n\n\n\n\n\nBenefits of this ETL Pipeline\n\nReal-time Data Processing: The pipeline processes and loads data in near real-time, providing up-to-date information.\nMulti-Storage Integration: The data is stored in various platforms, each with its strengths, ensuring data availability and flexibility.\nScalability: The modular approach allows easy addition of new sensors or data sources.\nAutomation: The use of scheduled tasks and services ensures continuous data collection and processing without manual intervention.\n\n\n\nConclusion\nThis project shows off a homegrown ETL pipeline that efficiently collects, processes, and stores environmental data from sensors. By using various storage solutions, it demonstrates how a flexible and scalable ETL process can be implemented for real-time data monitoring and analysis in a home automation context."
  },
  {
    "objectID": "posts/Soldering_ESP32/index.html",
    "href": "posts/Soldering_ESP32/index.html",
    "title": "ESP32 Project: Sensor Reliability/Power Efficiency",
    "section": "",
    "text": "Introduction\nAfter experimentation with the previous project at: https://jesse-anderson.github.io/Blog/_site/posts/ESP32_Post_1/ , I encountered issues with the DHT11 sensor maintaining reliable contact with the internal breadboard connections. To address this, I acquired someProtoboards from ElectroCookie and began the process of soldering the components together. I chose ElectroCookie as they were highly rated on Amazon and apparently the Adafruit Protoboards are corrosion prone. It also helps that the ElectroCookie ones were far cheaper! This post details the steps I took to resolve these issues and enhance the project’s overall performance.\n\n\nSoldering the Components\nUsing the same configuration as outlined in the initial ESP32 post, I carefully soldered the components onto the Protoboard. The result was a significant improvement in the consistency of the sensor readings, eliminating the previous contact issues. Below are images of the completed soldering work:\n\nTop View:\n\n\n\nBottom View:\n\n\n\n\nElectrical Safety\nAfter completing the soldering, I was concerned about the potential for short circuits if the board was placed on a conductive surface. To mitigate this risk, I checked the conductivity of a glue gun stick I planned to use for insulation. Confident in its suitability, I applied hot glue to the entire bottom of the board. I opted for large, removable globs of glue rather than thin layers to facilitate easy removal if needed. Here is the “final” product after applying the hot glue:\n\n\n\nIncorporating the Verter Buck-Boost Module\nTo extend the operational time of the ESP32 setup, I integrated the Verter buck-boost module from Adafruit along with their 4 AA battery pack. This configuration achieved an approximate runtime of 28.2 hours, which exceeded my expectations. I attribute part of this efficiency to the removable LED socket used for data transmission checks. The green LED was bright enough to disturb sleep in a darkened room, so I removed it during overnight testing. I estimate that it would have run for 20 ish hours with the LED on as that’s what I roughly estimated(the peak sensor reading plus transmission reading on my power bank may have been off….).\n\n\nWhat is a Buck-Boost Module\nA Buck-Boost Module is a type of DC-DC converter that can step up (boost) or step down (buck) an input voltage to a desired output voltage. This functionality is particularly useful in battery-powered projects where the input voltage can vary significantly as the battery discharges. The module ensures that the device receives a stable voltage, enhancing the reliability and efficiency of the system.\nThe Verter buck-boost module from Adafruit is a versatile power supply solution that can accept input voltages ranging from 3V to 12V and output a consistent 5.2V voltage, making it ideal for various applications. It is equipped with multiple safety features, including over-voltage protection and thermal shutdown, to ensure the safe operation of your electronics. I went with Adafruit over many companies on Amazon as a sensor failing is a disappointment whereas everything on my board being fried is infuriating. The output voltage of the Verter on the USB side is 5.2V and it is far better as a buck converter than a boost converter. Note that it uses a TPS630630 boost converter from TI and the USB connector can output 500mA according to the datasheet. The module overall can output ~1.0+A so if push comes to shove I can always adapt it if I need more current. It has 90+% operating efficiency in some cases and the efficiency graphs are below:\nEfficiency vs. Output Current:\n\nEfficiency vs. Input Voltage Current:\n\nWe see that at V_Out = 4.8V and V_Out = 5V we have an efficiency near or above 90% and that’s good enough for me.\nI ensured all voltages were within specification and verified the output using an opened USB cable. Here’s the setup with the Verter and battery pack:\n\nOnce I tested everything it was fairly straightforward to throw some NiMH Ikea batteries at 1.2V 1900mAh and double check the output voltage to make sure I was ok. Moving forward I would definitely opt for Adafruit’s 8 battery pack but 4 batteries works for my purposes for now. Verter + battery pack below, note that there are some screws you gotta tighten down on for your (+)/(-):\n\n\n\nFuture Improvements\nImplementing a battery monitoring system would enhance the project’s robustness, allowing for better power management. Additionally, incorporating a solar charging circuit could provide a sustainable power solution by using solar energy during the day to charge the batteries and running the device on battery power at night.\n\n\nAdding OLEDs\nFunny enough this tidbit actually predates the battery pack and Buck Boost Converter, but I felt that stabilizing the circuit and getting battery power working was of far higher importance than slapping a screen in the circuit.\nThe OLED display used is a simple 128x32 screen, acquired affordably from Amazon. Using the SSD1306 library to interface with the OLED is straightforward. My plan is to create an ESP32 GitHub repository containing all the sensor integrations packaged into a single, comprehensive folder of Python files for ease of use. This approach simplifies the process compared to tracking down and adapting various libraries.\nBelow are example circuits using standard SCL and SDA connections on the ESP32, as well as alternative connections by assigning the 14 and 13 pins to SCL and SDA respectively:\n\n1 OLED:\n\n\n\n2 OLED:\n\n\n\n\nAlternative Pin Configuration\nIt’s essential to note that the ESP32 allows for flexible pin assignment for the SDA (Serial Data Line) and SCL (Serial Clock Line). Here’s a table showcasing the alternative pin assignments:\n\n\n\nSDA\nSCL\n\n\n\n\n4\n12\n\n\n13\n14\n\n\n17\n16\n\n\n18\n17\n\n\n19\n18\n\n\n23\n19\n\n\n25\n23\n\n\n26\n25\n\n\n27\n26\n\n\n32\n27\n\n\n33\n32\n\n\n\nAdditionally, there are input-only pins: 34, 35, 36, 37, 38, and 39.\n\n\nInitializing and Testing the I2C Bus\nIdeally one would connect their peripheral then run the following code to ensure that they are getting a reading from their device(change pin numbers!):\n\n\nCode\n\nfrom machine import Pin, SoftI2C\nimport time\n\n# Initialize I2C with default pins\ni2c = SoftI2C(scl=Pin(22), sda=Pin(21))\n\ndef scan_i2c(i2c):\n    print(\"Scanning I2C bus...\")\n    devices = i2c.scan()\n    if len(devices) == 0:\n        print(\"No I2C devices found\")\n    else:\n        print(\"I2C devices found:\", len(devices))\n        for device in devices:\n            print(\"Decimal address:\", device, \" | Hex address:\", hex(device))\n\nwhile True:\n    scan_i2c(i2c)\n    time.sleep(5)\n\nGoing back to the OLEDs… After we verify that they are connected correctly we can run the following to print:\n\n\nCode\n\nfrom machine import Pin, SoftI2C\nimport ssd1306\nfrom time import sleep\n\n# ESP32 Pin default\ni2c = SoftI2C(scl=Pin(22), sda=Pin(21))\n\n# ESP8266 Pin default\n#i2c = SoftI2C(scl=Pin(5), sda=Pin(4))\n\noled_width = 128\noled_height = 32\noled = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c)\n\n# Clear the display buffer\noled.fill(0)\n\noled.text('Existence is a prison', 0, 0)\noled.text(\"prison.I'm bound.\", 0, 10)\noled.text('to this device', 0, 20)\n\noled.show()\n\n# ESP32 Pin assignment \ni2c_2 = SoftI2C(scl=Pin(14), sda=Pin(13))\n\n# Clear the display buffer\noled2.fill(0)\n\noled_width = 128\noled_height = 32\noled2 = ssd1306.SSD1306_I2C(oled_width, oled_height, i2c_2)\n\noled2.text('Existence is a prison', 0, 0)\noled2.text(\"prison.I'm bound.\", 0, 10)\noled2.text('to this device', 0, 20)\n\noled2.show()\n\n\n\nThe SSD1306 Library\nAn OLED works by emitting light from organic compounds that emit light when an electric current is applied. This technology allows for bright, clear displays that are highly efficient and have excellent contrast ratios. OLEDs are used in a variety of applications, from small displays on microcontrollers to large television screens. I direct you to the wikipedia article at: https://en.wikipedia.org/wiki/OLED if you want to do a deep dive into materials of construction, operating principle, etc.\nThe SSD1306 library is a widely used library for controlling OLED displays based on the SSD1306 driver. This library simplifies the process of communicating with the OLED and provides a set of functions to easily draw text, shapes, and images on the screen. By using the SSD1306 library, developers can quickly integrate OLED displays into their projects without needing to understand the low-level details of the communication protocol.\nBeyond simple text one can draw lines, rectangles, circles, and even bitmaps. This allows for the creation of detailed and informative graphical interfaces provided you have enough pixels.\n\n\nFuture Improvements\nSeveral quality of life improvements can further enhance the ESP32 environmental monitor. Implementing a battery monitoring system would provide better power management. Additionally, incorporating a solar charging circuit could offer a sustainable power solution by using solar energy during the day to charge the batteries and running the device on battery power at night. The design of the air intake box, sensor sampling rates, and minimizing power draw would constitute some one off things I’ll need to do, but for right now they aren’t too relevant. In any case, being able to quickly visualize what’s going on with the sensors is cool enough, but having the web interface is far more useful.\n\n\nConclusion\nThrough some careful(haphazard?) soldering I got a reliable temperature/humidity sensor working with a web interface and a fairly large battery capacity. One note for portability: If I ever wanted to remotely log data to Google/etc I would need to set up a hotspot and assign that as an alternative WiFi that tries to connect if the first one isn’t present.\nThis post is definitely a bit more stop and go than previous posts owing to the fact that I needed to document this mini project, but also have a fair bit of backlog when it comes to writing. At a later date I may revisit this and clean up the writing. The key focus is being able to reference this in the future for my own personal use.\nStay tuned for more updates."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my inaugural Quarto post! Though this marks my official entry into the Quarto blogging sphere, it isn’t my first dive into writing here. I initially crafted an insightful(?) post about the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a fascinating clustering algorithm that piqued my interest due to its rigorous yet intuitive approach to handling complex data sets.\nMy name is Jesse Anderson, and my academic and professional journey began in the world of chemical engineering, where I specialized in biochemical engineering and process automation. The intricate dance of chemical processes, moving systematically from one unit to another, serendipitously mirrored the logic and flow of computer science. It was this realization that bridged my transition from engineering physical processes to engineering digital ones.\nThis transition felt incredibly natural. In both domains, you’re essentially inputting a series of commands—whether to a machine in a lab or a line of code in a software program—and watching as complex processes unfold, often with a satisfying precision. This similarity is what drew me deeper into the world of computing, leading me to pursue a Master’s in Computer Science at the University of Illinois at Urbana-Champaign (UIUC). My undergraduate studies were completed at the University of Illinois at Chicago (UIC), where I earned a Bachelor of Science in Chemical Engineering.\nDuring my time at UIC, my fascination with optimization and automation took root. This passion wasn’t confined to coursework and theory. I actively engaged in developing research software, working closely with Dr. Ying Hu. My contributions during this period were not only fulfilling but also fruitful, culminating in several scholarly publications.\nOutside of academia, my hobbies closely mirror my professional interests, intertwining a love for optimization with a hands-on approach to problem-solving that spans far beyond algorithms and lab reactors. This passion for efficiency and process improvement extends into crafting tools and scripts that enhance productivity and automate the more mundane tasks of everyday life. But my knack for tool-making isn’t just confined to the digital realm; it’s a skill I’ve honed over many years, influenced heavily by my extensive background as an independent contractor.\nBefore venturing into the world of chemical engineering and computer science, I spent several years in the trades, tackling every aspect of plumbing, electrical work, HVAC, carpentry, and remodeling. This experience taught me the value of precision and strategic planning, skills that are just as applicable to programming as they are to physical construction and maintenance. Each project, whether installing a complex plumbing system or wiring a newly constructed home, sharpened my ability to think critically and adapt quickly—traits that have proven indispensable throughout my career.\nIn addition to my hands-on trade work, I managed the safety and project planning operations at G5 Environmental, a prominent street sweeping company based in Chicago. My role involved ensuring the smooth execution of projects, maintaining rigorous safety standards, and optimizing operations to enhance efficiency. This experience not only broadened my understanding of large-scale project management but also deepened my appreciation for the intricate dance between man, machine, and the environment. At G5 Environmental, we were committed to maintaining the cleanliness and safety of public spaces, a mission that echoes my current work’s focus on sustainability and environmental preservation through technology.\nMore recently, during my tenure at UL Solutions, I further developed my technical skills and applied them to real-world challenges:\n\nAutomation Software Development: I developed automation software in Python and VBA that successfully eliminated over 664+ hours on average of tedious engineering workflows annually.\nCAD/PDF Change Detection: I developed a few python scripts to automatically compare CAD files and compare long PDFs(like Acrobat Pro, but free and simpler).\nReal-Time Data Analysis for Testing: I implemented systems for real-time data analysis to support large-scale testing projects. This advancement has enabled more data-driven decision-making, optimizing testing procedures and outcomes through immediate feedback and analysis.\n\nWhile I massively enjoyed the automation projects I took on at UL, I was seeking broader challenges and opportunities for growth that UL could no longer provide. This realization led me to pursue a more fitting opportunity elsewhere, where I could further expand my expertise and impact in the fields of process automation and data-driven technology.\nThis diverse background has equipped me with a unique perspective that I bring to my current studies and research. Whether refining code to streamline data processing or rethinking a workflow to enhance laboratory operations, I am constantly learning and applying, pushing the boundaries of what I know about both chemical engineering and computer science. These experiences have molded me into a versatile professional capable of adapting to various roles and continuously refining my approach to both new and familiar challenges.\nAs I continue my studies at UIUC, my goal is to further explore how advanced computational techniques can be applied to solve complex problems in biochemical engineering and beyond. The integration of process automation into digital applications opens up exciting possibilities for innovation in both fields. Through this blog, I aim to share insights from my journey, discussing both the theoretical and practical aspects of my work and studies in hopes of connecting with fellow enthusiasts and professionals who share my zeal for optimization and automation.\nStay tuned for more posts where I’ll delve deeper into specific projects, challenges I’ve faced, and solutions I’ve discovered along the way. Thank you for joining me on this adventure in merging the systematic worlds of engineering and computing into one coherent, intertwined narrative."
  }
]