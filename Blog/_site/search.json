[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Jesse Anderson's Blog",
    "section": "",
    "text": "General readme.\n\n\nTo push a new blog or rather statically generate one.\n\n\ncd this folder in cmd\n\n\nquarto render"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A blog to dump random ideas",
    "section": "",
    "text": "DBSCAN Intro\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nDBSCAN\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/DBSCAN/index.html",
    "href": "posts/DBSCAN/index.html",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm notable for its proficiency in identifying clusters of varying shapes and sizes in large spatial datasets. This algorithm is especially useful in the field of spatiotemporal data analysis, where the goal is often to group similar data points that are in close proximity over time and space. In this blog post, we’ll delve into the mechanics of DBSCAN, discuss its critical parameters, and provide guidance on adjusting these parameters to achieve optimal clustering results for spatiotemporal data.\n\n\nDBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform.\n\n\n\nThe effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n Core, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here\n\n\n\n\nSpatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here\n\n\n\n\nScale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data.\n\n\n\n\nDBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/DBSCAN/index.html#what-is-dbscan",
    "href": "posts/DBSCAN/index.html#what-is-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform."
  },
  {
    "objectID": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "href": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "The effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n Core, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "href": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Spatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "href": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Scale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data."
  },
  {
    "objectID": "posts/DBSCAN/index.html#conclusion",
    "href": "posts/DBSCAN/index.html#conclusion",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my inaugural Quarto post! Though this marks my official entry into the Quarto blogging sphere, it isn’t my first dive into writing here. I initially crafted an insightful(?) post about the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a fascinating clustering algorithm that piqued my interest due to its rigorous yet intuitive approach to handling complex data sets.\nMy name is Jesse Anderson, and my academic and professional journey began in the world of chemical engineering, where I specialized in biochemical engineering and process automation. The intricate dance of chemical processes, moving systematically from one unit to another, serendipitously mirrored the logic and flow of computer science. It was this realization that bridged my transition from engineering physical processes to engineering digital ones.\nThis transition felt incredibly natural. In both domains, you’re essentially inputting a series of commands—whether to a machine in a lab or a line of code in a software program—and watching as complex processes unfold, often with a satisfying precision. This similarity is what drew me deeper into the world of computing, leading me to pursue a Master’s in Computer Science at the University of Illinois at Urbana-Champaign (UIUC). My undergraduate studies were completed at the University of Illinois at Chicago (UIC), where I earned a Bachelor of Science in Chemical Engineering.\nDuring my time at UIC, my fascination with optimization and automation took root. This passion wasn’t confined to coursework and theory. I actively engaged in developing research software, working closely with Dr. Ying Hu. My contributions during this period were not only fulfilling but also fruitful, culminating in several scholarly publications.\nOutside of academia, my hobbies closely mirror my professional interests, intertwining a love for optimization with a hands-on approach to problem-solving that spans far beyond algorithms and lab reactors. This passion for efficiency and process improvement extends into crafting tools and scripts that enhance productivity and automate the more mundane tasks of everyday life. But my knack for tool-making isn’t just confined to the digital realm; it’s a skill I’ve honed over many years, influenced heavily by my extensive background as an independent contractor.\nBefore venturing into the world of chemical engineering and computer science, I spent several years in the trades, tackling every aspect of plumbing, electrical work, HVAC, carpentry, and remodeling. This experience taught me the value of precision and strategic planning, skills that are just as applicable to programming as they are to physical construction and maintenance. Each project, whether installing a complex plumbing system or wiring a newly constructed home, sharpened my ability to think critically and adapt quickly—traits that have proven indispensable throughout my career.\nIn addition to my hands-on trade work, I managed the safety and project planning operations at G5 Environmental, a prominent street sweeping company based in Chicago. My role involved ensuring the smooth execution of projects, maintaining rigorous safety standards, and optimizing operations to enhance efficiency. This experience not only broadened my understanding of large-scale project management but also deepened my appreciation for the intricate dance between man, machine, and the environment. At G5 Environmental, we were committed to maintaining the cleanliness and safety of public spaces, a mission that echoes my current work’s focus on sustainability and environmental preservation through technology.\nMore recently, during my tenure at UL Solutions, I further developed my technical skills and applied them to real-world challenges:\n\nAutomation Software Development: I developed automation software in Python and VBA that successfully eliminated over 664+ hours on average of tedious engineering workflows annually.\nCAD/PDF Change Detection: I developed a few python scripts to automatically compare CAD files and compare long PDFs(like Acrobat Pro, but free and simpler).\nReal-Time Data Analysis for Testing: I implemented systems for real-time data analysis to support large-scale testing projects. This advancement has enabled more data-driven decision-making, optimizing testing procedures and outcomes through immediate feedback and analysis.\n\nWhile I massively enjoyed the automation projects I took on at UL, I was seeking broader challenges and opportunities for growth that UL could no longer provide. This realization led me to pursue a more fitting opportunity elsewhere, where I could further expand my expertise and impact in the fields of process automation and data-driven technology.\nThis diverse background has equipped me with a unique perspective that I bring to my current studies and research. Whether refining code to streamline data processing or rethinking a workflow to enhance laboratory operations, I am constantly learning and applying, pushing the boundaries of what I know about both chemical engineering and computer science. These experiences have molded me into a versatile professional capable of adapting to various roles and continuously refining my approach to both new and familiar challenges.\nAs I continue my studies at UIUC, my goal is to further explore how advanced computational techniques can be applied to solve complex problems in biochemical engineering and beyond. The integration of process automation into digital applications opens up exciting possibilities for innovation in both fields. Through this blog, I aim to share insights from my journey, discussing both the theoretical and practical aspects of my work and studies in hopes of connecting with fellow enthusiasts and professionals who share my zeal for optimization and automation.\nStay tuned for more posts where I’ll delve deeper into specific projects, challenges I’ve faced, and solutions I’ve discovered along the way. Thank you for joining me on this adventure in merging the systematic worlds of engineering and computing into one coherent, intertwined narrative."
  }
]