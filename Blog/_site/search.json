[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Jesse Anderson's Blog",
    "section": "",
    "text": "General readme.\n\n\nTo push a new blog or rather statically generate one.\n\n\ncd this folder in cmd\n\n\nquarto render"
  },
  {
    "objectID": "posts/VAE_GAN/VAEGAN.html",
    "href": "posts/VAE_GAN/VAEGAN.html",
    "title": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs",
    "section": "",
    "text": "Generative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\nDuring my time in the applied machine learning course(CS441) at the University of Illinois at Urbana-Champaign, I embarked on an ambitious journey into the realm of advanced machine learning technologies. The course required a comprehensive understanding of various machine learning concepts, providing both breadth and depth in our studies. For my final optional project, I chose to specialize in some of the most intriguing areas of generative models: Variational AutoEncoders (VAEs), Denoising AutoEncoders, Generative Adversarial Networks (GANs), and the innovative hybrid, Variational Autoencoder Generative Adversarial Networks (VAE-GANs). Here, I provide a high-level overview of each technology and discuss the outcomes of my experiments. For further information about the course please see sample syllabus.\n\nVariational AutoEncoders (VAEs)\nVAEs are powerful generative models that use the principles of probability and statistics to produce new data points that are similar to the training data. Unlike traditional autoencoders, which aim to compress and decompress data, VAEs introduce a probabilistic twist to encode input data into a distribution over latent space. This approach not only helps in generating new data but also improves the model’s robustness and the quality of generated samples. VAEs are particularly effective in tasks where you need a deep understanding of the data’s latent structure, such as in image generation and anomaly detection.\nThe encoder in a VAE is responsible for transforming high-dimensional input data into a lower-dimensional and more manageable representation. However, unlike standard autoencoders that directly encode data into a fixed point in latent space, the encoder in a VAE maps inputs into a distribution over the latent space. This distribution is typically parameterized by means (mu) and variances (sigma), which define a Gaussian probability distribution for each dimension in the latent space.\nThe latent space in VAEs is the core feature that distinguishes them from other types of autoencoders. It is a probabilistic space where each point is defined not just by coordinates, but by a distribution over possible values. This stochastic nature of the latent space allows VAEs to generate new data points by sampling from these distributions, providing a mechanism to capture and represent the underlying probabilistic properties of the data. It essentially acts as a compressed knowledge base of the data’s attributes.\nOnce a point in the latent space is sampled, the decoder part of the VAE takes over to map this probabilistic representation back to the original data space. The decoder learns to reconstruct the input data from its latent representation, aiming to minimize the difference between the original input and its reconstruction. This process is governed by a loss function that has two components: a reconstruction loss that measures how effectively the decoder reconstructs the input data from the latent space, and a regularization term that ensures the distribution characteristics in the latent space do not deviate significantly from a predefined prior (often a standard normal distribution).\nIn practice, the encoder’s output of means and variances provides a smooth and continuous latent space, which is crucial for generating new data points that are similar but not identical to the original data. This property makes VAEs particularly useful in tasks requiring a deep generative model, such as synthesizing new images that share characteristics with a training set, or identifying anomalies by seeing how well data points reconstruct using the learned distributions.\n\n\n\n\n    Attribution Example\n\n\n    By EugenioTL - Own work, CC BY-SA 4.0, Link\n\n\n\n\nDenoising AutoEncoders\nDenoising Autoencoders (DAEs) are specialized neural networks aimed at improving the quality of corrupted input data by learning to restore its original, uncorrupted state. This functionality is crucial in applications such as image restoration, where DAEs enhance image clarity by effectively removing noise. They achieve this through a training process that involves a dataset containing pairs of noisy and clean images. By continually adjusting through this training set, the DAE learns the underlying patterns necessary to filter out the distortions and recover the clean data. This ability to directly process and improve corrupted data makes DAEs valuable for various tasks beyond image restoration, including audio cleaning and improving data quality for analytical purposes.\n\nSource: https://blog.keras.io/building-autoencoders-in-keras.html\n\n\nGenerative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) utilize a unique framework involving two competing neural networks: a generator and a discriminator. These networks engage in an adversarial game, where the generator’s goal is to create synthetic data that is indistinguishable from real-world data, effectively “fooling” the discriminator. The discriminator’s job, on the other hand, is to distinguish between the authentic data and the synthetic creations of the generator.\nThis dynamic creates a feedback loop where the generator continually learns from the discriminator’s ability to detect fakes, driving it to improve its data generation. As the generator gets better, the discriminator’s task becomes more challenging, forcing it to improve its detection capabilities. Over time, this adversarial process leads to the generation of highly realistic and convincing data outputs.\nGANs have been particularly successful in the field of image generation, where they are used to create highly realistic images that are often indistinguishable from actual photographs. A prominent example is the ThisPersonDoesNotExistwebsite, which uses a model called StyleGAN2to generate lifelike images of human faces that do not correspond to real individuals. This technology has also been applied in other areas such as art creation, style transfer, and more. Eerie.\n\nSource: https://developers.google.com/machine-learning/gan/gan_structure\n\n\nVariational Autoencoder Generative Adversarial Networks (VAE-GANs)\nVAE-GANs are an innovative hybrid model that synergistically combines Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to enhance the quality and control of data generation. The model integrates the VAE’s capability for creating a compressed, latent representation of data with the GAN’s strength in generating high-fidelity outputs.\nIn a VAE-GAN system, the encoder part of the VAE compresses input data into a latent space (a condensed representation), which the GAN’s generator then uses to reconstruct outputs that are as realistic as possible. This setup leverages the VAE’s ability to manage and interpret the latent space effectively, providing a structured, meaningful input for the GAN’s generator. The discriminator in the GAN setup then evaluates these outputs against real data, guiding the generator to improve its outputs continually.\nThe fusion of these two models allows for a more controlled generation process, which can lead to higher quality outputs than what might be achieved by either model alone. This approach not only enhances the detail and realism of the generated data but also improves the model’s ability to learn diverse and complex data distributions, making VAE-GANs particularly useful in tasks that require a high level of detail and accuracy, such as in image generation and modification.\n\nSource: Larsen, Anders & Sønderby, Søren & Winther, Ole. (2015). Autoencoding beyond pixels using a learned similarity metric.\n\n\nProject Outcomes\nThe practical application of these models in my project yielded fascinating insights and results. For instance, the VAEs demonstrated an impressive ability to generate new images that closely resembled the original dataset, while the Denoising AutoEncoders more or less restored a significant portion of corrupted images to their original state. Similarly, the GANs produced images that were often indistinguishable from real ones, highlighting their potential in creating synthetic data for training other machine learning models without the need for extensive real-world data collection.\nThe VAE-GANs, however, were the highlight, combining the best aspects of their constituent models to generate supremely realistic and diverse outputs. While I am unable to share specific code snippets of the DAE/VAE due to copyright restrictions on the course content, the qualitative outcomes were highly encouraging and indicative of the powerful capabilities of hybrid generative models.\n\n\nResults\n\n\nDenoising AutoEncoder\n\nAs you can see in the image above, it does an ok job of denoising the middle image. The top image is the original image, the middle is the image with 50% noise, and the bottom is the model’s outputted denoised image. If I trained the model longer and varied up the training data I likely would have been able to get a better result. Additionally Principal Component Analysis and calculation of the Mean Residual Error was performed to determine how well the model works. See below:\n\nLoss over Epochs(Note the plots say reconstruction error when I really meant Mean Squared Error Loss):\n\nI decided to use a standard fixed learning rate here and only trained for 240 epochs.\n\n\nVariational AutoEncoder\nHere we were tasked with coming up with a VAE which would generate images and also be able to interpolate between images.\nSame Digits:\n\nDifferent Digits:\n\nOriginal vs. Reconstructed:\n\nAs you can see the VAE did a fairly good job of generating images.\nLoss over 400 Epochs:\n\nI was playing around with progressively reducing the learning rate as parameters changed(or didn’t) and thus reduced the learning rate progressively. This actually seemed to result in the exponentialLR type scheduler funnily enough. See here. The model was trained for 400 epochs. I likely won’t spin the DAE/VAE back up for videos as I did for the GAN and VAE-GAN.\n\n\nGenerative Adversarial Network\nI decided to go a bit further and try to get a Generative Adversarial Network running to generate new images of numbers. I went beyond the standard requirements of the course, but not too far as I didn’t want to “waste” too much time as there are newer technologies nowadays. Here’s the repo. There is a video below that shows the evolution of the training of the model. Additionally here’s the final result and video below:\n\n\n\n\n\n\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\nLoad and Play Video\n\n\n\n\n\nLosses over 540 Epochs:\n\n\n\nVAE-GAN\nI finally went a bit further(probably too far) and decided to implement a VAE-GAN. There was a lot more balancing involved between the autoencoder portion and the generative portion and I was able to achieve a passable result, but definitely not worth the time and effort to balance parameters. It was strangely smoothed out, yet blurred where it mattered to generate the images.\nFinal result image and video below:\n\n\n\n\n\n\n\n\n\nClick to Play Video\n\n\n\n  \n  Your browser does not support MP4 videos.\n\n\n\nLoad and Play Video\n\n\n\n\n\nHere’s the associated Losses over 1000 Epochs(note my discriminator freaking out…):\n\n\n\nFinal Thoughts\nExploring these advanced generative models not only enhanced my understanding of the deep theoretical underpinnings of machine learning but also provided a practical toolkit for addressing complex real-world data generation and enhancement challenges. The knowledge and experience gained through this project are invaluable and have opened up numerous possibilities for further research and application in the field of artificial intelligence. I anticipate broadening my skillset in Generative AI here soon and will continue to skill up. I also got to experience the sheer tedium of “untangling” a neural network wherein something went wrong in my layers… repeatedly."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html",
    "href": "posts/OPTICSWriteup/index.html",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Within the realm of unsupervised machine learning, clustering methods play a critical role in unraveling patterns and groupings within datasets where the labels are unknown. Among these clustering techniques, OPTICS(Ordering Points To Identify the Clustering Structure) stands out as a particularly powerful tool when dealing with complex datasets with varying densities or non-globular shapes. This exploration of OPTICS should get you up to speed on the basics of what OPTICS does and will help you understand its mechanics, benefits, and practical applications.\n\n\nDeveloped in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures.\n\n\n\nThe core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it.\n\n\n\n\n\n\nOPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management.\n\n\n\n\n\nTo determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets.\n\n\n\n\n\nXi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually.\n\n\n\n\nAs datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#introduction-to-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Developed in  1999 by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander(it’s worth a read) , extends the concepts of traditional density-based clustering algorithms. Unlike its predecessor DBSCAN, which partitions the data into clusters based on a set radius and minimum number of points, OPTICS explores data relationships at various scales of density. This flexibility allows it to identify clusters with varying densities and/or complex structures."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "href": "posts/OPTICSWriteup/index.html#core-concepts-of-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "The core of OPTICS is built around two concepts which are Reachability Distance and Ordering of Points.\n\nReachability Distance: Central to the functionality of OPTICS is the concept of reachability distance. This metric doesn’t merely measure the physical proximity between points(as DBSCAN might) but also considers the density of the surrounding area. It blends the spatial distance with local density to provide a high level perspective on how the points are related, highlighting the edges of clusters and distinguishing outliers. This reachability distance is absolutely critical in visualizing the data density which traditional distance measures might gloss over. In the image below from Ankerst et al(1999) we see three distinct clusters wherein the highest density region is indicated. \nOrdering of Points: OPTICS excels in its ability to produce an ordered list of the dataset’s points based on their density connectivity. This ordering goes beyond a simple sequence; it tells a story about how each point relates to its neighbors, forming a continuous, density-based narrative of the dataset. Such an ordered arrangement is not only pivotal for identifying clusters but also enhances the interpretability of the data, allowing clearer visualizations of the clustering structure. In the above structure, if we drew a horizontal line such that we would have 3 “valleys” we can establish a ceiling upon which points below the ceiling belong to a cluster and points above are noise. This is coded in Python and described in detail in my Optics In Python post: here. Please see the image below for an example of synthetic data of 4 clusters clustered using OPTICS. Note the colored data below the threshold and the noise above it."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "href": "posts/OPTICSWriteup/index.html#benefits-of-using-optics",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "OPTICS offers several advantages over more rigid clustering algorithms, making it particularly useful in diverse analytical scenarios:\nHandling Varying Density: One of the standout features of OPTICS is its ability to manage clusters of varying densities. In many real-world datasets, clusters aren’t uniform; some are tightly packed, while others are spread out. OPTICS adapts to these variations seamlessly, capturing a wide range of cluster configurations without the need for prior knowledge about their properties. Recall in the above example where the reachability distance value was set to identify four distinct clusters. It is also possible to first segment the clusters into three distinct clusters, segment that data into three parts, then perform cluster analysis again on the larger chunk in the upper right to further segment. This is one way of handling density issues with OPTICS.\nFlexibility in Cluster Shape: Unlike algorithms that assume clusters to be of a roughly spherical shape, OPTICS is effective at identifying clusters of arbitrary shapes. This capability is crucial when dealing with complex spatial data patterns that are common in many scientific and industrial applications such as biology where cells can take on a variety of shapes.\nReduced Sensitivity to Parameters: While OPTICS does require parameters similar to DBSCAN, such as the minimum number of points to form a cluster, it is less sensitive to the settings compared to DBSCAN’s reliance on the epsilon parameter. This reduced sensitivity decreases the need for repeated trial-and-error parameter tuning, which can be both time-consuming and technically challenging. In the example above, the only parameter set for the initial OPTICS model was min_pts. From there the optimal cutoff was determined mathematically using an elbow cutoff point as in the image below and then clustered using that parameter. Upon further analysis, it was determined by looking at the reachability plot that a value of 0.88 would more accurately capture the four clusters.\n\n\n\nOPTICS has been applied successfully across a broad spectrum of fields:\n\nEcology: For studying spatial patterns of species distributions and identifying environmental niches.\nMarket Research: In identifying customer segments based on purchasing behavior or preferences, which may vary greatly in their characteristics.\nAstronomy: Useful in star cluster analysis, where the density and shape of clusters can provide insights into cosmic structures and processes.\nTraffic Management: For analyzing congestion patterns across different times and areas, aiding in more dynamic traffic planning and management."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#evaluating-the-performance-of-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "To determine the effectiveness of the OPTICS clustering algorithm, several metrics can be employed that assess different aspects of clustering quality. Each metric provides valuable insights into how well the algorithm has identified meaningful clusters, offering a comprehensive view of its performance. Below, we discuss the most commonly used evaluation metrics for clustering algorithms such as OPTICS:\n1. Silhouette Score: This metric measures the degree of separation between clusters. The silhouette score evaluates how close each point in one cluster is to points in the neighboring clusters. This score ranges from -1 to 1, where a high value indicates that the clusters are well-separated and clearly defined. A high silhouette score is often indicative of a model that has effectively captured the natural groupings in the data.\n2. Davies-Bouldin Index: The Davies-Bouldin index is a measure of the clustering validation technique which provides a score for each cluster. This index is essentially the ratio of the sum of within-cluster scatter to between-cluster separation. The lower the Davies-Bouldin index, the better, as it indicates that the clusters are more compact and well-separated from each other. This index is particularly useful in scenarios where one needs to compare the performance of different clustering algorithms or configurations.\n3. Calinski-Harabasz Index (Variance Ratio Criterion): This index evaluates clusters by computing the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared). The higher the Calinski-Harabasz index, the better the clustering performance, suggesting that the clusters are dense and well-separated, which is an ideal scenario in many practical applications.\n4. Adjusted Rand Index (ARI): The Adjusted Rand Index is a measure of the similarity between two data clusterings - from the same dataset - and is adjusted for the chance grouping of elements. This index takes values between -1 and 1, where higher values indicate that the clusters in the dataset largely agree, except for some pairs of objects that are put in the same or different clusters across the compared clusterings. The ARI is particularly useful for validating the consistency of clustering results across different runs of the algorithm or parameter settings.\n5. Mutual Information: Mutual information measures the amount of information shared between two clusterings, providing insights into their similarity. It assesses how well one clustering predicts the other, with higher values indicating more shared information. Ideally, a high mutual information score, adjusted for chance (such as Adjusted Mutual Information), suggests that significant and relevant information about the dataset’s structure is captured by the clustering. This makes mutual information a robust assessment of clustering quality, especially in complex datasets with underlying patterns not readily discernible by other metrics\nThese evaluation metrics collectively offer a holistic view of OPTICS’ performance, highlighting its strengths and weaknesses in various data scenarios. By carefully analyzing these scores, one can gauge the clustering’s effectiveness and make informed decisions about the use of OPTICS in specific contexts or datasets."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "href": "posts/OPTICSWriteup/index.html#understanding-the-role-of-xi-in-optics-clustering",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "Xi (ξ): A crucial parameter in the OPTICS algorithm is Xi (ξ), which significantly influences how clusters are determined and extracted from the reachability plot. Xi represents the maximum steepness or relative decrease in reachability within a sequence of points that must be present for it to be considered the start or end of a cluster. Essentially, Xi is a threshold that defines what constitutes a sufficient density drop to separate clusters.\nXi plays a pivotal role by acting as a sensitivity parameter that helps in defining cluster boundaries based on reachability distances. It’s particularly useful in distinguishing between points that belong to separate clusters versus those that are merely outliers or noise within a single cluster. A lower Xi value typically results in more clusters with finer granularity, as it identifies smaller changes in density as significant. Conversely, a higher Xi value tends to produce fewer, larger clusters, indicating that only larger changes in reachability are considered meaningful for cluster separation.\n\n\n\nSelecting an appropriate Xi value is critical for achieving effective clustering results with OPTICS. There isn’t a one-size-fits-all value for Xi, as the optimal setting depends on the specific characteristics and distribution of the data. Experimentation and domain knowledge are often necessary to determine the most suitable Xi value for a given dataset. Visualization tools such as reachability plots can be extremely helpful in this process, providing a visual representation of how different Xi values affect clustering outcomes.\n\n\n\nThe choice of Xi can have profound effects on the resulting cluster structure:\n\nHigh Xi Values: May overlook subtle but potentially meaningful cluster boundaries, merging distinct groups into broader clusters.\nLow Xi Values: Can over-segment the data, identifying many small clusters that might better be understood as variations within larger clusters.\n\nIt’s also important to consider that the performance of clustering with different Xi values should be evaluated using metrics like the Silhouette Score, Davies-Bouldin Index, and others discussed earlier. These evaluations can confirm whether the selected Xi value effectively captures the underlying patterns in the data without introducing too much noise or losing significant structures. One such way of evaluating Xi is to evaluate the metrics discussed above with values of Xi between 0.01 and 1 in linspace() of roughly 1000. This way one can determine what the optimal value of Xi is based on each of those five metrics and also average the metrics with different weights to arrive at an ideal clustering. This is performed in the code described in this post. Please see the image below for the Xi values for each metric for the dataset shown above and the resulting “optimal” Xi clustering:\n\n\nAs one can see optimizing Xi sometimes has strange end results and sometimes it is best to stick with arbitrary cutoff points of reachability distance visually."
  },
  {
    "objectID": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "href": "posts/OPTICSWriteup/index.html#extending-optics-to-3-dimensions-and-beyond",
    "title": "Intro to OPTICS",
    "section": "",
    "text": "As datasets grow in complexity and dimensionality, traditional clustering algorithms often struggle to maintain effectiveness and efficiency. OPTICS, with its robust framework, scales well into higher dimensions, including three-dimensional (3D) data and beyond. This adaptability makes it particularly useful in fields such as geospatial analysis, 3D medical imaging, and multidimensional market segmentation.\n\n\nAs the dimensionality of the data increases, several challenges arise:\n\nCurse of Dimensionality: In higher dimensions, the volume of the space increases exponentially, making the data sparse. This sparsity dilutes the concept of proximity because all points tend to be equally far from each other, complicating the task of identifying dense regions.\nComputational Complexity: Calculating distances and maintaining neighborhood information becomes computationally more demanding as the number of dimensions grows.\nInterpretability: Visualizing and interpreting results in high-dimensional spaces is inherently difficult, which can make it challenging to derive actionable insights from the clustering output.\n\n\n\n\nDespite these challenges, OPTICS’s methodology is well-suited for high-dimensional spaces due to its focus on relative density rather than absolute distance measures. Here’s how OPTICS can be effectively applied to higher-dimensional data:\n\nDimensionality Reduction Techniques: Before applying OPTICS, it can be beneficial to employ techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the data. This preprocessing step helps mitigate the curse of dimensionality and can highlight inherent structures in the data more effectively.\nParameter Adjustment: In higher dimensions, the parameters of OPTICS, especially min_samples and Xi, might need adjustment. A higher min_samples value might be necessary to adequately capture the density structure in a more sparse environment.\nEnhanced Distance Metrics: Employing different metrics for calculating distances (e.g., Manhattan distance for higher-dimensional geometrical data or cosine similarity for text data) can help in better capturing the true dissimilarities between data points in higher-dimensional spaces.\n\n\n\n\nOPTICS’ flexibility in dealing with different types of data and its robustness in various dimensions have facilitated its application across a wide range of domains:\n\nGeospatial Analysis: Clustering geographical data points not only based on latitude and longitude but also considering elevation, leading to more nuanced cluster formations that can inform environmental studies and urban planning.\nMedical Imaging: In medical imaging, especially in studies involving 3D scans, OPTICS can be used to identify regions of interest, such as tumors in 3D MRI scans, based on their density and spatial properties.\nFinancial Markets: Analyzing multi-dimensional data in finance, such as movements in various market indicators, where OPTICS can help identify anomalous behaviors or cluster markets by trading patterns.\n\nIn conclusion, OPTICS stands as a formidable and versatile tool within the field of unsupervised machine learning, adept at uncovering intricate cluster patterns in complex datasets where traditional methods might falter. Its unique approach to handling varying densities and shapes of data clusters makes it invaluable for a wide array of practical applications, from ecological studies to advanced medical imaging and beyond. By integrating sophisticated metrics to evaluate its performance and extending its capabilities into higher dimensions, OPTICS not only meets the demands of modern data challenges but also opens new avenues for innovative data exploration and analysis. Whether dealing with 2D or multi-dimensional datasets, OPTICS offers a powerful, flexible solution for extracting meaningful insights from raw data, demonstrating its enduring relevance and utility in an increasingly data-driven world."
  },
  {
    "objectID": "posts/DBSCAN/index.html",
    "href": "posts/DBSCAN/index.html",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Density-Based Spatial Clustering of Applications with Noise (DBSCAN) is a popular clustering algorithm notable for its proficiency in identifying clusters of varying shapes and sizes in large spatial datasets. This algorithm is especially useful in the field of spatiotemporal data analysis, where the goal is often to group similar data points that are in close proximity over time and space. In this blog post, we’ll delve into the mechanics of DBSCAN, discuss its critical parameters, and provide guidance on adjusting these parameters to achieve optimal clustering results for spatiotemporal data.\n\n\nDBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform.\n\n\n\nThe effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n\nCore, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here\n\n\n\n\nSpatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here\n\n\n\n\nScale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data.\n\n\n\n\nDBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "posts/DBSCAN/index.html#what-is-dbscan",
    "href": "posts/DBSCAN/index.html#what-is-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN is a non-parametric clustering algorithm that focuses on the concept of density. Unlike centroid-based algorithms like K-means, DBSCAN categorizes points into clusters based on their density connectivity, thereby capably handling anomalies or noise. This makes it an excellent choice for data with irregular patterns and an undefined number of clusters. Note that DBSCAN is a great multi-purpose clustering algorithm, but it does not handle clusters with differing densities quite as well as OPTICS (to be blogged about later…). While DBSCAN requires a single set of parameters (ε and MinPts) across the entire dataset, which can be limiting in complex datasets with varying densities, OPTICS (Ordering Points To Identify the Clustering Structure) addresses this by creating a reachability plot that adapts to density variations, allowing for the identification of clusters based on varying density thresholds. This feature makes OPTICS particularly useful for more heterogeneous datasets where cluster density isn’t uniform."
  },
  {
    "objectID": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "href": "posts/DBSCAN/index.html#key-parameters-of-dbscan",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "The effectiveness of DBSCAN hinges on two primary parameters:\n\nEpsilon (ε): This parameter defines the radius of a neighborhood around a given point. Points within this radius are considered directly reachable from the initial point and potentially part of the same cluster. You can think of epsilon as a circle around which a point is “searched” to find the parameter MinPts. In optimizing the value of this parameter and avoiding repeat trials try to product a k-distance elbow plot and finding the “elbow” of the plot(discussed below).\nMinimum Points (MinPts): This parameter specifies the minimum number of points required to form a dense region. A point is classified as a core point if there are at least MinPts within its ε-neighborhood, including itself. A “noise” point or non-clustered point may exist if the point does not have enough points in its ε-neighborhood to meet the MinPts criterion. Specifically, if the number of points within ε distance of a point is less than MinPts, that point is labeled as “noise” or an outlier. This classification as noise means the point does not belong to any cluster. It is important to note that noise points are not included in any cluster, but they are crucial for identifying and isolating outliers in the dataset.\n\nCore, Border, and Noise Points with Epsilon = 3.0, minPts = 4, Samples = 300, and 4 distinct centers. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "href": "posts/DBSCAN/index.html#adjusting-dbscan-parameters-for-spatiotemporal-data",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Spatiotemporal data, such as geographic locations recorded over time, often exhibits complexities like varying density and noise. The correct setting of DBSCAN’s ε and MinPts parameters is crucial for effective clustering:\nChoosing the Right Epsilon (ε)\n\nSpatial Component: The choice of ε can depend on the scale and resolution of your spatial data. For geographic data, ε might be set based on meaningful physical distances (e.g., meters or kilometers).\nTemporal Component: If time is a factor, ε should also consider temporal proximity. This could mean setting ε to encapsulate points that occur within a certain time frame (e.g., seconds, minutes). One practical method to determine an appropriate ε value is to plot a k-distance graph, where you plot the distance to the k-th nearest neighbor for each point, sorted by distance. The point of maximum curvature in this plot often gives a good indication of a suitable ε. I particularly enjoy MATLAB’s documentation here.\n\nSetting Minimum Points (MinPts)\n\nData Dimensionality: A general rule of thumb is to set MinPts at least one greater than the dimensionality of the dataset. For spatiotemporal data, if you consider time as an additional dimension, this might increase MinPts accordingly.\nDensity Expectation: In areas where temporal density (i.e., frequent data points over short periods) is high, a larger MinPts may be necessary to differentiate between actual clusters and random fluctuations.\nHigher MinPts: Increasing the MinPts value generally results in a greater focus on more significant clusters, thus reducing the noise sensitivity. This setting can be useful in datasets where noise points are close to actual clusters, helping to distinguish between them more clearly.\nLower MinPts: A lower MinPts value makes the algorithm more sensitive to noise, and can lead to detecting more clusters. This setting is beneficial when the dataset contains many small but meaningful clusters that you do not want to miss.\n\n\nK-Nearest Neighbor Graph, 2000 points, 4 clusters. Code here"
  },
  {
    "objectID": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "href": "posts/DBSCAN/index.html#practical-tips-for-using-dbscan-in-spatiotemporal-analysis",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "Scale and Normalize:Ensure that spatial and temporal measurements are on comparable scales to avoid one type of distance overpowering the other.\nExperiment with Parameters: Due to the nature of DBSCAN and the diversity of spatiotemporal data, iterative testing and adjustment of parameters are often required.\nVisualize Results: Use visual tools to examine the clustering outcomes. This can help in fine-tuning parameters and understanding the spatial-temporal distribution of the data."
  },
  {
    "objectID": "posts/DBSCAN/index.html#conclusion",
    "href": "posts/DBSCAN/index.html#conclusion",
    "title": "DBSCAN Intro",
    "section": "",
    "text": "DBSCAN stands out for its ability to handle anomalies and discover clusters of arbitrary shapes, making it ideal for analyzing complex spatiotemporal datasets. By carefully selecting the parameters ε and MinPts, you can tailor DBSCAN effectively to your specific data characteristics, enhancing both the accuracy and relevancy of your clustering results.\nIncorporating visual aids, like plots of ε values or animations of the clustering process, can greatly enhance the understanding and application of DBSCAN in real-world scenarios. Whether dealing with urban planning, environmental monitoring, or dynamic population studies, DBSCAN offers a robust framework for insightful data analysis.\nPlease see the visualization below written in Javascript, based on Naftali Harris’ original implementation:"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Raspberry Pi Sensor Server Project\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\n\n\n\n\n\nOPTICS in Python\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\n\n\n\n\n\nIntro to OPTICS\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\n\n\n\n\n\nDBSCAN Intro\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I launched this blog in April of 2024 as part of a unique project aimed at creating a space for my thoughts and analyses, free from the constraints and subscriptions of platforms like Medium, and distinct from the algorithm-driven feeds of LinkedIn. While there’s certainly more content on the way, I hope this introduction serves as a more engaging placeholder than a simple “This is my blog.”"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Engineering/Science/Tech Blog",
    "section": "",
    "text": "Raspberry Pi Sensor Server Project\n\n\n\n\n\n\nRaspberry Pi\n\n\nIoT\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nOPTICS in Python\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative Adventures: Exploring the Frontiers with VAEs, GANs, and VAE-GANs\n\n\n\n\n\n\nML\n\n\nGenerative AI\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nIntro to OPTICS\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nOPTICS\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nDBSCAN Intro\n\n\n\n\n\n\nML\n\n\nClustering\n\n\nDBSCAN\n\n\n\n\n\n\n\n\n\nApr 19, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 16, 2024\n\n\nJesse Anderson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html",
    "href": "posts/OPTICSinPython/OPTICS.html",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "href": "posts/OPTICSinPython/OPTICS.html#import-libraries",
    "title": "OPTICS in Python",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n\nThis block imports all necessary Python libraries. ‘numpy’ and ‘matplotlib’ are used for numerical operations and plotting(respectively). ‘sklearn’ provides the OPTICS clustering algorithm(for better or worse) and the blob generator. ‘kneed’ is used to identify the “knee”/“elbow” point of the data, but didn’t work in this case. We will explore alternatives below. ‘matplotlib.colors’ and ‘scipy.spatial’ help with the visualization and the convex hull calculations respectively."
  },
  {
    "objectID": "posts/Pi-Sensor-Proj-May-2024/index.html",
    "href": "posts/Pi-Sensor-Proj-May-2024/index.html",
    "title": "Raspberry Pi Sensor Server Project",
    "section": "",
    "text": "I finally decided to use the Raspberry Pi 4 Model B+ 8gb I had lying around to play around with some sensors. Luckily I took an electrical engineering course in circuits[ECE210 at UIC], which made it pretty straight forward to wire things up. I had also already flashed an OS to the SD card and only encountered a few issues with booting up with the pi in its case(with fan!) and the temperature/humidity sensor plugged in. Please note that setting up VNC Server(RPI) and VNC Viewer(Desktop) will speed this up dramatically. Below is a pretty simple mockup of the connection I used with the Fan’s Power on pin 4[+,5.0VDC] and pin 14[-] and the DHT11 sensor on Pin 2[+,5.0VDC], Pin 6[-], and Pin 7[GPIO7].\n\nImage source: https://www.raspberrypi.com/documentation/computers/raspberry-pi.html\nAnd here’s the setup:\n\nAs a side note, Fritzing worked really well to generate the image above and I used the build at: https://github.com/Move2win/Fritzing-0.9.9.64.pc-Compiled-Build\nIt is a compiled .exe on a random github repository and one should take care….. but it was definitely faster than trying to build Fritzing from source.\nNext I got a basic python script working on my Raspberry Pi after installing the Adafruit_DHT library.\nInstallation was pretty straight forward. Enter this into the command prompt\nsudo apt-get install git-core\nNext:\ngit clone https://github.com/adafruit/Adafruit_Python_DHT.git\nChange directories:\ncd Adafruit_Python_DHT\nNow:\nsudo apt-get install build-essential python-dev\nFinally….:\nsudo python setup.py install\nNow create a .py file and enter the following:\n\n\nCode\n\n{python}\nimport Adafruit_DHT\nimport requests\nimport time\nfrom datetime import datetime\n\n# Sensor setup\nSENSOR = Adafruit_DHT.DHT11  # Using DHT11 sensor\nPIN = 4  # Change this to the GPIO pin number that the sensor is connected to\n\n# Buffer to store data\ndata_buffer = []\n\nwhile True:\n    # Read humidity and temperature from DHT sensor\n    humidityPercent, temperature_C = Adafruit_DHT.read_retry(SENSOR, PIN)\n    \n    if humidityPercent is not None and temperature_C is not None:\n        \n        # Prepare the data payload\n        temperature_F = temperature_C * 9.0 / 5.0 + 32\n        now = datetime.now()\n        date = now.strftime(\"%m-%d-%Y\")\n        timeNow = now.strftime(\"%H:%M:%S\")\n        \n        data = {\n            'date': date,\n            'time': timeNow,\n            'humidityPercent': humidityPercent,\n            'temperatureFahrenheit': temperature_F,\n            'temperatureCelsius': temperature_C\n        }\n        # Log data to buffer\n        data_buffer.append(data)\n        print(f\"Logged data: {data}\")\n        #clear data buffer\n        data_buffer.clear()\n    # Wait for 1 second before logging the next reading\n    time.sleep(1)\n\n\n\nCode Summary\n\nThis Python code snippet reads humidity and temperature data from a DHT11 sensor connected to a Raspberry Pi, formats the data with timestamps, and logs it. Here’s a breakdown of the key steps:\n\nImports Necessary Libraries:\n\nAdafruit_DHT for interfacing with the DHT11 sensor.\nrequests for sending HTTP requests (though not used in this snippet).\ntime for managing sleep intervals.\ndatetime for timestamping data.\n\nSensor Setup:\n\nThe sensor type is defined as DHT11.\nThe GPIO pin to which the sensor is connected is set to PIN = 4.\n\nData Buffer Initialization:\n\nAn empty list data_buffer is initialized to store the sensor readings.\n\nContinuous Data Logging Loop:\n\nEnters an infinite loop to continuously read data from the sensor.\nReads humidity and temperature (in Celsius) from the DHT11 sensor using Adafruit_DHT.read_retry(SENSOR, PIN).\nIf valid data is read (i.e., not None), the following actions are performed:\n\nConverts the temperature from Celsius to Fahrenheit.\nGets the current date and time using datetime.now() and formats them as strings.\nPrepares a dictionary data containing the date, time, humidity, temperature in Celsius, and temperature in Fahrenheit.\nAppends the data dictionary to data_buffer.\nLogs the data by printing it to the console.\nClears the data_buffer list to reset it for the next set of readings.\n\nWaits for 1 second before taking the next reading using time.sleep(1).\n\n\nOverall, this code continuously monitors environmental data from a DHT11 sensor, timestamps the readings, and logs them to a buffer (though the buffer is cleared immediately in this example).\n\nThis script will grab the current date, time, Percent Humidity, Temp in Fahrenheit/Celsius and display it to the user. Note that we are appending it to a buffer which will become important later.\nNext I tried various server/serverless options to get realtime data and decided on Vercel. I also tried out ThingSpeak and really liked its interface, but the fact that I would have to pay(if I wasn’t a student) made me consider other options. To implement a basic realtime logging of sensor data in ThingSpeak one would sign up for an account, create a channel, populate a channel and add field labels such as Temperature and Humidity, and save the channel to receive a unique Channel ID and API key. The code for thingspeak is pretty straightforward and one can implement the code below to populate a ThingSpeak channel.\n\n\nCode\n\n{python}\nimport Adafruit_DHT\nimport requests\nimport time\nfrom datetime import datetime\n\n# Sensor setup\nSENSOR = Adafruit_DHT.DHT11  # Using DHT11 sensor\nPIN = 4  # Change this to the GPIO pin number that the sensor is connected to\n\n# Buffer to store data\ndata_buffer = []\n\n# ThingSpeak URL\nbase_url = 'https://api.thingspeak.com/update'\n\n# Replace 'YOUR_API_KEY' with your actual ThingSpeak channel write API key\napi_key = 'YOUR_API_KEY'\n\nwhile True:\n    # Read humidity and temperature from DHT sensor\n    humidityPercent, temperature_C = Adafruit_DHT.read_retry(SENSOR, PIN)\n    \n    if humidityPercent is not None and temperature_C is not None:\n        \n        # Prepare the data payload\n        temperature_F = temperature_C * 9.0 / 5.0 + 32\n        now = datetime.now()\n        date = now.strftime(\"%m-%d-%Y\")\n        timeNow = now.strftime(\"%H:%M:%S\")\n        \n        data = {\n            'api_key': api_key,\n            'field1': temperature_C,\n            'field2': temperature_F,\n            'field3': humidityPercent\n        }\n        # Send data to ThingSpeak\n        try:\n            response = requests.get(base_url, params=payload)\n            print('Data posted to ThingSpeak', response.text)\n        except requests.exceptions.RequestException as e:\n            print('Failed to send data:', e)\n        # Log data to buffer\n        data_buffer.append(data)\n        print(f\"Logged data: {data}\")\n        #clear data buffer\n        data_buffer.clear()\n    # Wait for 1 second before logging the next reading\n    time.sleep(1)\n\n\n\nCode Summary\n\nThis Python script reads environmental data from a DHT11 sensor and sends it to the ThingSpeak cloud service. Here is a summary of the main components and functionality:\n\nImports Necessary Libraries:\n\nAdafruit_DHT for interfacing with the DHT11 sensor.\nrequests for making HTTP requests to ThingSpeak.\ntime for managing sleep intervals.\ndatetime for timestamping data.\n\nSensor Setup:\n\nThe sensor type is defined as DHT11.\nThe GPIO pin to which the sensor is connected is set to PIN = 4.\n\nData Buffer Initialization:\n\nAn empty list data_buffer is initialized to store the sensor readings.\n\nThingSpeak Configuration:\n\nThe base URL for ThingSpeak updates is defined as base_url.\nAn API key placeholder api_key is set to 'YOUR_API_KEY'. Replace this with your actual ThingSpeak channel write API key.\n\nContinuous Data Logging Loop:\n\nEnters an infinite loop to continuously read data from the sensor.\nReads humidity and temperature (in Celsius) from the DHT11 sensor using Adafruit_DHT.read_retry(SENSOR, PIN).\nIf valid data is read (i.e., not None), the following actions are performed:\n\nConverts the temperature from Celsius to Fahrenheit.\nGets the current date and time using datetime.now() and formats them as strings.\nPrepares a dictionary data containing the API key, temperature in Celsius, temperature in Fahrenheit, and humidity.\nSends the data dictionary to ThingSpeak via a GET request using requests.get().\nLogs the response from ThingSpeak and any exceptions that occur during the request.\nLogs the data to data_buffer by appending the data dictionary.\nClears the data_buffer list to reset it for the next set of readings.\n\nWaits for 1 second before taking the next reading using time.sleep(1).\n\n\nThis script continuously monitors environmental data from a DHT11 sensor, sends the data to ThingSpeak, and logs the readings locally.\n\nThe resulting channel is functional enough:\n\nLocation: https://thingspeak.com/channels/2545447\nRealistically, I may incorporate sending the data to ThingSpeak as well as the other option I chose for monitoring.\nI got my account up and running with Vercel, installing it on a private Github repo. I then installed Node.js and npm. Then I navigated to the repo and opened up a command prompt:\nnpm init -y\nI then created a ‘/api’ directory and created a file for my sensor data handling called ‘/api/sensor.js’. Note the addition of the API_KEY variable that you should add to your Vercel global environment variables to make sure there’s some added security.\n\n\nCode\n\n{javascript}\nconst API_KEY = process.env.API_KEY; // Retrieve the API key from environment variables\n\n// Function for real-time data monitoring\nmodule.exports.realTimeDataMonitoring = async (req, res) =&gt; {\n    if (req.method === 'POST') {\n        try {\n            // Extract API key from request headers\n            const providedApiKey = req.headers['x-api-key'];\n\n            // Check if API key is provided and matches the expected API key\n            if (!providedApiKey || providedApiKey !== API_KEY) {\n                return res.status(401).json({ error: 'Unauthorized' });\n            }\n\n            // Extract data from request body just to log and test the handling\n            const { temperature, humidity } = req.body;\n\n            // Log the data received to console for verification\n            console.log(`Received - Temperature: ${temperature}, Humidity: ${humidity}`);\n\n            // Send a successful response back to the client without any database interaction\n            res.status(200).json({ message: 'Data received successfully!', temperature, humidity });\n        } catch (e) {\n            // Handle errors and send an error response\n            console.error(e);\n            res.status(500).json({ error: 'An internal error occurred', details: e.message });\n        }\n    } else {\n        // Respond with method not allowed if not a POST request\n        res.status(405).json({ error: 'Method not allowed' });\n    }\n};\n\n\n\nCode Summary\n\nThis JavaScript code defines a function for real-time data monitoring, intended to be used as part of an API in a Node.js environment. The function is exported as realTimeDataMonitoring and handles HTTP POST requests to log temperature and humidity data received from clients. Here is a summary of its main components and functionality:\n\nAPI Key Retrieval:\n\nThe API key is retrieved from environment variables using process.env.API_KEY.\n\nFunction Definition:\n\nThe realTimeDataMonitoring function is an asynchronous function designed to handle HTTP requests, specifically POST requests.\n\nRequest Handling:\n\nThe function first checks if the request method is POST. If not, it responds with a 405 Method Not Allowed status.\n\nAPI Key Validation:\n\nThe function extracts the provided API key from the request headers (x-api-key).\nIt checks if the provided API key matches the expected API key. If not, it responds with a 401 Unauthorized status.\n\nData Extraction and Logging:\n\nIf the API key is valid, the function extracts temperature and humidity data from the request body.\nIt logs the received data to the console for verification.\n\nResponse to Client:\n\nThe function sends a 200 OK response back to the client, confirming successful data reception, along with the received temperature and humidity data.\n\nError Handling:\n\nIf an error occurs during the process, it catches the exception, logs the error, and responds with a 500 Internal Server Error status, including the error details.\n\nExporting the Function:\n\nThe function is exported using module.exports for use in other parts of the application.\n\n\nThis setup ensures secure and controlled data reception for real-time monitoring, with proper error handling and validation mechanisms in place.\n\nFrom here I pushed the changes to github which caused the Vercel site to redeploy and changed the code on the Raspberry Pi to the following:\n\n\nCode\n\n{python}\nimport Adafruit_DHT\nimport requests\nimport time\nfrom datetime import datetime\n\n# Sensor setup\nSENSOR = Adafruit_DHT.DHT11  # Using DHT11 sensor\nPIN = 4  # Change this to the GPIO pin number that the sensor is connected to\n\n# Buffer to store data\ndata_buffer = []\n\n# Vercel endpoint URL\nURL = 'https://your-vercel-url.vercel.app/api/sensor'\n\n# Define your Vercel API key\nAPI_KEY = 'YOUR_API_KEY'\n\nwhile True:\n    # Read humidity and temperature from DHT sensor\n    humidityPercent, temperature_C = Adafruit_DHT.read_retry(SENSOR, PIN)\n    \n    if humidityPercent is not None and temperature_C is not None:\n        \n        # Prepare the data payload\n        temperature_F = temperature_C * 9.0 / 5.0 + 32\n        now = datetime.now()\n        date = now.strftime(\"%m-%d-%Y\")\n        timeNow = now.strftime(\"%H:%M:%S\")\n        \n        data = {\n            'date': date,\n            'time': timeNow,\n            'humidityPercent': humidityPercent,\n            'temperatureFahrenheit': temperature_F,\n            'temperatureCelsius': temperature_C\n        }\n\n        # Send data to Vercel\n        try:\n            # Send the data to the server with the API key in the headers\n            headers = {'x-api-key': API_KEY}\n            response = requests.post(URL, json=data, headers=headers)\n            print(f\"Data sent to server: {response.text}\")\n        except requests.exceptions.RequestException as e:\n            # Handle exceptions that arise from sending the request\n            print(f\"Failed to send data to server: {e}\")\n    else:\n        # Handle cases where sensor fails to read\n        print(\"Failed to retrieve data from sensor\")\n    \n    # Wait for 1 second before logging the next reading\n    time.sleep(1)\n\n\n\nCode Summary\n\nThis Python script reads environmental data from a DHT11 sensor and sends it to a Vercel endpoint for real-time monitoring. Here are the main components and functionalities of the script:\n\nImports Necessary Libraries:\n\nAdafruit_DHT for interfacing with the DHT11 sensor.\nrequests for making HTTP POST requests to the Vercel endpoint.\ntime for managing sleep intervals.\ndatetime for timestamping data.\n\nSensor Setup:\n\nDefines the sensor type as DHT11.\nSets the GPIO pin connected to the sensor as PIN = 4.\n\nData Buffer Initialization:\n\nInitializes an empty list data_buffer to store the sensor readings (though it is not used in this script).\n\nVercel Configuration:\n\nDefines the Vercel endpoint URL (URL).\nSets an API key (API_KEY) for authenticating with the Vercel endpoint.\n\nContinuous Data Logging Loop:\n\nEnters an infinite loop to continuously read data from the sensor.\nReads humidity and temperature (in Celsius) from the DHT11 sensor using Adafruit_DHT.read_retry(SENSOR, PIN).\nIf valid data is read (i.e., not None), the following actions are performed:\n\nConverts the temperature from Celsius to Fahrenheit.\nGets the current date and time using datetime.now() and formats them as strings.\nPrepares a dictionary data containing the date, time, humidity, temperature in Celsius, and temperature in Fahrenheit.\nSends the data dictionary to the Vercel endpoint via a POST request using requests.post().\nPrints the server’s response to the console.\nCatches and handles any exceptions that arise from sending the request, printing an error message if the request fails.\n\nIf the sensor fails to read data, it prints an error message.\nWaits for 1 second before taking the next reading using time.sleep(1).\n\n\nThis script continuously monitors environmental data from a DHT11 sensor, sends the data to a Vercel endpoint for real-time monitoring, and logs the readings locally. It includes error handling for both sensor read failures and HTTP request failures.\n\nI will omit the fact that I spent forever trying to also get MongoDB to work within Vercel and later found out that I needed to perform some sort of installation to get it to work. I did however find out that Vercel offered a PostgreSQL implementation so I could store my data as it came in. I navigated to Storage and found it was a few pretty simple clicks to get it going.\nI created a table using:\n{sql}\nCREATE TABLE readings (\n    id SERIAL PRIMARY KEY,\n    Date DATE NOT NULL,\n    Time TIME NOT NULL,\n    humidityPercent FLOAT NOT NULL,\n    temperatureFahrenheit FLOAT NOT NULL,\n    temperatureCelsius FLOAT NOT NULL,\n);\nAnd extended sensor.js a bit…Namely I edited it so it can handle a data_buffer of multiple points as well as singular points to cut down on server connection overhead. I also added some logging for the sake of sanity on the off chance anything ever goes wrong.\nNote, you need to install pg on your github directory for PostgreSQL to work.\ncd yourDirectory\nnpm install pg\n\n\nCode\n\n{javascript}\nconst API_KEY = process.env.API_KEY; // Retrieve the API key from environment variables\n\nconsole.log('API Key:', API_KEY);  // For debugging purposes\n\nconst { Pool } = require('pg');\n\n// PostgreSQL connection setup\nconst pool = new Pool({\n    connectionString: process.env.POSTGRES_URL, // Make sure to set this environment variable in Vercel\n    ssl: {\n        rejectUnauthorized: false\n    }\n});\n\n// Function to handle logging and appending data to PostgreSQL\nconst handleSensorData = async (req, res) =&gt; {\n    if (req.method !== 'POST') {\n        return res.status(405).json({ error: 'Method not allowed' });\n    }\n\n    try {\n        console.log('Request received');  // For debugging purposes\n\n        // Extract API key from request headers\n        const providedApiKey = req.headers['x-api-key'];\n        console.log('Provided API Key:', providedApiKey);  // For debugging purposes\n\n        // Check if API key is provided and matches the expected API key\n        if (!providedApiKey || providedApiKey !== API_KEY) {\n            return res.status(401).json({ error: 'Unauthorized' });\n        }\n\n        // Extract data from request body\n        const data = req.body;\n\n        // Log the data received to console for verification\n        console.log('Received data:', JSON.stringify(data, null, 2));\n\n        let query;\n        let values;\n\n        // Check if data is an array (multiple readings) or a single reading\n        if (Array.isArray(data)) {\n            // SQL query to insert multiple readings\n            query = `\n                INSERT INTO readings (temperatureCelsius, temperatureFahrenheit, humidityPercent, date, time)\n                VALUES ${data.map((_, index) =&gt; `($${index * 5 + 1}, $${index * 5 + 2}, $${index * 5 + 3}, $${index * 5 + 4}, $${index * 5 + 5})`).join(', ')}\n                RETURNING *;\n            `;\n\n            // Flatten the array of data into a single array of values\n            values = data.flatMap(({ temperatureCelsius, temperatureFahrenheit, humidityPercent, date, time }) =&gt; [\n                temperatureCelsius !== null && temperatureCelsius !== undefined ? temperatureCelsius : 0,\n                temperatureFahrenheit !== null && temperatureFahrenheit !== undefined ? temperatureFahrenheit : 0,\n                humidityPercent !== null && humidityPercent !== undefined ? humidityPercent : 0,\n                date !== null && date !== undefined ? date : new Date().toISOString().split('T')[0],\n                time !== null && time !== undefined ? time : new Date().toISOString().split('T')[1].split('.')[0]\n            ]);\n        } else {\n            // SQL query to insert a single reading\n            query = `\n                INSERT INTO readings (temperatureCelsius, temperatureFahrenheit, humidityPercent, date, time)\n                VALUES ($1, $2, $3, $4, $5)\n                RETURNING *;\n            `;\n\n            // Single reading values\n            values = [\n                data.temperatureCelsius !== null && data.temperatureCelsius !== undefined ? data.temperatureCelsius : 0,\n                data.temperatureFahrenheit !== null && data.temperatureFahrenheit !== undefined ? data.temperatureFahrenheit : 0,\n                data.humidityPercent !== null && data.humidityPercent !== undefined ? data.humidityPercent : 0,\n                data.date !== null && data.date !== undefined ? data.date : new Date().toISOString().split('T')[0],\n                data.time !== null && data.time !== undefined ? data.time : new Date().toISOString().split('T')[1].split('.')[0]\n            ];\n        }\n\n        // Execute the query\n        const result = await pool.query(query, values);\n        console.log('Data stored in PostgreSQL:', result.rows);\n\n        // Send a successful response back to the client\n        res.status(200).json({ message: 'Data received and stored successfully!', data: result.rows });\n    } catch (e) {\n        // Handle errors and send an error response\n        console.error(\"Error connecting to PostgreSQL or inserting data:\", e);\n        res.status(500).json({ error: 'Failed to connect to database or insert data', details: e.message });\n    }\n};\n\n// Export the function for Vercel\nmodule.exports = handleSensorData;\n\n\n\nCode Summary\n\nThis JavaScript code handles real-time sensor data logging and storage in a PostgreSQL database. It is designed to be used in a Node.js environment and is likely intended to run on Vercel. Here is a summary of the main components and functionality:\n\nAPI Key Retrieval:\n\nThe API key is retrieved from environment variables using process.env.API_KEY.\n\nDebugging Logs:\n\nLogs the API key and other debugging information to the console.\n\nPostgreSQL Connection Setup:\n\nUses the pg library to create a connection pool to the PostgreSQL database.\nThe connection string is retrieved from the POSTGRES_URL environment variable.\nSSL connection is configured with rejectUnauthorized: false.\n\nFunction to Handle Sensor Data:\n\nThe function handleSensorData is exported for use in Vercel.\nIt handles HTTP POST requests to log and store sensor data.\n\nRequest Handling:\n\nChecks if the request method is POST. If not, it responds with a 405 Method Not Allowed status.\nExtracts the provided API key from the request headers and logs it for debugging.\nCompares the provided API key with the expected API key. If they do not match, it responds with a 401 Unauthorized status.\n\nData Extraction and Logging:\n\nExtracts the sensor data from the request body and logs it to the console for verification.\n\nSQL Query Preparation:\n\nPrepares an SQL query to insert the sensor data into the readings table in PostgreSQL.\nSupports both single reading and multiple readings (batch) insertions.\nConstructs the query and flattens the data array for batch insertions.\n\nDatabase Insertion:\n\nExecutes the SQL query using the connection pool.\nLogs the inserted data to the console.\n\nResponse to Client:\n\nSends a 200 OK response back to the client, confirming successful data reception and storage, along with the inserted data.\n\nError Handling:\n\nCatches any exceptions that occur during the database connection or data insertion.\nLogs the error and responds with a 500 Internal Server Error status, including error details.\n\n\nThis script ensures secure, real-time logging of sensor data, with proper validation and error handling, and stores the data in a PostgreSQL database.\n\nFrom there what my final product looks like is an html page which displays the latest sensor readings, a javascript function to pull the entire dataset, a javascript function which pushes the data into the database, and the python script on the raspberry pi sending the data. I actually bunch up the data before I push it to save on the overhead costs of establishing a connection. Realistically the data isn’t too time sensitive and a window of 1-5 minutes is perfectly acceptable for readings. They are below:\nHtml:\n\n\nCode\n\n{html}\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Download Table Data&lt;/title&gt;\n    &lt;style&gt;\n        /* CSS to style the table with borders */\n        table {\n            border-collapse: collapse;\n            width: 100%;\n        }\n        th, td {\n            border: 1px solid #dddddd;\n            text-align: left;\n            padding: 8px;\n        }\n        th {\n            background-color: #f2f2f2;\n        }\n    &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Download Table Data&lt;/h1&gt;\n    &lt;a href=\"/api/displaySQL\"&gt;Download Table Data as CSV&lt;/a&gt;\n\n    &lt;h2&gt;Current Readings&lt;/h2&gt;\n    &lt;table id=\"currentReadings\"&gt;\n&lt;tr&gt;\n&lt;th&gt;Date&lt;/th&gt;\n&lt;th&gt;Time&lt;/th&gt;\n&lt;th&gt;Humidity (%)&lt;/th&gt;\n&lt;th&gt;Temperature (F)&lt;/th&gt;\n&lt;th&gt;Temperature (C)&lt;/th&gt;\n&lt;/tr&gt;\n&lt;/table&gt;\n\n    &lt;script&gt;\n        // Function to format the date\n        function formatDate(dateString) {\n            const date = new Date(dateString);\n            return date.toLocaleDateString('en-US');\n}\n\n        // Fetch the last row data from the serverless function\n        fetch('/api/displayLastRowSQL')\n            .then(response =&gt; response.json())\n            .then(data =&gt; {\n                // Extract the relevant data from the last row\n                const time = data.time;\n                const humidity = data.humiditypercent;\n                const temperatureF = data.temperaturefahrenheit;\n                const temperatureC = data.temperaturecelsius;\n                const date = formatDate(data.date);\n\n                // Display the data in the HTML table\n                const currentReadingsTable = document.getElementById('currentReadings');\n                const newRow = currentReadingsTable.insertRow();\n                newRow.innerHTML = `\n                    &lt;td&gt;${date}&lt;/td&gt;\n                    &lt;td&gt;${time}&lt;/td&gt;\n                    &lt;td&gt;${humidity}&lt;/td&gt;\n                    &lt;td&gt;${temperatureF}&lt;/td&gt;\n                    &lt;td&gt;${temperatureC}&lt;/td&gt;\n                `;\n            })\n            .catch(error =&gt; {\n                // Handle errors\n                console.error('Error fetching current readings:', error);\n            });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nJavascript to pull entire dataset:\n\n\nCode\n\n{javascript}\n// Import the necessary libraries\nconst { Pool } = require('pg');\nconst { Parser } = require('json2csv');\n\n// PostgreSQL connection setup\nconst pool = new Pool({\n    connectionString: process.env.POSTGRES_URL, // Make sure to set this environment variable in Vercel\n    ssl: {\n        rejectUnauthorized: false\n    }\n});\n\n// Serverless function exported for Vercel //note slow loop of formatted rows. FIX!!!!\nmodule.exports = async (req, res) =&gt; {\n    try {\n        // SQL Query to select all records from the table\n        const query = 'SELECT * FROM readings';\n        \n        // Execute the query\n        const result = await pool.query(query);\n\n        // Format the date field in each row\n        const formattedRows = result.rows.map(row =&gt; {\n            const formattedDate = row.date ? new Date(row.date).toLocaleDateString('en-US') : ''; // Format date to locale string (excluding time)\n            return { ...row, date: formattedDate };\n        });\n\n        // Convert the formatted rows to CSV format\n        const json2csvParser = new Parser();\n        const csvData = json2csvParser.parse(formattedRows);\n\n        // Set response headers to indicate a CSV file download\n        res.setHeader('Content-Type', 'text/csv');\n        res.setHeader('Content-Disposition', 'attachment; filename=\"table_data.csv\"');\n\n        // Send the CSV data as the response\n        res.status(200).send(csvData);\n    } catch (error) {\n        // Handle errors and send an error response\n        console.error(\"Error fetching data from PostgreSQL:\", error);\n        res.status(500).json({ error: 'Failed to fetch data from database', details: error.message });\n    }\n};\n\n\n\nCode Summary\n\nThis JavaScript code defines a serverless function that fetches data from a PostgreSQL database, formats the data, converts it to CSV format, and sends it as a downloadable file in the HTTP response. It is designed to be deployed on Vercel. Here is a summary of the main components and functionality:\n\nImport Necessary Libraries:\n\npg for connecting to the PostgreSQL database.\njson2csv for converting JSON data to CSV format.\n\nPostgreSQL Connection Setup:\n\nUses the pg library to create a connection pool to the PostgreSQL database.\nThe connection string is retrieved from the POSTGRES_URL environment variable.\nSSL connection is configured with rejectUnauthorized: false.\n\nServerless Function Exported for Vercel:\n\nThe function is exported to be used as a serverless function in Vercel.\n\nFetching Data from PostgreSQL:\n\nDefines an SQL query to select all records from the readings table.\nExecutes the query using the connection pool and stores the result.\n\nFormatting Data:\n\nFormats the date field in each row to a locale date string (excluding time).\nUses Array.map() to iterate over each row and apply the date formatting.\n\nConverting Data to CSV:\n\nUses json2csv to convert the formatted JSON data to CSV format.\nInitializes a Parser object and calls the parse() method with the formatted rows.\n\nSetting Response Headers for CSV Download:\n\nSets the Content-Type header to text/csv.\nSets the Content-Disposition header to indicate a file download with the filename table_data.csv.\n\nSending the CSV Data as Response:\n\nSends the CSV data as the HTTP response with a 200 OK status.\n\nError Handling:\n\nCatches any errors that occur during data fetching or processing.\nLogs the error and responds with a 500 Internal Server Error status, including error details.\n\n\nThis script ensures that data from the PostgreSQL database is formatted and made available for download as a CSV file, with proper error handling to manage potential issues during execution.\n\nJavascript to display latest sensor readings:\n\n\nCode\n\n{javascript}\n// Import the necessary libraries\nconst { Pool } = require('pg');\n\n// PostgreSQL connection setup\nconst pool = new Pool({\n    connectionString: process.env.POSTGRES_URL, // Make sure to set this environment variable in Vercel\n    ssl: {\n        rejectUnauthorized: false\n    }\n});\n\n// Serverless function exported for Vercel\nmodule.exports = async (req, res) =&gt; {\n    try {\n        // SQL Query to select the last row from the table with the specified order\n        const query = `\n            SELECT time,\n                   humiditypercent,\n                   temperaturefahrenheit,\n                   temperaturecelsius,\n                   date\n            FROM readings\n            ORDER BY id DESC\n            LIMIT 1\n        `;\n        \n        // Execute the query\n        const result = await pool.query(query);\n\n        // Check if result.rows is not empty\n        if (result.rows.length === 0) {\n            res.status(404).json({ error: 'No data found in the database' });\n            return;\n        }\n\n        // Format the date field to 'YYYY-MM-DD'\n        const row = result.rows[0];\n        row.date = row.date ? new Date(row.date).toLocaleDateString('en-us').slice(0, 10) : 'Invalid date';\n\n        // Debugging: Log the formatted date to inspect it\n        console.log(\"Formatted date:\", row.date);\n\n        // Send the last row data as the response\n        res.status(200).json(row); // Assuming there is at least one row in the table\n    } catch (error) {\n        // Handle errors and send an error response\n        console.error(\"Error fetching data from PostgreSQL:\", error);\n        res.status(500).json({ error: 'Failed to fetch data from database', details: error.message });\n    }\n};\n\n\n\nCode Summary\n\nThis JavaScript code defines a serverless function that fetches the most recent data entry from a PostgreSQL database and returns it as a JSON response. It is designed to be deployed on Vercel. Here is a summary of the main components and functionality:\n\nImport Necessary Libraries:\n\npg for connecting to the PostgreSQL database.\n\nPostgreSQL Connection Setup:\n\nUses the pg library to create a connection pool to the PostgreSQL database.\nThe connection string is retrieved from the POSTGRES_URL environment variable.\nSSL connection is configured with rejectUnauthorized: false.\n\nServerless Function Exported for Vercel:\n\nThe function is exported to be used as a serverless function in Vercel.\n\nFetching the Latest Data from PostgreSQL:\n\nDefines an SQL query to select the last row from the readings table, ordered by the id column in descending order.\nLimits the query to return only one row (LIMIT 1).\n\nExecuting the SQL Query:\n\nExecutes the query using the connection pool and stores the result.\n\nChecking if Data is Available:\n\nChecks if the result.rows array is not empty.\nIf no data is found, responds with a 404 Not Found status and an error message.\n\nFormatting the Date Field:\n\nFormats the date field in the retrieved row to the ‘YYYY-MM-DD’ format.\nUses new Date(row.date).toLocaleDateString('en-us').slice(0, 10) to format the date, ensuring only the date part is included.\n\nDebugging Log:\n\nLogs the formatted date to the console for inspection.\n\nSending the Data as Response:\n\nSends the retrieved and formatted data as the HTTP response with a 200 OK status.\n\nError Handling:\n\nCatches any errors that occur during data fetching or processing.\nLogs the error and responds with a 500 Internal Server Error status, including error details.\n\n\nThis script ensures that the most recent data entry from the PostgreSQL database is retrieved, formatted, and returned as a JSON response, with proper error handling to manage potential issues during execution. The debugging log provides insight into the formatted date for verification.\n\nPython script on the raspberry pi:\n\n\nCode\n\n{python}\nimport Adafruit_DHT\nimport requests\nimport time\nfrom datetime import datetime\n\n# Define your Vercel API key\nAPI_KEY = 'YOUR_API_KEY'\n\n# Sensor setup\nSENSOR = Adafruit_DHT.DHT11  # Using DHT11 sensor\nPIN = 4  # Change this to the GPIO pin number that the sensor is connected to\n\n# Vercel endpoint URL\nURL = 'https://your-vercel-url.vercel.app/api/sensor'\n\n# Buffer to store data\ndata_buffer = []\ni = 1\ndef send_data(data):\n    \"\"\"Send data to the server.\"\"\"\n    try:\n        # Include API key in the headers\n        headers = {'X-API-Key': API_KEY}\n        response = requests.post(URL, json=data, headers=headers)\n        print(f\"Data sent to server: {response.text}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to send data to server: {e}\")\n\nwhile True:\n    # Read humidity and temperature from DHT sensor\n    humidityPercent, temperature_C = Adafruit_DHT.read_retry(SENSOR, PIN)\n    \n    if humidityPercent is not None and temperature_C is not None:\n        # Prepare the data payload\n        temperature_F = temperature_C * 9.0 / 5.0 + 32\n        \n        now = datetime.now()\n        date = now.strftime(\"%m-%d-%Y\")\n        timeNow = now.strftime(\"%H:%M:%S\")\n        \n        data = {\n            'date': date,\n            'time': timeNow,\n            'humidityPercent': humidityPercent,\n            'temperatureFahrenheit': temperature_F,\n            'temperatureCelsius': temperature_C\n        }\n        \n        # Log data to buffer\n        data_buffer.append(data)\n        if i % 60 == 0:\n            print(f\"Logged data: {data}\")\n        \n        # Check if 300 seconds have passed\n        if len(data_buffer) &gt;= 300:\n            send_data(data_buffer)\n            \n            # Clear the buffer after sending\n            data_buffer.clear()\n            i = 0\n    else:\n        # Handle cases where sensor fails to read\n        print(\"Failed to retrieve data from sensor\")\n    \n    # Wait for 1 second before logging the next reading\n    time.sleep(1)\n    i += 1\n\n\n\nCode\n\nThis Python script reads environmental data from a DHT11 sensor every second, stores it in a buffer, and sends the buffered data to a Vercel endpoint every 5 minutes (300 seconds). The script includes error handling for sensor read failures and HTTP request failures. Here is a summary of the main components and functionality:\n\nImports Necessary Libraries:\n\nAdafruit_DHT for interfacing with the DHT11 sensor.\nrequests for making HTTP POST requests to the Vercel endpoint.\ntime for managing sleep intervals.\ndatetime for timestamping data.\n\nVercel API Key and Endpoint Setup:\n\nDefines the Vercel API key (API_KEY) and endpoint URL (URL).\n\nSensor Setup:\n\nDefines the sensor type as DHT11.\nSets the GPIO pin connected to the sensor (PIN = 4).\n\nBuffer Initialization:\n\nInitializes an empty list data_buffer to store sensor readings.\nInitializes a counter i to keep track of the number of readings.\n\nData Sending Function:\n\nsend_data(data): Sends the buffered data to the Vercel endpoint.\n\nIncludes the API key in the request headers.\nSends the data using an HTTP POST request.\nLogs the response or any exceptions that occur.\n\n\nContinuous Data Logging Loop:\n\nEnters an infinite loop to continuously read data from the sensor every second.\nReads humidity and temperature (in Celsius) from the DHT11 sensor using Adafruit_DHT.read_retry(SENSOR, PIN).\nIf valid data is read (i.e., not None), the following actions are performed:\n\nConverts the temperature from Celsius to Fahrenheit.\nGets the current date and time using datetime.now() and formats them as strings.\nPrepares a dictionary data containing the date, time, humidity, temperature in Celsius, and temperature in Fahrenheit.\nAppends the data dictionary to data_buffer.\nLogs the data to the console every 60 readings.\nChecks if 300 readings (300 seconds) have been collected.\n\nIf so, calls send_data(data_buffer) to send the buffered data to the Vercel endpoint.\nClears the data_buffer and resets the counter i.\n\n\nIf the sensor fails to read data, it logs an error message.\nWaits for 1 second before taking the next reading using time.sleep(1).\nIncrements the counter i with each iteration.\n\n\nThis script ensures continuous monitoring and logging of environmental data from a DHT11 sensor, with periodic transmission of the data to a Vercel endpoint for real-time monitoring or further processing.\n\nI intend to add more sensors such as a VOC sensor, CO2 sensor, PM2.5/PM10 sensor to have real time air quality data. That should be plug and play and a few lines of code. Getting the raspberry pi, server, and database to get along is a lot more work than wiring up a few sensors. I will also likely throw the data into a mongoDB and also push the data to ThingSpeak regularly once I have figured out what the best storage medium is. Unfortunately server uptime is counted as compute time for the purpose of using PostgreSQL in Vercel, so its great for testing, but definitely won’t be my long term solution. I might just do the unhinged option and use Google Sheets as a database. It should be possible to have up to 10 million cells which, when coupled with the data being logged at Date, Time, Humidity, TempF, and TempC that means I can have roughly 2 million rows before I need to think of pushing to another sheet. With a safety factor of 2 I have 1 million and that means I have 1,000,000/60 = 16,666 seconds/60 = 277 hours/24 = 11.5 days of data before I need to consider using another sheet. I can likely shorten this to 7 days and dynamically generate a new sheet every week.\n**Edit**: I actually went ahead and tried the google sheets option. Created a Google Cloud Project, enabled the Google Sheets API, and created credentials/downloaded the resultant JSON.\nI also had to make sure some Google Python libraries were installed:\npip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\nNext I implemented a pretty basic program to send a few values to a Google Sheet:\n\n\nCode\n\n{python}\nimport os\nimport google.auth\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\nfrom datetime import datetime\n\n# Define the scopes required for the Google Sheets API\nSCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n\n# Function to get authenticated service\ndef get_sheets_service():\n    creds = None\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n    service = build('sheets', 'v4', credentials=creds)\n    return service\n\n# Function to create a new Google Sheet with a given name\ndef create_new_sheet(sheet_name):\n    service = get_sheets_service()\n    spreadsheet = {\n        'properties': {\n            'title': sheet_name\n        }\n    }\n    spreadsheet = service.spreadsheets().create(body=spreadsheet, fields='spreadsheetId').execute()\n    print(f\"Created new spreadsheet with ID: {spreadsheet.get('spreadsheetId')}, Name: {sheet_name}\")\n    return spreadsheet.get('spreadsheetId')\n\n# Function to check the number of cells in the Google Sheet and create a new one if it doesn't exist\ndef check_sheet_size(spreadsheet_id, new_sheet_name):\n    service = get_sheets_service()\n    try:\n        sheet = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n        sheets = sheet.get('sheets', [])\n    except HttpError as e:\n        print(f\"HttpError encountered: Status code: {e.resp.status}, Reason: {e.error_details}\")\n        if e.resp.status == 404:\n            print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Creating a new sheet.\")\n            return create_new_sheet(new_sheet_name), 0\n        else:\n            raise\n\n    total_cells = 0\n    for sheet in sheets:\n        properties = sheet.get('properties', {})\n        grid_properties = properties.get('gridProperties', {})\n        rows = grid_properties.get('rowCount', 0)\n        cols = grid_properties.get('columnCount', 0)\n        total_cells += rows * cols\n\n    return spreadsheet_id, total_cells\n\n# Function to read data from a Google Sheet\ndef read_data(spreadsheet_id, range_name):\n    service = get_sheets_service()\n    result = service.spreadsheets().values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()\n    rows = result.get('values', [])\n    return rows\n\n# Function to write data to a Google Sheet\ndef write_data(spreadsheet_id, range_name, values):\n    service = get_sheets_service()\n    body = {\n        'values': values\n    }\n    result = service.spreadsheets().values().update(\n        spreadsheetId=spreadsheet_id, range=range_name,\n        valueInputOption='RAW', body=body).execute()\n    return result\n\n# Function to append data to a Google Sheet\ndef append_data(spreadsheet_id, range_name, values):\n    service = get_sheets_service()\n    body = {\n        'values': values\n    }\n    result = service.spreadsheets().values().append(\n        spreadsheetId=spreadsheet_id, range=range_name,\n        valueInputOption='RAW', body=body).execute()\n    return result\n\n# Function to delete data in a Google Sheet\ndef clear_data(spreadsheet_id, range_name):\n    service = get_sheets_service()\n    result = service.spreadsheets().values().clear(spreadsheetId=spreadsheet_id, range=range_name).execute()\n    return result\n\ndef main():\n    spreadsheet_id = 'your-spreadsheet-id'  # Replace with your Google Sheet ID\n    read_range = 'Sheet1!A1:E5'\n    write_range = 'Sheet1!A1'\n    append_range = 'Sheet1!A1'\n    clear_range = 'Sheet1!A1:E5'\n    max_cells = 5000000\n    \n    # Get current date and time\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    new_sheet_name = f\"Pi-Sensor {current_time}\"  # New sheet name with date and time\n    \n    # Check sheet size and create new sheet if necessary\n    spreadsheet_id, total_cells = check_sheet_size(spreadsheet_id, new_sheet_name)\n    print(f\"Total cells in the sheet: {total_cells}\")\n    \n    if total_cells &gt;= max_cells:\n        print(\"The sheet has reached the maximum cell limit. Creating a new sheet.\")\n        spreadsheet_id = create_new_sheet(new_sheet_name)\n    else:\n        print(\"The sheet has not reached the maximum cell limit.\")\n    \n    # Reading data\n    rows = read_data(spreadsheet_id, read_range)\n    print(\"Read data:\")\n    for row in rows:\n        print(row)\n    \n    # Writing data\n    values_to_write = [\n        ['Name', 'Age', 'City'],\n        ['John Doe', '30', 'New York'],\n        ['Jane Smith', '25', 'Los Angeles']\n    ]\n    write_result = write_data(spreadsheet_id, write_range, values_to_write)\n    print(f\"Data written: {write_result}\")\n    \n    # Appending data\n    values_to_append = [\n        ['Alice', '35', 'Chicago'],\n        ['Bob', '40', 'San Francisco']\n    ]\n    append_result = append_data(spreadsheet_id, append_range, values_to_append)\n    print(f\"Data appended: {append_result}\")\n    \n    # # Clearing data\n    # clear_result = clear_data(spreadsheet_id, clear_range)\n    # print(f\"Data cleared: {clear_result}\")\n\nif __name__ == '__main__':\n    main()\n\n\n\nCode Summary\n\nThis Python script provides functionalities to interact with Google Sheets using the Google Sheets API. It includes authentication, creating new sheets, checking sheet size, reading data, writing data, appending data, and clearing data in a Google Sheet. Here is a summary of the main components and functionality:\n\nKey Components:\n\nImports and Authentication:\n\nImports necessary libraries for Google Sheets API, Google authentication, and handling HTTP errors.\nDefines the scope required for accessing the Google Sheets API.\n\nAuthentication Function:\n\nget_sheets_service(): Authenticates the user using OAuth 2.0 and returns a service object for interacting with Google Sheets.\n\nCreate New Google Sheet:\n\ncreate_new_sheet(sheet_name): Creates a new Google Sheet with the given name and returns its spreadsheet ID.\n\nCheck Sheet Size:\n\ncheck_sheet_size(spreadsheet_id, new_sheet_name): Checks the number of cells in the specified Google Sheet and creates a new sheet if the current sheet doesn’t exist or has reached the maximum cell limit.\n\nRead Data from Google Sheet:\n\nread_data(spreadsheet_id, range_name): Reads data from the specified range in the Google Sheet and returns the rows of data.\n\nWrite Data to Google Sheet:\n\nwrite_data(spreadsheet_id, range_name, values): Writes the provided values to the specified range in the Google Sheet.\n\nAppend Data to Google Sheet:\n\nappend_data(spreadsheet_id, range_name, values): Appends the provided values to the specified range in the Google Sheet.\n\nClear Data in Google Sheet:\n\nclear_data(spreadsheet_id, range_name): Clears data in the specified range in the Google Sheet.\n\n\n\n\nMain Function:\n\nInitialization:\n\nSpecifies the Google Sheet ID (spreadsheet_id) and various ranges for reading, writing, appending, and clearing data.\nDefines the maximum number of cells allowed (max_cells).\n\nCurrent Date and Time:\n\nRetrieves the current date and time to create a unique name for new sheets.\n\nCheck and Create Sheet:\n\nChecks the size of the specified sheet and creates a new sheet if necessary.\n\nRead Data:\n\nReads data from the specified range and prints it.\n\nWrite Data:\n\nWrites a set of values to the specified range in the Google Sheet.\n\nAppend Data:\n\nAppends another set of values to the specified range in the Google Sheet.\n\nClear Data (Commented Out):\n\nOptionally clears data in the specified range.\n\n\n\n\nScript Execution:\n\nThe main() function is called when the script is executed, performing the above operations sequentially.\n\n\n\nExample Output:\n\nThe script provides feedback through print statements, indicating the creation of new sheets, reading data, writing data, appending data, and handling errors.\n\n\n\nNotes:\n\nReplace 'your-spreadsheet-id' with your actual Google Sheet ID.\nEnsure you have token.json for storing the user’s access and refresh tokens.\n\nThis script facilitates seamless interaction with Google Sheets, allowing for efficient data management and automation of tasks.\n\nOnce I was assured that the Google Sheets API was functioning correctly I tweaked the existing Python code to also send data to a Google Sheet. The sheet was set to read only globally so I could later on access it via my github.io site or similar via javascript. I also added a *.txt file for persistence across runs where that sheet contains my spreadsheet_id, workbook_name, and sheet_name so I can start and stop the Python script whenever I wanted. I also added back in the ThingSpeak code from before with two additional parameters, date and time. That way I can bypass the 15second update limit of ThingSpeak by sending bulk data every 15 seconds and I can also use the date/time parameters to generate a plot on the off chance that the datasent uses the timestamp of receipt as the X axis when plotting. The final result is the plot below:\n\nThe Matlab code used:\n\n\nCode\n\n{matlab}\n% Template MATLAB code for visualizing data from a channel as a 2D line\n% plot using PLOT function.\n\n% Prior to running this MATLAB code template, assign the channel variables.\n% Set 'readChannelID' to the channel ID of the channel to read from. \n% Also, assign the read field ID to 'fieldID1'. \n\n% TODO - Replace the [] with channel ID to read data from:\nreadChannelID = 2545447;\n% TODO - Replace the [] with the Field ID to read data from:\nfieldID1 = 1;\nfieldID2 = 2;\nfieldID3 = 3;\nfieldID4 = 4;\nfieldID5 = 5;\n% Channel Read API Key \n% If your channel is private, then enter the read API\n% Key between the '' below: \nreadAPIKey = '';\n\n%% Read Data %%\n[data, time] = thingSpeakRead(readChannelID, 'Field', fieldID1, 'NumPoints', 2400, 'ReadKey', readAPIKey);\n[data2, time] = thingSpeakRead(readChannelID, 'Field', fieldID2, 'NumPoints', 2400, 'ReadKey', readAPIKey);\n[data3, time] = thingSpeakRead(readChannelID, 'Field', fieldID3, 'NumPoints', 2400, 'ReadKey', readAPIKey);\n[data4, time] = thingSpeakRead(readChannelID, 'Field', fieldID4, 'NumPoints', 2400, 'ReadKey', readAPIKey,'OutputFormat', 'timetable');\n[data5, ~] = thingSpeakRead(readChannelID, 'Field', fieldID5, 'NumPoints', 2400, 'ReadKey', readAPIKey,'OutputFormat', 'timetable');\n%disp(\"data4\")\n%disp(data4)\n%disp(\"data5\")\n%disp(data5)\n\n% Convert timestamps to datetime\ntimeComponents = split(data4.Time, ':');\nhour = str2double(timeComponents(:,1));\nminute = str2double(timeComponents(:,2));\nsecond = str2double(timeComponents(:,3));\ntimeOfDay = duration(hour, minute, second);\ndateTime = data5.Timestamps + timeOfDay;\n\n\n% Trim or interpolate data if necessary to match the length of timeData\ndata = data(1:length(dateTime));\ndata2 = data2(1:length(dateTime));\ndata3 = data3(1:length(dateTime));\n\n% Visualize Data\nfigure;\nplot(dateTime, [data, data2, data3]);\nxlabel('');\nylabel('Data');\ntitle('ThingSpeak Data');\nlegend('Temp_C', 'Temp_F', 'Humidity%', 'Location', 'eastoutside');\n\n\n\nCode Summary\n\nThis MATLAB script visualizes data from a ThingSpeak channel as a 2D line plot. The script reads data from multiple fields of the specified ThingSpeak channel, processes the timestamps, and then plots the data. Here is a detailed summary of the script:\n\n\nKey Components:\n\nChannel Configuration:\n\nreadChannelID: The ID of the ThingSpeak channel to read data from.\nfieldID1 to fieldID5: The field IDs from which data will be read.\nreadAPIKey: The API key to access the channel (if it is private).\n\nReading Data:\n\nthingSpeakRead: Reads data from the specified channel fields, up to 2400 points.\n\nProcessing Timestamps:\n\nConverts the timestamps into a format that MATLAB can use for plotting.\nCombines the date from one field with the time from another to create a datetime array.\n\nData Trimming:\n\nEnsures that the data arrays match in length to avoid plotting issues.\n\nPlotting Data:\n\nUses the plot function to create a 2D line plot of the data.\n\nAdds labels, a title, and a legend to the plot.\n\n\n\nExplanation:\n\nChannel and Field Setup:\n\nReplace readChannelID and fieldID1 to fieldID5 with your actual ThingSpeak channel ID and field IDs.\nIf the channel is private, provide the readAPIKey.\n\nReading Data:\n\nThe thingSpeakRead function is used to read data from each specified field.\nThe ‘NumPoints’ parameter limits the data to the latest 2400 points.\n\nTimestamp Processing:\n\nExtracts and splits the time components (hours, minutes, and seconds) from data4.Time.\nCreates a duration array for the time of day and combines it with data5.Timestamps to form a datetime array.\n\nData Trimming:\n\nEnsures all data arrays (data, data2, data3) match the length of dateTime to avoid issues in plotting.\n\nPlotting:\n\nCreates a line plot of the data with the time on the x-axis and the data values on the y-axis.\nAdds labels to the x and y axes, a title to the plot, and a legend.\n\n\nThis script provides a clear and structured way to visualize data from multiple fields of a ThingSpeak channel in MATLAB. Adjust the channel ID, field IDs, and API key as needed for your specific use case.\n\nNow as far as the Python code goes, it is pretty lengthy at this point so buyer beware:\n\n\nCode\n\n{python}\nimport os\nimport time as time_module\nimport requests\nfrom datetime import datetime\nimport random\nimport json\nimport Adafruit_DHT\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\n# Control flags\nSEND_TO_VERCEL = True\nSEND_TO_GOOGLE_SHEETS = True\nSEND_TO_THINGSPEAK = True\n\n# ThingSpeak API settings\nTHINGSPEAK_API_KEY = 'KEY' # Replace with your ThingSpeak API key\nTHINGSPEAK_BASE_URL = 'https://api.thingspeak.com/update'\n\nVERCEL_API_KEY = 'SETMEUPINENVIRONMENTVARIABLES'\n# Sensor setup\nSENSOR = Adafruit_DHT.DHT11  # Using DHT11 sensor\nPIN = 4  # Change this to the GPIO pin number that the sensor is connected to\n\n# Vercel endpoint URL\nURL = 'YOURURL'\n\n# Buffer to store data\ndata_buffer = []\nthingspeak_buffer = []  # Buffer for ThingSpeak data\n\n# Define loop time in seconds\nLOOP_TIME = 1  # You can change this to the desired loop time in seconds\n\n# Define the scopes required for the Google Sheets and Drive APIs\nSCOPES = [\n    'https://www.googleapis.com/auth/spreadsheets',\n    'https://www.googleapis.com/auth/drive.file'\n]\n\n# Global variable to hold credentials\ncreds = None\n\n# Function to get authenticated Sheets service\ndef get_sheets_service():\n    global creds\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n    return build('sheets', 'v4', credentials=creds)\n\n# Function to get authenticated Drive service\ndef get_drive_service():\n    global creds\n    if creds is None:\n        get_sheets_service()  # This will initialize creds\n    return build('drive', 'v3', credentials=creds)\n\n# Function to set a Google Sheet's permissions to public\ndef set_sheet_public(spreadsheet_id):\n    drive_service = get_drive_service()\n    permission = {\n        'type': 'anyone',\n        'role': 'reader'\n    }\n    try:\n        drive_service.permissions().create(fileId=spreadsheet_id, body=permission).execute()\n        print(f\"Set spreadsheet with ID {spreadsheet_id} to public\")\n    except HttpError as error:\n        print(f\"An error occurred: {error}\")\n\n# Function to create a new Google Sheet with a given name\ndef create_new_sheet(sheet_name):\n    service = get_sheets_service()\n    spreadsheet = {\n        'properties': {\n            'title': sheet_name\n        }\n    }\n    spreadsheet = service.spreadsheets().create(body=spreadsheet, fields='spreadsheetId').execute()\n    spreadsheet_id = spreadsheet.get('spreadsheetId')\n    print(f\"Created new spreadsheet with ID: {spreadsheet_id}, Name: {sheet_name}\")\n\n    # Set the new sheet to be publicly viewable\n    set_sheet_public(spreadsheet_id)\n\n    # Save the new spreadsheet info\n    save_spreadsheet_info(spreadsheet_id, sheet_name, \"Sheet1\")\n\n    return spreadsheet_id\n\n# Function to check the number of cells in the Google Sheet and create a new one if it doesn't exist\ndef check_sheet_size(spreadsheet_id, new_sheet_name):\n    service = get_sheets_service()\n    try:\n        sheet = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n        sheets = sheet.get('sheets', [])\n    except HttpError as e:\n        print(f\"HttpError encountered: Status code: {e.resp.status}, Reason: {e.error_details}\")\n        if e.resp.status == 404:\n            print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Creating a new sheet.\")\n            return create_new_sheet(new_sheet_name), 0\n        else:\n            raise\n\n    total_cells = 0\n    for sheet in sheets:\n        properties = sheet.get('properties', {})\n        grid_properties = properties.get('gridProperties', {})\n        rows = grid_properties.get('rowCount', 0)\n        cols = grid_properties.get('columnCount', 0)\n        total_cells += rows * cols\n\n    return spreadsheet_id, total_cells\n\n# Function to find the last empty row in the Google Sheet\ndef find_last_empty_row(service, spreadsheet_id, sheet_name):\n    max_retries = 5\n    retry_count = 0\n    backoff_factor = 2\n\n    range_name = f\"{sheet_name}!A:A\"\n\n    while retry_count &lt; max_retries:\n        try:\n            result = service.spreadsheets().values().get(\n                spreadsheetId=spreadsheet_id,\n                range=range_name\n            ).execute()\n\n            values = result.get('values', [])\n            print(f\"Debug - Number of rows in column A: {len(values)}\")\n            return len(values) + 1\n        except HttpError as e:\n            if e.resp.status in [500, 503]:\n                retry_count += 1\n                sleep_time = backoff_factor ** retry_count + random.uniform(0, 1)\n                print(f\"HttpError {e.resp.status} encountered. Retrying in {sleep_time:.1f} seconds...\")\n                time.sleep(sleep_time)\n            else:\n                raise\n    raise Exception(\"Failed to retrieve last empty row after several retries\")\n\n# Function to append data to a Google Sheet\ndef append_data_to_sheet(spreadsheet_id, sheet_name, values):\n    if SEND_TO_GOOGLE_SHEETS:\n        service = get_sheets_service()\n        last_empty_row = find_last_empty_row(service, spreadsheet_id, sheet_name)\n        range_name = f\"{sheet_name}!A{last_empty_row}\"\n        body = {\n            'values': values\n        }\n        result = service.spreadsheets().values().append(\n            spreadsheetId=spreadsheet_id, range=range_name,\n            valueInputOption='RAW', body=body).execute()\n        return result\n\n# Function to send data to the server\ndef send_data(data):\n    \"\"\"Send data to the server.\"\"\"\n    if SEND_TO_VERCEL:\n        try:\n            # Include API key in the headers\n            headers = {'x-api-key': VERCEL_API_KEY}\n            response = requests.post(URL, json=data, headers=headers)\n            response.raise_for_status()\n            print(data)\n            print(headers)\n            print(f\"Data sent to server: {response.text}\")\n        except requests.exceptions.RequestException as e:\n            print(f\"Failed to send data to server: {e}\")\n        except Exception as e:\n            print(f\"An unexpected error occurred: {e}\")\n# Function to send data to ThingSpeak\ndef send_data_to_thingspeak():\n    \"\"\"Send data to ThingSpeak.\"\"\"\n    if SEND_TO_THINGSPEAK and thingspeak_buffer:\n        data = thingspeak_buffer.pop(0)  # Get the first item in the buffer\n        payload = {\n            'api_key': THINGSPEAK_API_KEY,\n            'field1': data['temperature_C'],\n            'field2': data['temperature_F'],\n            'field3': data['humidityPercent'],\n            'field4': data['time'],\n            'field5': data['date']\n        }\n        try:\n            response = requests.get(THINGSPEAK_BASE_URL, params=payload)\n            print('Data posted to ThingSpeak', response.text)\n        except requests.exceptions.RequestException as e:\n            print('Failed to send data to ThingSpeak:', e)\n\n# Function to save spreadsheet info to a file\ndef save_spreadsheet_info(spreadsheet_id, workbook_name, sheet_name):\n    info = {\n        'spreadsheet_id': spreadsheet_id,\n        'workbook_name': workbook_name,\n        'sheet_name': sheet_name\n    }\n    with open('spreadsheet_info.txt', 'w') as f:\n        json.dump(info, f)\n\n# Function to load spreadsheet info from a file\ndef load_spreadsheet_info():\n    if os.path.exists('spreadsheet_info.txt'):\n        with open('spreadsheet_info.txt', 'r') as f:\n            info = json.load(f)\n            return info['spreadsheet_id'], info['workbook_name'], info['sheet_name']\n    return None, None, None\n\ndef main():\n    # Load spreadsheet info from file\n    spreadsheet_id, workbook_name, sheet_name = load_spreadsheet_info()\n\n    if spreadsheet_id is None or workbook_name is None or sheet_name is None:\n        # Initialize new sheet if no info found\n        workbook_name = 'your-workbook-name'  # Replace with your Google Sheet ID\n        spreadsheet_id = 'your-spreadsheet-id'  # Replace with your Google Sheet ID\n        sheet_name = 'Sheet1'  # Adjust the sheet name as needed\n\n    max_cells = 5000000\n\n    # Initialize loop counter\n    i = 1\n\n    # Get current date and time for the new sheet name if needed\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    new_sheet_name = f\"Pi-Sensor {current_time}\"  # New sheet name with date and time\n    \n    # Check sheet size and create new sheet if necessary\n    if SEND_TO_GOOGLE_SHEETS:\n        spreadsheet_id, total_cells = check_sheet_size(spreadsheet_id, new_sheet_name)\n        print(f\"Total cells in the sheet: {total_cells}\")\n        \n        if total_cells &gt;= max_cells:\n            print(\"The sheet has reached the maximum cell limit. Creating a new sheet.\")\n            spreadsheet_id = create_new_sheet(new_sheet_name)\n            sheet_name = 'Sheet1'  # Reset to default sheet name for new spreadsheet\n        else:\n            print(\"The sheet has not reached the maximum cell limit.\")\n\n    # Time tracking for ThingSpeak\n    last_thingspeak_update = time_module.time()\n    \n    while True:\n        # Get the current time at the start of the loop\n        start_time = time_module.time()\n        \n        # Read humidity and temperature from DHT sensor\n        humidityPercent, temperature_C = Adafruit_DHT.read_retry(SENSOR, PIN)\n        \n        if humidityPercent is not None and temperature_C is not None:\n            # Prepare the data payload\n            temperature_F = temperature_C * 9.0 / 5.0 + 32\n            \n            now = datetime.now()\n            date = now.strftime(\"%m-%d-%Y\")\n            timeNow = now.strftime(\"%H:%M:%S\")\n            \n            data = {\n                'date': date,\n                'time': timeNow,\n                'humidityPercent': humidityPercent,\n                'temperatureFahrenheit': temperature_F,\n                'temperatureCelsius': temperature_C\n            }\n            \n            # Log data to buffer\n            data_buffer.append(data)\n            if i % 60 == 0:\n                print(f\"Logged data: {data}\")\n            \n            # Append data to Google Sheet\n            append_data_to_sheet(spreadsheet_id, sheet_name, [[date, timeNow, humidityPercent, temperature_F, temperature_C]])\n            \n            # Add data to ThingSpeak buffer\n            thingspeak_buffer.append({\n                'temperature_C': temperature_C,\n                'temperature_F': temperature_F,\n                'humidityPercent': humidityPercent,\n                'time': timeNow,\n                'date': date\n            })\n            \n            # Check if 300 readings have been logged\n            if len(data_buffer) &gt;= 300:\n                send_data(data_buffer)\n                \n                # Clear the buffer after sending\n                data_buffer.clear()\n                i = 0\n\n            # Check if it's time to send data to ThingSpeak\n            if time_module.time() - last_thingspeak_update &gt;= 15:\n                send_data_to_thingspeak()\n                last_thingspeak_update = time_module.time()\n        else:\n            # Handle cases where sensor fails to read\n            print(\"Failed to retrieve data from sensor\")\n        \n        # Wait until the LOOP_TIME has elapsed\n        while time_module.time() - start_time &lt; LOOP_TIME:\n            time_module.sleep(0.01)  # Sleep a short time to avoid busy-waiting\n\n        i += 1\n\nif __name__ == '__main__':\n    main()\n\n\n\nCode Summary\n\nThis Python script reads environmental data from a DHT11 sensor, processes the data, and sends it to three different endpoints: Vercel, Google Sheets, and ThingSpeak. The script includes functionalities for authentication, data reading, data logging, and sending data to different services. Here is a summary of the main components and functionality:\n\n\nKey Components:\n\nImports and Setup:\n\nImports necessary libraries for reading sensor data (Adafruit_DHT), making HTTP requests (requests), handling time (time, datetime), and interacting with Google APIs (google.auth, google.oauth2, googleapiclient).\nDefines control flags to enable/disable sending data to Vercel, Google Sheets, and ThingSpeak.\n\nAPI Key and Endpoint Configuration:\n\nTHINGSPEAK_API_KEY and THINGSPEAK_BASE_URL for ThingSpeak.\nVERCEL_API_KEY and URL for Vercel.\nGoogle Sheets and Drive API scopes are defined.\n\nSensor Setup:\n\nDefines the DHT11 sensor and the GPIO pin to which it is connected.\n\nData Buffers:\n\nInitializes buffers for storing data before sending it to Vercel and ThingSpeak.\n\nGoogle Sheets Authentication:\n\nget_sheets_service(): Authenticates and returns the Google Sheets service object.\nget_drive_service(): Authenticates and returns the Google Drive service object.\n\nGoogle Sheets Management:\n\nset_sheet_public(spreadsheet_id): Sets the permissions of a Google Sheet to public.\ncreate_new_sheet(sheet_name): Creates a new Google Sheet with the specified name.\ncheck_sheet_size(spreadsheet_id, new_sheet_name): Checks the size of a Google Sheet and creates a new one if necessary.\nfind_last_empty_row(service, spreadsheet_id, sheet_name): Finds the last empty row in a Google Sheet.\nappend_data_to_sheet(spreadsheet_id, sheet_name, values): Appends data to a Google Sheet.\nsave_spreadsheet_info(spreadsheet_id, workbook_name, sheet_name): Saves Google Sheet info to a file.\nload_spreadsheet_info(): Loads Google Sheet info from a file.\n\nSending Data:\n\nsend_data(data): Sends data to the Vercel endpoint.\nsend_data_to_thingspeak(): Sends data to the ThingSpeak channel.\n\nMain Function:\n\nLoads spreadsheet info or initializes a new sheet.\nChecks the size of the Google Sheet and creates a new one if it has reached its cell limit.\nEnters an infinite loop to read data from the DHT11 sensor every second.\nPrepares and logs data to buffers.\nAppends data to Google Sheets.\nSends buffered data to Vercel every 300 readings.\nSends data to ThingSpeak every 15 seconds.\nIncludes error handling for sensor read failures and HTTP request failures.\n\n\n\n\nExample Output:\n\nThe script provides feedback through print statements, indicating the creation of new sheets, reading data, writing data, appending data, and handling errors.\n\n\n\nNotes:\n\nReplace YOUR_API_KEY with your actual ThingSpeak API key.\nReplace YOURURL with your actual Vercel endpoint URL.\nReplace your-workbook-name and your-spreadsheet-id with your actual Google Sheet name and ID.\nEnsure you have credentials.json for Google API OAuth 2.0 and token.json for storing the user’s access and refresh tokens.\n\nThis script facilitates continuous monitoring and logging of environmental data from a DHT11 sensor, with automated data management and reporting using Vercel, Google Sheets, and ThingSpeak.\n\nThat’s 3 services down, with one more crack at MongoDB. The tentative plan is to push everything from the Python script then use some form of authentication to eventually be able to grab the data directly from MongoDB but use the Vercel site to “host” the csv/data. A rough sketch of the setup is below:\n\nThis was fairly straightforward and with one function and a couple of lines of Python I was all set up. Note that I changed the timings, notably Vercel. As the server spins up and stays active for 5 minutes after it receives data. The limit for Compute time is 60 hrs/month, so at the basic vCPU of 0.6 I will be at 0.6*(5minutes/60minutes)*24hour*30days=36 hours if I update it hourly. This is just safe enough for me to do and as a bonus its not like this is an AWS instance with my Credit Card linked.\nTimings:\n\n\n\n\n\n\n\n\nDatabase\nUpdate Interval\nCapacity\n\n\nGoogle Sheets\n1 second(with network delay more like 2)\n2 million entries(10 million cells)\n\n\nThingSpeak\n15 seconds(bulk update works around this)\n2 million entries(this is suspicious…)\n\n\nMongoDB\n60 seconds\n512mb\n\n\nVercel PostgreSQL\n3600 seconds\n256mb\n\n\n\n\n\nCode\n\nimport os\nimport time as time_module\nimport requests\nfrom datetime import datetime\nimport random\nimport json\nimport subprocess\nimport Adafruit_DHT\nfrom pymongo import MongoClient\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\n# Control flags\nSEND_TO_VERCEL = True\nSEND_TO_GOOGLE_SHEETS = True\nSEND_TO_THINGSPEAK = True\nSEND_TO_MONGODB = True\n\n# ThingSpeak API settings\nTHINGSPEAK_API_KEY = 'Your-API-Key'# Replace with your ThingSpeak API key\n\nTHINGSPEAK_BASE_URL = 'https://api.thingspeak.com/update'\nTHINGSPEAK_CHANNEL_ID = 000000 'Replace with your channel ID\nTHINGSPEAK_BULK_UPDATE_URL = 'https://api.thingspeak.com/channels/'+str(THINGSPEAK_CHANNEL_ID)+'/bulk_update.json'\nprint(THINGSPEAK_BULK_UPDATE_URL)\n\nVERCEL_API_KEY = 'Your-Vercel-ID'\n\nMONGODB_URI = 'mongodb+srv://&lt;user&gt;:&lt;password&gt;@&lt;cluster-number&gt;.&lt;specialURL&gt;.mongodb.net'\nMONGODB_DB_NAME = 'Raspberry_Pi' #Your DB Name\nMONGODB_COLLECTION_NAME = 'Readings'#Your Collection Name\n# Sensor setup\nSENSOR = Adafruit_DHT.DHT11  # Using DHT11 sensor\nPIN = 4  # Change this to the GPIO pin number that the sensor is connected to\n\n# Vercel endpoint URL\nURL = 'https://your-vercel-url.vercel.app/api/sensor'\n\n# Buffer to store data\ndata_buffer_vercel = []\ndata_buffer_mongodb = []\nthingspeak_buffer = []  # Buffer for ThingSpeak data\n\n# Define loop time in seconds\nLOOP_TIME = 1  # You can change this to the desired loop time in seconds\n\n# Define the scopes required for the Google Sheets and Drive APIs\nSCOPES = [\n    'https://www.googleapis.com/auth/spreadsheets',\n    'https://www.googleapis.com/auth/drive.file'\n]\n\n# Global variable to hold credentials\ncreds = None\n\n# Function to get authenticated Sheets service\ndef get_sheets_service():\n    global creds\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n    return build('sheets', 'v4', credentials=creds)\n\n# Function to get authenticated Drive service\ndef get_drive_service():\n    global creds\n    if creds is None:\n        get_sheets_service()  # This will initialize creds\n    return build('drive', 'v3', credentials=creds)\n\n# Function to set a Google Sheet's permissions to public\ndef set_sheet_public(spreadsheet_id):\n    drive_service = get_drive_service()\n    permission = {\n        'type': 'anyone',\n        'role': 'reader'\n    }\n    try:\n        drive_service.permissions().create(fileId=spreadsheet_id, body=permission).execute()\n        print(f\"Set spreadsheet with ID {spreadsheet_id} to public\")\n    except HttpError as error:\n        print(f\"An error occurred: {error}\")\n\n# Function to create a new Google Sheet with a given name\ndef create_new_sheet(sheet_name):\n    service = get_sheets_service()\n    spreadsheet = {\n        'properties': {\n            'title': sheet_name\n        }\n    }\n    spreadsheet = service.spreadsheets().create(body=spreadsheet, fields='spreadsheetId').execute()\n    spreadsheet_id = spreadsheet.get('spreadsheetId')\n    print(f\"Created new spreadsheet with ID: {spreadsheet_id}, Name: {sheet_name}\")\n\n    # Set the new sheet to be publicly viewable\n    set_sheet_public(spreadsheet_id)\n\n    # Save the new spreadsheet info\n    save_spreadsheet_info(spreadsheet_id, sheet_name, \"Sheet1\")\n\n    return spreadsheet_id\n\n# Function to check the number of cells in the Google Sheet and create a new one if it doesn't exist\ndef check_sheet_size(spreadsheet_id, new_sheet_name):\n    service = get_sheets_service()\n    try:\n        sheet = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n        sheets = sheet.get('sheets', [])\n    except HttpError as e:\n        print(f\"HttpError encountered: Status code: {e.resp.status}, Reason: {e.error_details}\")\n        if e.resp.status == 404:\n            print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Creating a new sheet.\")\n            return create_new_sheet(new_sheet_name), 0\n        else:\n            raise\n\n    total_cells = 0\n    for sheet in sheets:\n        properties = sheet.get('properties', {})\n        grid_properties = properties.get('gridProperties', {})\n        rows = grid_properties.get('rowCount', 0)\n        cols = grid_properties.get('columnCount', 0)\n        total_cells += rows * cols\n\n    return spreadsheet_id, total_cells\n\n# Function to find the last empty row in the Google Sheet\ndef find_last_empty_row(service, spreadsheet_id, sheet_name,i):\n    max_retries = 5\n    retry_count = 0\n    backoff_factor = 2\n\n    range_name = f\"{sheet_name}!A:A\"\n\n    while retry_count &lt; max_retries:\n        try:\n            result = service.spreadsheets().values().get(\n                spreadsheetId=spreadsheet_id,\n                range=range_name\n            ).execute()\n\n            values = result.get('values', [])\n            if i%15==0:\n                print(f\"Debug Google - Number of rows in column A: {len(values)}\")\n            return len(values) + 1\n        except HttpError as e:\n            if e.resp.status in [500, 503]:\n                retry_count += 1\n                sleep_time = backoff_factor ** retry_count + random.uniform(0, 1)\n                print(f\"HttpError {e.resp.status} encountered. Retrying in {sleep_time:.1f} seconds...\")\n                time.sleep(sleep_time)\n            else:\n                raise\n    raise Exception(\"Failed to retrieve last empty row after several retries\")\n\n# Function to append data to a Google Sheet\ndef append_data_to_sheet(spreadsheet_id, sheet_name, values,i):\n    if SEND_TO_GOOGLE_SHEETS:\n        service = get_sheets_service()\n        last_empty_row = find_last_empty_row(service, spreadsheet_id, sheet_name,i)\n        range_name = f\"{sheet_name}!A{last_empty_row}\"\n        body = {\n            'values': values\n        }\n        result = service.spreadsheets().values().append(\n            spreadsheetId=spreadsheet_id, range=range_name,\n            valueInputOption='RAW', body=body).execute()\n        return result\n\n# Function to send data to the server\ndef send_data_vercel(data):\n    \"\"\"Send data to the server.\"\"\"\n    if SEND_TO_VERCEL:\n        try:\n            # Include API key in the headers\n            headers = {'x-api-key': VERCEL_API_KEY}\n            print(data)\n            print(headers)\n            response = requests.post(URL, json=data, headers=headers)\n            response.raise_for_status()\n            #print(data)\n            #print(headers)\n            print(f\"Data sent to Vercel server: {response.text}\")\n        except requests.exceptions.RequestException as e:\n            print(f\"Failed to send data to server: {e}\")\n        except Exception as e:\n            print(f\"An unexpected error occurred: {e}\")\n# Function to send data to ThingSpeak\ndef thingspeak_update():\n    \"\"\"Send data to ThingSpeak.\"\"\"\n    if SEND_TO_THINGSPEAK and thingspeak_buffer:\n        if len(thingspeak_buffer) &gt; 1:\n            # Bulk update\n            payload = {\n                'write_api_key': THINGSPEAK_API_KEY,\n                'updates': []\n            }\n            #print(THINGSPEAK_API_KEY)\n            # Convert buffer data to the format required by ThingSpeak\n            \n            for data in thingspeak_buffer:\n                update = {\n                    'created_at': f\"{data['date']} {data['time']} -0500\",\n                    'field1': data['temperature_C'],\n                    'field2': data['temperature_F'],\n                    'field3': data['humidityPercent'],\n                    'field4': data['time'],\n                    'field5': data['date']\n                }\n                payload['updates'].append(update)\n\n            try:\n                # Send the bulk update request to ThingSpeak\n                headers = {'Content-Type': 'application/json'}\n                #print(len(thingspeak_buffer))\n                #print(headers)\n                #print(json.dumps(payload))\n                # Convert the data payload to JSON format\n                json_data = json.dumps(payload)\n                response = requests.post(THINGSPEAK_BULK_UPDATE_URL,headers=headers,data=json_data)\n                if response.status_code == 202:\n                    print('Data posted to ThingSpeak (bulk update):', response.text)\n                    thingspeak_buffer.clear()  # Clear the buffer after successful update\n                else:\n                    print(f'Failed to send data to ThingSpeak (bulk update): {response.status_code}, {response.text}')\n            except requests.exceptions.RequestException as e:\n                print('Failed to send data to ThingSpeak (bulk update):', e)\n        else:\n            # Simple update\n            data = thingspeak_buffer[0]\n            payload = {\n                'api_key': THINGSPEAK_API_KEY,\n                'field1': data['temperature_C'],\n                'field2': data['temperature_F'],\n                'field3': data['humidityPercent'],\n                'field4': data['time'],\n                'field5': data['date'],\n                'created_at': f\"{data['date']}T{data['time']}Z\"\n            }\n\n            try:\n                # Send the simple update request to ThingSpeak\n                headers = {\n                    'User-Agent': 'mw.doc.simple-update (Raspberry Pi)',\n                    'Content-Type': 'application/x-www-form-urlencoded'\n                }\n                response = requests.post(THINGSPEAK_BASE_URL, headers=headers, params=payload)\n                if response.status_code == 200:\n                    print('Data posted to ThingSpeak (simple update):', response.text)\n                    thingspeak_buffer.clear()  # Clear the buffer after successful update\n                else:\n                    print(f'Failed to send data to ThingSpeak (simple update): {response.status_code}, {response.text}')\n            except requests.exceptions.RequestException as e:\n                print('Failed to send data to ThingSpeak (simple update):', e)\n\n# Function to send data to MongoDB\ndef send_data_mongodb(data_mongo):\n    \"\"\"Send data to MongoDB.\"\"\"\n    if SEND_TO_MONGODB:\n        try:\n            client = MongoClient(MONGODB_URI)\n            db = client[MONGODB_DB_NAME]\n            collection = db[MONGODB_COLLECTION_NAME]\n            result = collection.insert_many(data_mongo)\n            print(f\"Data sent to MongoDB: {result.inserted_ids}\")\n        except Exception as e:\n            print(f\"Failed to send data to MongoDB: {e}\")\n# Function to save spreadsheet info to a file\ndef save_spreadsheet_info(spreadsheet_id, workbook_name, sheet_name):\n    info = {\n        'spreadsheet_id': spreadsheet_id,\n        'workbook_name': workbook_name,\n        'sheet_name': sheet_name\n    }\n    with open('spreadsheet_info.txt', 'w') as f:\n        json.dump(info, f)\n\n# Function to load spreadsheet info from a file\ndef load_spreadsheet_info():\n    if os.path.exists('spreadsheet_info.txt'):\n        with open('spreadsheet_info.txt', 'r') as f:\n            info = json.load(f)\n            return info['spreadsheet_id'], info['workbook_name'], info['sheet_name']\n    return None, None, None\n\ndef main():\n    # Load spreadsheet info from file\n    spreadsheet_id, workbook_name, sheet_name = load_spreadsheet_info()\n\n    if spreadsheet_id is None or workbook_name is None or sheet_name is None:\n        # Initialize new sheet if no info found\n        workbook_name = 'your-workbook-name'  # Replace with your Google Sheet ID\n        spreadsheet_id = 'your-spreadsheet-id'  # Replace with your Google Sheet ID\n        sheet_name = 'Sheet1'  # Adjust the sheet name as needed\n\n    max_cells = 5000000\n\n    # Initialize loop counter\n    i = 1\n\n    # Get current date and time for the new sheet name if needed\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    new_sheet_name = f\"Pi-Sensor {current_time}\"  # New sheet name with date and time\n    \n    # Check sheet size and create new sheet if necessary\n    if SEND_TO_GOOGLE_SHEETS:\n        spreadsheet_id, total_cells = check_sheet_size(spreadsheet_id, new_sheet_name)\n        print(f\"Total cells in the sheet: {total_cells}\")\n        \n        if total_cells &gt;= max_cells:\n            print(\"The sheet has reached the maximum cell limit. Creating a new sheet.\")\n            spreadsheet_id = create_new_sheet(new_sheet_name)\n            sheet_name = 'Sheet1'  # Reset to default sheet name for new spreadsheet\n        else:\n            print(\"The sheet has not reached the maximum cell limit.\")\n\n    # Time tracking for ThingSpeak\n    last_thingspeak_update = time_module.time()\n    last_mongodb_update = time_module.time()\n    last_vercel_update = time_module.time()\n    while True:\n        # Get the current time at the start of the loop\n        start_time = time_module.time()\n        \n        # Read humidity and temperature from DHT sensor\n        humidityPercent, temperature_C = Adafruit_DHT.read_retry(SENSOR, PIN)\n        \n        if humidityPercent is not None and temperature_C is not None:\n            # Prepare the data payload\n            temperature_F = temperature_C * 9.0 / 5.0 + 32\n            \n            now = datetime.now()\n            date = now.strftime(\"%m-%d-%Y\")\n            timeNow = now.strftime(\"%H:%M:%S\")\n            \n            dataVercel = {\n                'date': date,\n                'time': timeNow,\n                'humidityPercent': humidityPercent,\n                'temperatureFahrenheit': temperature_F,\n                'temperatureCelsius': temperature_C\n            }\n            dataMongo = {\n                'date': date,\n                'time': timeNow,\n                'humidityPercent': humidityPercent,\n                'temperatureFahrenheit': temperature_F,\n                'temperatureCelsius': temperature_C\n            }\n            # Log data to buffer. had to separate data as sometimes ObjectID would somehow get passed to Vercel's buffer.\n            data_buffer_vercel.append(dataVercel)\n            data_buffer_mongodb.append(dataMongo)\n            if i % 60 == 0:\n                print(f\"Logged data: {dataVercel}\")\n            \n            # Append data to Google Sheet\n            append_data_to_sheet(spreadsheet_id, sheet_name, [[date, timeNow, humidityPercent, temperature_F, temperature_C]],i)\n            \n            # Add data to ThingSpeak buffer\n            thingspeak_buffer.append({\n                'temperature_C': temperature_C,\n                'temperature_F': temperature_F,\n                'humidityPercent': humidityPercent,\n                'time': timeNow,\n                'date': datetime.strptime(date, \"%m-%d-%Y\").strftime(\"%Y-%m-%d\")\n            })\n            \n            # Check if 300 readings have been logged\n            if time_module.time() - last_vercel_update &gt;= 3600:\n                send_data_vercel(data_buffer_vercel)\n                # Clear the buffer after sending\n                data_buffer_vercel.clear()\n                last_vercel_update = time_module.time()\n            # Check if it's time to send data to Mongodb\n            if time_module.time() - last_mongodb_update &gt;= 60:\n                send_data_mongodb(data_buffer_mongodb)\n                data_buffer_mongodb.clear()\n                last_mongodb_update = time_module.time()\n                \n            # Check if it's time to send data to ThingSpeak\n            if time_module.time() - last_thingspeak_update &gt;= 15:\n                thingspeak_update()\n                last_thingspeak_update = time_module.time()\n        else:\n            # Handle cases where sensor fails to read\n            print(\"Failed to retrieve data from sensor\")\n        \n        # Wait until the LOOP_TIME has elapsed\n        while time_module.time() - start_time &lt; LOOP_TIME:\n            time_module.sleep(0.01)  # Sleep a short time to avoid busy-waiting\n\n        i += 1\n\nif __name__ == '__main__':\n    main()\n\n\n\nCode Summary\n\nThis Python script reads environmental data from a DHT11 sensor and sends it to multiple endpoints: Vercel, Google Sheets, ThingSpeak, and MongoDB. The script also includes functionality for managing Google Sheets, such as creating new sheets and appending data to them. Below is a detailed summary of the main components and their functionality:\n\n\nKey Components:\n\nImports and Setup:\n\nImports necessary libraries for reading sensor data, making HTTP requests, handling time, and interacting with Google APIs and MongoDB.\nDefines control flags to enable/disable sending data to Vercel, Google Sheets, ThingSpeak, and MongoDB.\n\nAPI Key and Endpoint Configuration:\n\nThingSpeak API settings including API key, base URL, channel ID, and bulk update URL.\nVercel API key.\nMongoDB URI, database name, and collection name.\n\nSensor Setup:\n\nDefines the DHT11 sensor and the GPIO pin to which it is connected.\n\nData Buffers:\n\nInitializes buffers for storing data before sending it to Vercel, MongoDB, and ThingSpeak.\n\nGoogle Sheets Authentication:\n\nget_sheets_service(): Authenticates and returns the Google Sheets service object.\nget_drive_service(): Authenticates and returns the Google Drive service object.\n\nGoogle Sheets Management:\n\nset_sheet_public(spreadsheet_id): Sets the permissions of a Google Sheet to public.\ncreate_new_sheet(sheet_name): Creates a new Google Sheet with the specified name.\ncheck_sheet_size(spreadsheet_id, new_sheet_name): Checks the size of a Google Sheet and creates a new one if necessary.\nfind_last_empty_row(service, spreadsheet_id, sheet_name, i): Finds the last empty row in a Google Sheet.\nappend_data_to_sheet(spreadsheet_id, sheet_name, values, i): Appends data to a Google Sheet.\nsave_spreadsheet_info(spreadsheet_id, workbook_name, sheet_name): Saves Google Sheet info to a file.\nload_spreadsheet_info(): Loads Google Sheet info from a file.\n\nSending Data:\n\nsend_data_vercel(data): Sends data to the Vercel endpoint.\nthingspeak_update(): Sends data to the ThingSpeak channel, either as a bulk update or a simple update.\nsend_data_mongodb(data_mongo): Sends data to MongoDB.\n\nMain Function:\n\nLoads spreadsheet info or initializes a new sheet.\nChecks the size of the Google Sheet and creates a new one if it has reached its cell limit.\nEnters an infinite loop to read data from the DHT11 sensor every second.\nPrepares and logs data to buffers.\nAppends data to Google Sheets.\nSends buffered data to Vercel every hour.\nSends data to MongoDB every 60 seconds.\nSends data to ThingSpeak every 15 seconds.\nIncludes error handling for sensor read failures and HTTP request failures.\n\n\n\n\nExample Output:\n\nThe script provides feedback through print statements, indicating the creation of new sheets, reading data, writing data, appending data, and handling errors.\n\n\n\nNotes:\n\nReplace 'Your-API-Key', 'Your-Vercel-ID', and 'mongodb+srv://&lt;user&gt;:&lt;password&gt;@&lt;cluster-number&gt;.&lt;specialURL&gt;.mongodb.net' with your actual API keys and URIs.\nReplace your-workbook-name and your-spreadsheet-id with your actual Google Sheet name and ID.\nEnsure you have credentials.json and token.json files for Google API OAuth 2.0 and token management.\n\nThis script facilitates continuous monitoring and logging of environmental data from a DHT11 sensor, with automated data management and reporting using Vercel, Google Sheets, ThingSpeak, and MongoDB.\n\nCurrently, sending to all 4 servers, I experience a max “drop” of the sensor data of 3 seconds and that is running a .py file with a desktop running in the background.\nTo set up the .py script to start up at boot I did the following:\nsudo nano /etc/systemd/system/sensor_data.service\nAnd within that file:\n[Unit]\nDescription=Sensor Data Collection Service\nAfter=network.target\n\n[Service]\nExecStart=/usr/local/bin/python3.12 /home/pi/Desktop/TempHumidityVercelGoogleMongoThinkSpeak.py\nWorkingDirectory=/home/pi/Desktop\nStandardOutput=append:/home/pi/sensor_data.log\nStandardError=append:/home/pi/sensor_data.log\nRestart=always\nUser=pi\n\n[Install]\nWantedBy=multi-user.target\nThis sets it up to run at boot and also to log the output.\nEnable and start the service:\nsudo systemctl enable sensor_data.service\nsudo systemctl start sensor_data.service\nCheck the status:\nsudo systemctl status sensor_data.service\nI made a separate service file to make sure I can see the log by firing up my pi and leaving it be:\nsudo apt-get install logrotate\nsudo nano /etc/logrotate.d/sensor_data\nAnd within that file:\n/home/pi/sensor_data.log {\n    daily\n    missingok\n    rotate 7\n    compress\n    delaycompress\n    notifempty\n    create 0640 pi pi\n    postrotate\n        systemctl restart sensor_data.service &gt; /dev/null\n    endscript\n}\nThis will rotate the log file daily, keep 7 rotations, compress the old logs, and restart the service after every log rotation.\nNext set up a script that checks for specific keywords including errors and sends a notification:\nnano /home/pi/check_logs.sh\n#!/bin/bash\n\nLOG_FILE=\"/home/pi/sensor_data.log\"\nERROR_KEYWORDS=(\"ERROR\" \"FAIL\" \"exception\")\n\nfor keyword in \"${ERROR_KEYWORDS[@]}\"; do\n    if grep -q \"$keyword\" \"$LOG_FILE\"; then\n        echo \"Error detected in sensor_data service logs:\"\n        grep \"$keyword\" \"$LOG_FILE\"\n        break\n    fi\ndone\nMake it executable:\nchmod +x /home/pi/check_logs.sh\nAccess Cron to schedule jobs:\ncrontab -e\nSet the job to run * * * * *, or every minute of every hour of every day of every month.\n* * * * * /home/pi/check_logs.sh\nCheck that it worked:\ncrontab -l\nFrom here I also wanted to display the output at boot as well. This is optional, but I wanna see a running log of what’s going on:\nsudo nano /etc/systemd/system/monitor_sensor_logs.service\n[Unit]\nDescription=Monitor Sensor Data Service Logs\nAfter=sensor_data.service\nRequires=sensor_data.service\n\n[Service]\nExecStart=/bin/bash -c 'journalctl -u sensor_data.service -f'\nStandardOutput=inherit\nStandardError=inherit\nRestart=always\nUser=pi\n\n[Install]\nWantedBy=multi-user.target\nEnable on boot:\nsudo systemctl enable monitor_sensor_logs.service\nDisabling the desktop[Boot into Command Line Interface]:\nsudo raspi-config\n\n\n\nFinally reboot and you’re done!\nsudo reboot\nAfter setting up the python script to run at bootup and disabling the desktop I experience a drop of “1-2” seconds with a larger amount of 1 second drops than 2.\nI must have made some sort of mistake in getting the logging to work, but the data is being sent and that’s good enough for now.\nRunning this command after the pi boots up works well enough:\ntail -f /home/pi/sensor_data.log\nAnd don’t forget that if you want to connect via SSH(if you didn’t already)”\nssh pi@&lt;IP_ADDRESS_OF_YOUR_PI&gt;\nIf you want to get the desktop back simply type into the Pi console:\nstartx\nNow for the results:\nTo get the Google Script working I deployed a Google Aps Script:\nGo to script.google.com and create a new project\nWrite a function to fetch data from my Google Sheets:\nfunction doGet(e) {\n  try {\n    // Open the spreadsheet by ID and get the specified sheet\n    const sheet = SpreadsheetApp.openById('1n4iZYfgdhv08PeREqz26F4-eQYfnXdHaSQywnQ8UYWk').getSheetByName('Sheet1');\n    Logger.log('Sheet accessed successfully');\n    \n    // Get the last row number\n    const lastRow = sheet.getLastRow();\n    Logger.log('Last row number: ' + lastRow);\n    \n    // Get the values of the last row\n    const lastRowData = sheet.getRange(lastRow, 1, 1, sheet.getLastColumn()).getValues()[0];\n    Logger.log('Last row data: ' + JSON.stringify(lastRowData));\n    \n    // Get the headers from the first row\n    const headers = sheet.getRange(1, 1, 1, sheet.getLastColumn()).getValues()[0];\n    Logger.log('Headers: ' + JSON.stringify(headers));\n    \n    // Create an object to store the last row data\n    const result = {};\n    \n    // Populate the result object with the headers as keys and last row data as values\n    headers.forEach((header, index) =&gt; {\n      result[header] = lastRowData[index];\n    });\n    Logger.log('Result object: ' + JSON.stringify(result));\n    \n    // Convert the result object to JSON\n    const json = JSON.stringify(result);\n    \n    // Set CORS headers\n    const output = ContentService.createTextOutput(json).setMimeType(ContentService.MimeType.JSON);\n    \n    return output;\n  } catch (error) {\n    Logger.log('Error: ' + error.message);\n    return ContentService.createTextOutput(JSON.stringify({ error: error.message })).setMimeType(ContentService.MimeType.JSON);\n  }\n}\n\nDeploy the script as a web app:\n\nClick on the “Deploy” button.\nChoose “Manage Deployments”.\nClick “New Deployment”.\nSelect “Web app”.\nSet “Execute the app as” to “Me” and “Who has access” to “Anyone”.\n\n\nAnd then finally use the web app URL in my client-side code. Note that I tried fetching it manually, but got a:\nAccess to fetch at 'https://docs.google.com/spreadsheets/d/1n4iZYfgdhv08PeREqz26F4-eQYfnXdHaSQywnQ8UYWk/gviz/tq?tqx=out:json&sheet=Sheet1&range=A:E' from origin 'null' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource. If an opaque response serves your needs, set the request's mode to 'no-cors' to fetch the resource with CORS disabled.\nLatest sensor reading Google Sheets(button loads latest data):\n\n\n\n    Google Sheets Data Display\n    Load Google Sheets Data\n    \n\n\n\nDate\nTime\nHumidity (%)\nTemp F\nTemp C\n\n\n\n\n\n\n\n\n\n\n\n\nLatest Sensor Reading ThingSpeak(press button):\n\n\n\n    ThingSpeak Data Display\n    Load ThingSpeak Data\n    \n\n\n\nCreated At\nEntry ID\nTemperature C\nTemperature F\nHumidity\nTime\nDate\n\n\n\n\n\n\n\n\n\n\n\nConfiguring MongoDB to grab the sensor readings and the CSV required me to do quite a bit of configuring of Vercel(timeout increase) and making sure that the MongoDB was no longer using fixed IP addresses(I’m not implementing some cursed dynamic IP allocation). I also implement a 3 second wait just in case there’s a lot of requests/readers(unlikely).\nLatest Sensor Reading MongoDB(Read-Only User):\n\n\n\n    MongoDB Data Display\n    Download MongoDB Table Data as CSV\n    Load MongoDB Data\n    \n\n\n\nDate\nTime\nHumidity (%)\nTemperature (F)\nTemperature (C)\n\n\n\n\n\n\n\n\n\n\nLatest Sensor Reading Vercel PostgreSQL(Read-Only User):\n\n\n\n    Vercel Data Display\n    Download Vercel Table Data as CSV\n    Load Vercel Data\n    \n\n\n\nDate\nTime\nHumidity (%)\nTemperature (F)\nTemperature (C)\n\n\n\n\n\n\n\n\n\n\nFinal Python Script on the off chance something changed:\n*Note you can run with this pretty easily, change the flags to True for whatever “database” you’re using.*\n\n\nCode\n\nimport os\nimport time as time_module\nimport requests\nfrom datetime import datetime\nimport random\nimport json\nimport subprocess\nimport Adafruit_DHT\nfrom pymongo import MongoClient\nfrom google.auth.transport.requests import Request\nfrom google.oauth2.credentials import Credentials\nfrom google_auth_oauthlib.flow import InstalledAppFlow\nfrom googleapiclient.discovery import build\nfrom googleapiclient.errors import HttpError\n\n# Control flags\nSEND_TO_VERCEL = True\nSEND_TO_GOOGLE_SHEETS = True\nSEND_TO_THINGSPEAK = True\nSEND_TO_MONGODB = True\n\n# ThingSpeak API settings\nTHINGSPEAK_API_KEY = 'YOUR-THINGSPEAK-API-KEY'  # Replace with your ThingSpeak API key\n\nTHINGSPEAK_BASE_URL = 'https://api.thingspeak.com/update'\nTHINGSPEAK_CHANNEL_ID = 00000000 #CHANGE ME\nTHINGSPEAK_BULK_UPDATE_URL = 'https://api.thingspeak.com/channels/'+str(THINGSPEAK_CHANNEL_ID)+'/bulk_update.json'\nprint(THINGSPEAK_BULK_UPDATE_URL)\n\nVERCEL_API_KEY = 'YOUR-VERCEL-API-KEY'\n\nMONGODB_URI = 'mongodb+srv://&lt;USERNAME&gt;:&lt;PASSWORD&gt;@cluster0.YOUR-URL.mongodb.net'\nMONGODB_DB_NAME = 'Raspberry_Pi'\nMONGODB_COLLECTION_NAME = 'Readings'\n# Sensor setup\nSENSOR = Adafruit_DHT.DHT11  # Using DHT11 sensor\nPIN = 4  # Change this to the GPIO pin number that the sensor is connected to\n\n# Vercel endpoint URL\nURL = 'https://YOUR-VERCEL-URL.vercel.app/api/sensor'\n\n# Buffer to store data\ndata_buffer_vercel = []\ndata_buffer_mongodb = []\nthingspeak_buffer = []  # Buffer for ThingSpeak data\n\n# Define loop time in seconds\nLOOP_TIME = 1  # You can change this to the desired loop time in seconds\n\n# Define the scopes required for the Google Sheets and Drive APIs\nSCOPES = [\n    'https://www.googleapis.com/auth/spreadsheets',\n    'https://www.googleapis.com/auth/drive.file'\n]\n\n# Global variable to hold credentials\ncreds = None\n\n# Function to get authenticated Sheets service\ndef get_sheets_service():\n    global creds\n    if os.path.exists('token.json'):\n        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n    if not creds or not creds.valid:\n        if creds and creds.expired and creds.refresh_token:\n            creds.refresh(Request())\n        else:\n            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n            creds = flow.run_local_server(port=0)\n        with open('token.json', 'w') as token:\n            token.write(creds.to_json())\n    return build('sheets', 'v4', credentials=creds)\n\n# Function to get authenticated Drive service\ndef get_drive_service():\n    global creds\n    if creds is None:\n        get_sheets_service()  # This will initialize creds\n    return build('drive', 'v3', credentials=creds)\n\n# Function to set a Google Sheet's permissions to public\ndef set_sheet_public(spreadsheet_id):\n    drive_service = get_drive_service()\n    permission = {\n        'type': 'anyone',\n        'role': 'reader'\n    }\n    try:\n        drive_service.permissions().create(fileId=spreadsheet_id, body=permission).execute()\n        print(f\"Set spreadsheet with ID {spreadsheet_id} to public\")\n    except HttpError as error:\n        print(f\"An error occurred: {error}\")\n\n# Function to create a new Google Sheet with a given name\ndef create_new_sheet(sheet_name):\n    service = get_sheets_service()\n    spreadsheet = {\n        'properties': {\n            'title': sheet_name\n        }\n    }\n    spreadsheet = service.spreadsheets().create(body=spreadsheet, fields='spreadsheetId').execute()\n    spreadsheet_id = spreadsheet.get('spreadsheetId')\n    print(f\"Created new spreadsheet with ID: {spreadsheet_id}, Name: {sheet_name}\")\n\n    # Set the new sheet to be publicly viewable\n    set_sheet_public(spreadsheet_id)\n\n    # Save the new spreadsheet info\n    save_spreadsheet_info(spreadsheet_id, sheet_name, \"Sheet1\")\n\n    return spreadsheet_id\n\n# Function to check the number of cells in the Google Sheet and create a new one if it doesn't exist\ndef check_sheet_size(spreadsheet_id, new_sheet_name):\n    service = get_sheets_service()\n    try:\n        sheet = service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()\n        sheets = sheet.get('sheets', [])\n    except HttpError as e:\n        print(f\"HttpError encountered: Status code: {e.resp.status}, Reason: {e.error_details}\")\n        if e.resp.status == 404:\n            print(f\"Spreadsheet with ID '{spreadsheet_id}' not found. Creating a new sheet.\")\n            return create_new_sheet(new_sheet_name), 0\n        else:\n            raise\n\n    total_cells = 0\n    for sheet in sheets:\n        properties = sheet.get('properties', {})\n        grid_properties = properties.get('gridProperties', {})\n        rows = grid_properties.get('rowCount', 0)\n        cols = grid_properties.get('columnCount', 0)\n        total_cells += rows * cols\n\n    return spreadsheet_id, total_cells\n\n# Function to find the last empty row in the Google Sheet\ndef find_last_empty_row(service, spreadsheet_id, sheet_name,i):\n    max_retries = 5\n    retry_count = 0\n    backoff_factor = 2\n\n    range_name = f\"{sheet_name}!A:A\"\n\n    while retry_count &lt; max_retries:\n        try:\n            result = service.spreadsheets().values().get(\n                spreadsheetId=spreadsheet_id,\n                range=range_name\n            ).execute()\n\n            values = result.get('values', [])\n            if i%15==0:\n                print(f\"Debug Google - Number of rows in column A: {len(values)}\")\n            return len(values) + 1\n        except HttpError as e:\n            if e.resp.status in [500, 503]:\n                retry_count += 1\n                sleep_time = backoff_factor ** retry_count + random.uniform(0, 1)\n                print(f\"HttpError {e.resp.status} encountered. Retrying in {sleep_time:.1f} seconds...\")\n                time.sleep(sleep_time)\n            else:\n                raise\n    raise Exception(\"Failed to retrieve last empty row after several retries\")\n\n# Function to append data to a Google Sheet\ndef append_data_to_sheet(spreadsheet_id, sheet_name, values,i):\n    if SEND_TO_GOOGLE_SHEETS:\n        service = get_sheets_service()\n        last_empty_row = find_last_empty_row(service, spreadsheet_id, sheet_name,i)\n        range_name = f\"{sheet_name}!A{last_empty_row}\"\n        body = {\n            'values': values\n        }\n        result = service.spreadsheets().values().append(\n            spreadsheetId=spreadsheet_id, range=range_name,\n            valueInputOption='RAW', body=body).execute()\n        return result\n\n# Function to send data to the server\ndef send_data_vercel(data):\n    \"\"\"Send data to the server.\"\"\"\n    if SEND_TO_VERCEL:\n        try:\n            # Include API key in the headers\n            headers = {'x-api-key': VERCEL_API_KEY}\n            print(data)\n            print(headers)\n            response = requests.post(URL, json=data, headers=headers)\n            response.raise_for_status()\n            #print(data)\n            #print(headers)\n            print(f\"Data sent to Vercel server: {response.text}\")\n        except requests.exceptions.RequestException as e:\n            print(f\"Failed to send data to server: {e}\")\n        except Exception as e:\n            print(f\"An unexpected error occurred: {e}\")\n# Function to send data to ThingSpeak\ndef thingspeak_update():\n    \"\"\"Send data to ThingSpeak.\"\"\"\n    if SEND_TO_THINGSPEAK and thingspeak_buffer:\n        if len(thingspeak_buffer) &gt; 1:\n            # Bulk update\n            payload = {\n                'write_api_key': THINGSPEAK_API_KEY,\n                'updates': []\n            }\n            #print(THINGSPEAK_API_KEY)\n            # Convert buffer data to the format required by ThingSpeak\n            \n            for data in thingspeak_buffer:\n                update = {\n                    'created_at': f\"{data['date']} {data['time']} -0500\",\n                    'field1': data['temperature_C'],\n                    'field2': data['temperature_F'],\n                    'field3': data['humidityPercent'],\n                    'field4': data['time'],\n                    'field5': data['date']\n                }\n                payload['updates'].append(update)\n\n            try:\n                # Send the bulk update request to ThingSpeak\n                headers = {'Content-Type': 'application/json'}\n                #print(len(thingspeak_buffer))\n                #print(headers)\n                #print(json.dumps(payload))\n                # Convert the data payload to JSON format\n                json_data = json.dumps(payload)\n                response = requests.post(THINGSPEAK_BULK_UPDATE_URL,headers=headers,data=json_data)\n                if response.status_code == 202:\n                    print('Data posted to ThingSpeak (bulk update):', response.text)\n                    thingspeak_buffer.clear()  # Clear the buffer after successful update\n                else:\n                    print(f'Failed to send data to ThingSpeak (bulk update): {response.status_code}, {response.text}')\n            except requests.exceptions.RequestException as e:\n                print('Failed to send data to ThingSpeak (bulk update):', e)\n        else:\n            # Simple update\n            data = thingspeak_buffer[0]\n            payload = {\n                'api_key': THINGSPEAK_API_KEY,\n                'field1': data['temperature_C'],\n                'field2': data['temperature_F'],\n                'field3': data['humidityPercent'],\n                'field4': data['time'],\n                'field5': data['date'],\n                'created_at': f\"{data['date']}T{data['time']}Z\"\n            }\n\n            try:\n                # Send the simple update request to ThingSpeak\n                headers = {\n                    'User-Agent': 'mw.doc.simple-update (Raspberry Pi)',\n                    'Content-Type': 'application/x-www-form-urlencoded'\n                }\n                response = requests.post(THINGSPEAK_BASE_URL, headers=headers, params=payload)\n                if response.status_code == 200:\n                    print('Data posted to ThingSpeak (simple update):', response.text)\n                    thingspeak_buffer.clear()  # Clear the buffer after successful update\n                else:\n                    print(f'Failed to send data to ThingSpeak (simple update): {response.status_code}, {response.text}')\n            except requests.exceptions.RequestException as e:\n                print('Failed to send data to ThingSpeak (simple update):', e)\n\n# Function to send data to MongoDB\ndef send_data_mongodb(data_mongo):\n    \"\"\"Send data to MongoDB.\"\"\"\n    if SEND_TO_MONGODB:\n        try:\n            client = MongoClient(MONGODB_URI)\n            db = client[MONGODB_DB_NAME]\n            collection = db[MONGODB_COLLECTION_NAME]\n            result = collection.insert_many(data_mongo)\n            print(f\"Data sent to MongoDB: {result.inserted_ids}\")\n        except Exception as e:\n            print(f\"Failed to send data to MongoDB: {e}\")\n\n# Function to initialize the current count of the index from MongoDB\n#I was debating on introducing an index to make sure I knew what the current count of values was..\n#But that seemed pretty wasteful.\n# def initialize_index():\n#     global index\n#     try:\n#         client = MongoClient(MONGODB_URI)\n#         db = client[MONGODB_DB_NAME]\n#         collection = db[MONGODB_COLLECTION_NAME]\n#         last_entry = collection.find().sort([('_id', -1)]).limit(1)\n#         if last_entry.count() &gt; 0:\n#             index = last_entry[0].get('index', 0) + 1\n#         else:\n#             index = 1\n#         print(f\"Index initialized to {index}\")\n#     except Exception as e:\n#         print(f\"Failed to initialize index: {e}\")\n        \n# Function to save spreadsheet info to a file\ndef save_spreadsheet_info(spreadsheet_id, workbook_name, sheet_name):\n    info = {\n        'spreadsheet_id': spreadsheet_id,\n        'workbook_name': workbook_name,\n        'sheet_name': sheet_name\n    }\n    with open('spreadsheet_info.txt', 'w') as f:\n        json.dump(info, f)\n\n# Function to load spreadsheet info from a file\ndef load_spreadsheet_info():\n    if os.path.exists('spreadsheet_info.txt'):\n        with open('spreadsheet_info.txt', 'r') as f:\n            info = json.load(f)\n            return info['spreadsheet_id'], info['workbook_name'], info['sheet_name']\n    return None, None, None\n\ndef main():\n    # Load spreadsheet info from file\n    spreadsheet_id, workbook_name, sheet_name = load_spreadsheet_info()\n\n    if spreadsheet_id is None or workbook_name is None or sheet_name is None:\n        # Initialize new sheet if no info found\n        workbook_name = 'your-workbook-name'  # Replace with your Google Sheet ID\n        spreadsheet_id = 'your-spreadsheet-id'  # Replace with your Google Sheet ID\n        sheet_name = 'Sheet1'  # Adjust the sheet name as needed\n\n    max_cells = 5000000\n\n    # Initialize loop counter\n    i = 1\n\n    # Get current date and time for the new sheet name if needed\n    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    new_sheet_name = f\"Pi-Sensor {current_time}\"  # New sheet name with date and time\n    \n    # Check sheet size and create new sheet if necessary\n    if SEND_TO_GOOGLE_SHEETS:\n        spreadsheet_id, total_cells = check_sheet_size(spreadsheet_id, new_sheet_name)\n        print(f\"Total cells in the sheet: {total_cells}\")\n        \n        if total_cells &gt;= max_cells:\n            print(\"The sheet has reached the maximum cell limit. Creating a new sheet.\")\n            spreadsheet_id = create_new_sheet(new_sheet_name)\n            sheet_name = 'Sheet1'  # Reset to default sheet name for new spreadsheet\n        else:\n            print(\"The sheet has not reached the maximum cell limit.\")\n\n    # Time tracking for ThingSpeak\n    last_thingspeak_update = time_module.time()\n    last_mongodb_update = time_module.time()\n    last_vercel_update = time_module.time()\n    \n    while True:\n        # Get the current time at the start of the loop\n        start_time = time_module.time()\n        \n        # Read humidity and temperature from DHT sensor\n        humidityPercent, temperature_C = Adafruit_DHT.read_retry(SENSOR, PIN)\n        \n        if humidityPercent is not None and temperature_C is not None:\n            # Prepare the data payload\n            temperature_F = temperature_C * 9.0 / 5.0 + 32\n            \n            now = datetime.now()\n            date = now.strftime(\"%m-%d-%Y\")\n            timeNow = now.strftime(\"%H:%M:%S\")\n            \n            dataVercel = {\n                'date': date,\n                'time': timeNow,\n                'humidityPercent': humidityPercent,\n                'temperatureFahrenheit': temperature_F,\n                'temperatureCelsius': temperature_C\n            }\n            dataMongo = {\n                'date': date,\n                'time': timeNow,\n                'humidityPercent': humidityPercent,\n                'temperatureFahrenheit': temperature_F,\n                'temperatureCelsius': temperature_C,\n            }\n            # Log data to buffer. had to separate data as sometimes ObjectID would somehow get passed to Vercel's buffer.\n            data_buffer_vercel.append(dataVercel)\n            data_buffer_mongodb.append(dataMongo)\n            if i % 60 == 0:\n                print(f\"Logged data: {dataVercel}\")\n            \n            # Append data to Google Sheet\n            append_data_to_sheet(spreadsheet_id, sheet_name, [[date, timeNow, humidityPercent, temperature_F, temperature_C]],i)\n            \n            # Add data to ThingSpeak buffer\n            thingspeak_buffer.append({\n                'temperature_C': temperature_C,\n                'temperature_F': temperature_F,\n                'humidityPercent': humidityPercent,\n                'time': timeNow,\n                'date': datetime.strptime(date, \"%m-%d-%Y\").strftime(\"%Y-%m-%d\")\n            })\n            \n            # Check if 300 readings have been logged\n            if time_module.time() - last_vercel_update &gt;= 3600:\n                send_data_vercel(data_buffer_vercel)\n                # Clear the buffer after sending\n                data_buffer_vercel.clear()\n                last_vercel_update = time_module.time()\n            # Check if it's time to send data to Mongodb\n            if time_module.time() - last_mongodb_update &gt;= 60:\n                send_data_mongodb(data_buffer_mongodb)\n                data_buffer_mongodb.clear()\n                last_mongodb_update = time_module.time()\n                \n            # Check if it's time to send data to ThingSpeak\n            if time_module.time() - last_thingspeak_update &gt;= 15:\n                thingspeak_update()\n                last_thingspeak_update = time_module.time()\n        else:\n            # Handle cases where sensor fails to read\n            print(\"Failed to retrieve data from sensor\")\n        \n        # Wait until the LOOP_TIME has elapsed\n        while time_module.time() - start_time &lt; LOOP_TIME:\n            time_module.sleep(0.01)  # Sleep a short time to avoid busy-waiting\n\n        i += 1\n\nif __name__ == '__main__':\n    main()\n\nAnd there you have it. 4 different databases and the ability to write/read to them in near real time via a simple web portal. I could have used any of these individually but I wanted to explore the strengths/weaknesses of different configurations. I like the idea of just using a pretty long Google Sheet for most projects and completely ignoring actual databases for simple home automation. Using MongoDB was pretty straightforward via VSCode and it is a strong second choice. Tying for second would be ThingSpeak with its cool online interface, but I’m wary of Mathworks in general with their pay to play scheme. Last place is definitely Vercel’s own implementation of PostgreSQL mostly because of the compute/size limitations.\nI would have implemented multiple sensors, likely in the form of PM2.5/10, CO2, a better DHT22 sensor, and a VOC sensor. Here’s my Amazon wish list for future projects:\nPM2.5/10[PMS5003]: https://www.adafruit.com/product/3686\nCO2[MH-Z19]: https://www.amazon.com/EC-Buying-Monitoring-Concentration-Detection/dp/B0CRKH5XVX/\nTemperature/Humidity[DHT22]: https://www.amazon.com/SHILLEHTEK-Digital-Temperature-Humidity-Sensor/dp/B0CN5PN225/\nTemp/Humidity/Pressure/VOC[BME680]: https://www.amazon.com/CJMCU-680-Temperature-Humidity-Ultra-Small-Development/dp/B07K1CGQTJ/\nAir Quality/VOC[MQ135]: https://www.amazon.com/Ximimark-Quality-Hazardous-Detection-Arduino/dp/B07L73VTTY/"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to my inaugural Quarto post! Though this marks my official entry into the Quarto blogging sphere, it isn’t my first dive into writing here. I initially crafted an insightful(?) post about the Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a fascinating clustering algorithm that piqued my interest due to its rigorous yet intuitive approach to handling complex data sets.\nMy name is Jesse Anderson, and my academic and professional journey began in the world of chemical engineering, where I specialized in biochemical engineering and process automation. The intricate dance of chemical processes, moving systematically from one unit to another, serendipitously mirrored the logic and flow of computer science. It was this realization that bridged my transition from engineering physical processes to engineering digital ones.\nThis transition felt incredibly natural. In both domains, you’re essentially inputting a series of commands—whether to a machine in a lab or a line of code in a software program—and watching as complex processes unfold, often with a satisfying precision. This similarity is what drew me deeper into the world of computing, leading me to pursue a Master’s in Computer Science at the University of Illinois at Urbana-Champaign (UIUC). My undergraduate studies were completed at the University of Illinois at Chicago (UIC), where I earned a Bachelor of Science in Chemical Engineering.\nDuring my time at UIC, my fascination with optimization and automation took root. This passion wasn’t confined to coursework and theory. I actively engaged in developing research software, working closely with Dr. Ying Hu. My contributions during this period were not only fulfilling but also fruitful, culminating in several scholarly publications.\nOutside of academia, my hobbies closely mirror my professional interests, intertwining a love for optimization with a hands-on approach to problem-solving that spans far beyond algorithms and lab reactors. This passion for efficiency and process improvement extends into crafting tools and scripts that enhance productivity and automate the more mundane tasks of everyday life. But my knack for tool-making isn’t just confined to the digital realm; it’s a skill I’ve honed over many years, influenced heavily by my extensive background as an independent contractor.\nBefore venturing into the world of chemical engineering and computer science, I spent several years in the trades, tackling every aspect of plumbing, electrical work, HVAC, carpentry, and remodeling. This experience taught me the value of precision and strategic planning, skills that are just as applicable to programming as they are to physical construction and maintenance. Each project, whether installing a complex plumbing system or wiring a newly constructed home, sharpened my ability to think critically and adapt quickly—traits that have proven indispensable throughout my career.\nIn addition to my hands-on trade work, I managed the safety and project planning operations at G5 Environmental, a prominent street sweeping company based in Chicago. My role involved ensuring the smooth execution of projects, maintaining rigorous safety standards, and optimizing operations to enhance efficiency. This experience not only broadened my understanding of large-scale project management but also deepened my appreciation for the intricate dance between man, machine, and the environment. At G5 Environmental, we were committed to maintaining the cleanliness and safety of public spaces, a mission that echoes my current work’s focus on sustainability and environmental preservation through technology.\nMore recently, during my tenure at UL Solutions, I further developed my technical skills and applied them to real-world challenges:\n\nAutomation Software Development: I developed automation software in Python and VBA that successfully eliminated over 664+ hours on average of tedious engineering workflows annually.\nCAD/PDF Change Detection: I developed a few python scripts to automatically compare CAD files and compare long PDFs(like Acrobat Pro, but free and simpler).\nReal-Time Data Analysis for Testing: I implemented systems for real-time data analysis to support large-scale testing projects. This advancement has enabled more data-driven decision-making, optimizing testing procedures and outcomes through immediate feedback and analysis.\n\nWhile I massively enjoyed the automation projects I took on at UL, I was seeking broader challenges and opportunities for growth that UL could no longer provide. This realization led me to pursue a more fitting opportunity elsewhere, where I could further expand my expertise and impact in the fields of process automation and data-driven technology.\nThis diverse background has equipped me with a unique perspective that I bring to my current studies and research. Whether refining code to streamline data processing or rethinking a workflow to enhance laboratory operations, I am constantly learning and applying, pushing the boundaries of what I know about both chemical engineering and computer science. These experiences have molded me into a versatile professional capable of adapting to various roles and continuously refining my approach to both new and familiar challenges.\nAs I continue my studies at UIUC, my goal is to further explore how advanced computational techniques can be applied to solve complex problems in biochemical engineering and beyond. The integration of process automation into digital applications opens up exciting possibilities for innovation in both fields. Through this blog, I aim to share insights from my journey, discussing both the theoretical and practical aspects of my work and studies in hopes of connecting with fellow enthusiasts and professionals who share my zeal for optimization and automation.\nStay tuned for more posts where I’ll delve deeper into specific projects, challenges I’ve faced, and solutions I’ve discovered along the way. Thank you for joining me on this adventure in merging the systematic worlds of engineering and computing into one coherent, intertwined narrative."
  }
]