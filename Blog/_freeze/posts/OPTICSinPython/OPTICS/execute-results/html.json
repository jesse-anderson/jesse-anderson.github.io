{
  "hash": "9c4a9945c362f4f84e2b17d6e890ba5d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: OPTICS in Python\nauthor: Jesse Anderson\ndate: '2024-05-12'\ncategories:\n  - ML\n  - Clustering\n  - OPTICS\nformat: html\n---\n\n## Import Libraries\n\n::: {#0c833175 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import OPTICS\nfrom sklearn import datasets\nfrom kneed import KneeLocator\nfrom matplotlib.patches import Polygon\nimport matplotlib.colors as mcolors\nfrom scipy.spatial import ConvexHull\n```\n:::\n\n\nThis block imports all necessary Python libraries. 'numpy' and 'matplotlib' are used for numerical operations and plotting(respectively). 'sklearn' provides the OPTICS clustering algorithm(for better or worse) and the blob generator. 'kneed' is used to identify the \"knee\"/\"elbow\" point of the data, but didn't work in this case. We will explore alternatives below. 'matplotlib.colors' and 'scipy.spatial' help with the visualization and the convex hull calculations respectively.\n\n# Load and Visualize the Blob Data\n\n::: {#fb323fec .cell execution_count=2}\n``` {.python .cell-code}\n# Load example data\nn_samples = 1000\nmin_samples = 15\ndata, _ = datasets.make_blobs(n_samples=n_samples, centers=4, random_state=42, cluster_std=2.0)\n\n# Plot the original data\nplt.figure(figsize=(8, 6))\nplt.scatter(data[:, 0], data[:, 1], alpha=0.6, edgecolors='w', s=30)\nplt.title('Blob Data Distribution')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](OPTICS_files/figure-html/cell-3-output-1.png){width=670 height=523}\n:::\n:::\n\n\nSynthetic blob data is generated using 'make_blobs' and we specify the number of points, centers(clusters) , random_state(for repeatability), and the cluster standard deviation(spread). The plot will help the user to get a feel for what the raw, unclustered data looks like. Keep in mind that we are using OPTICS to get to the \"meat\" of the cluster, there will be points identified as noise or otherwise of less significance than the core points.\n\n# Apply sklearn OPTICS Clustering\n\n::: {#34f9e7e9 .cell execution_count=3}\n``` {.python .cell-code}\n# Step 1: Apply OPTICS\noptics_model = OPTICS(min_samples=min_samples)\noptics_model.fit(data)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OPTICS(min_samples=15)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OPTICS</label><div class=\"sk-toggleable__content\"><pre>OPTICS(min_samples=15)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nOPTICS(Ordering Points to Identify Clustering Structure) is an algorithm similar enough to DBSCAN, but handles varying densities better. This block will initialize the OPTICS model with a min sample count per cluster and fit it to the dataset. This will allow us to extract the reachability distances later on, which will quantify the density connectivity of the data points.\n\n# Get Reachability Distances & Handle Infinity Values\n\n::: {#249f76ec .cell execution_count=4}\n``` {.python .cell-code}\n# Step 2: Plot the reachability diagram\nspace = np.arange(len(data))\nreachability = optics_model.reachability_[optics_model.ordering_]\n\n# Check for infinity values in reachability distances\nif np.isinf(reachability).any():\n    print(\"Infinity values found in reachability distances, handling...\")\n    # Identify finite indices where reachability values are not infinity\n    finite_indices = np.isfinite(reachability)\n\n    # Filter out infinite reachability values and their corresponding indices\n    reachability = reachability[finite_indices]\n    space = space[finite_indices]\n\nplt.figure(figsize=(10, 5))\nplt.bar(space, reachability, color='g')\nplt.title('OPTICS Reachability Plot')\nplt.xlabel('Data points')\nplt.ylabel('Reachability Distance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInfinity values found in reachability distances, handling...\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](OPTICS_files/figure-html/cell-5-output-2.png){width=812 height=449}\n:::\n:::\n\n\nThis section will generate a plot of the unordered reachability distances and then further check that the reachability distances haven't \"exploded\" to infinity. If they have they will be filtered out. the reachability plot is then plotted. Keep in mind that the reachability plot visualizes the distance at which each point becomes directly density-reachable from its predecessor in the ordered list. This will help you to understand the cluster structure and identify areas of high density(low reachability distance). You are looking for \"waves\" in which the reachability distance increases, drops off, and then begins to increase. Later on we will create an arbitrary cutoff for the reachability distance to identify each cluster.\n\n# Identify the \"Elbow\" Point in the Sorted Reachability Plot\n\n::: {#5b157d9e .cell execution_count=5}\n``` {.python .cell-code}\n# Plot sorted reachability distances\nsorted_reachability = np.sort(reachability)\nindices = np.arange(len(sorted_reachability))\n# plt.figure(figsize=(10, 5))\n# plt.plot(indices, sorted_reachability, label='Reachability Distance')\n\n# Compute the line endpoints based on the first and last reachability distance\nline_start = np.array([0, sorted_reachability[0]])\nline_end = np.array([len(sorted_reachability) - 1, sorted_reachability[-1]])\n\n# Calculate the slope and intercept of the line (y = mx + b)\nslope = (line_end[1] - line_start[1]) / (line_end[0] - line_start[0])\nintercept = line_start[1] - (slope * line_start[0])\n\n# Calculate the y-values of the line across all x-values\nline_y = (slope * indices) + intercept\n\n# Plot the line\n# plt.plot(indices, line_y, 'r--', label='Line from first to last index')\n\n# Find the elbow index (maximum deviation point)\ndeviations = np.abs(sorted_reachability - line_y)\nelbow_index = np.argmax(deviations)\n\nelbow_cutoff = sorted_reachability[elbow_index]\n\n\n# Mark the elbow point\n# plt.axvline(x=elbow_index, color='b', linestyle='--', label=f'Elbow Point at index {elbow_index} = {elbow_cutoff}')\n# plt.axhline(y = elbow_cutoff, color = 'k', linestyle = ':', label = 'Cutoff for reachability distance.')\n# plt.title('Sorted Reachability Distances with Maximum Deviation Line')\n# plt.xlabel('Data points (sorted)')\n# plt.ylabel('Reachability Distance')\n# plt.legend()\n# plt.show()\n```\n:::\n\n\n![](ElbowPlot.png)\n\nWe plot the reachability distances sorted from least to greatest to get an idea of how the distribution looks. In this case we have a nice transition point where the distance explodes and that is roughly where we are aiming for our first \"guess\" of the reachability distance cutoff. We then implement a pretty basic algorithm wherein we find the maximum deviation point, that is the point farthest from the horizontal line drawn between the first and last reachability points. You will find a MATLAB based explaination [here](https://www.mathworks.com/help/radar/ref/clusterdbscan.clusterdbscan.estimateepsilon.html). Note that we do NOT use KneeLocator from the kneed library as it gives erroneous results in this case.\n\n# Trying the Maximum Deviation Line for Clustering\n\n::: {#0a740cac .cell execution_count=6}\n``` {.python .cell-code}\n#  Create a list of colors to use for bars below the threshold, and black for above\n# We'll generate a color map to cycle through a set of colors for segments below the threshold\nbelow_threshold_colors = list(mcolors.TABLEAU_COLORS.values())  # Get a set of unique colors\ncolor_index = 0\ncurrent_color = below_threshold_colors[color_index]\ncolors = []\n\n# Track whether the previous bar was below the threshold\nprevious_bar_below = False\n\n# Assign colors to bars based on their height relative to the threshold\nfor val in reachability:\n    if val <= elbow_cutoff:\n        # If we're transitioning from above to below the threshold, change the color\n        if not previous_bar_below:\n            color_index = (color_index + 1) % len(below_threshold_colors)\n            current_color = below_threshold_colors[color_index]\n        colors.append(current_color)\n        previous_bar_below = True\n    else:\n        colors.append('k')  # Black for bars above the threshold\n        previous_bar_below = False\n\n# Now, let's create the reachability plot with custom bar colors\n# plt.figure(figsize=(10, 5))\n# plt.axhline(y=elbow_cutoff, color='k', linestyle=':', label='Cutoff for reachability distance.')\n# plt.bar(space, reachability, color=colors)\n# plt.title('OPTICS Reachability Plot')\n# plt.xlabel('Data points')\n# plt.ylabel('Reachability Distance')\n# plt.legend()\n# plt.show()\n```\n:::\n\n\n::: {#764c76e4 .cell execution_count=7}\n``` {.python .cell-code}\n# Step 3: Extract clusters using the reachability distance at the elbow point as a cutoff\n# The reachability distance at the elbow is used as a threshold for cluster extraction\nreachability_distance_at_elbow = elbow_cutoff\n\n# Retrieve the ordered reachability distances from the OPTICS model\nordered_reachability = optics_model.reachability_[optics_model.ordering_]\nordered_indices = optics_model.ordering_\n\n# Initialize all points as noise (-1)\nlabels = np.full(ordered_reachability.shape, -1)  \n\n# Manually assign clusters based on the new threshold, apparently OPTICS.expand_clusters or similar doesn't work anymore so here we are...\ncluster_id = 0\nfor i in range(len(ordered_reachability)):\n    if ordered_reachability[i] <= reachability_distance_at_elbow:\n        if labels[ordered_indices[i]] == -1:  # If this point has not been assigned a cluster\n            # Start a new cluster from this point\n            labels[ordered_indices[i]] = cluster_id\n            # Add all directly reachable points to the same cluster\n            for j in range(i + 1, len(ordered_reachability)):\n                if ordered_reachability[j] <= reachability_distance_at_elbow:\n                    labels[ordered_indices[j]] = cluster_id\n                else:\n                    break\n            cluster_id += 1\n\n# Step 4: Plot the clusters based on the new manually set labels\n# plt.figure(figsize=(10, 7))\nunique_labels = set(labels)\n\n# Generate colors for each cluster label\ncolors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n\n# for klass, color in zip(unique_labels, colors):\n#     class_member_mask = (labels == klass)\n#     xy = data[class_member_mask]\n#     if klass != -1:\n#         # Draw the convex hull and fill it with a light shade of the cluster color\n#         if xy.shape[0] > 2:  # ConvexHull needs at least 3 points to compute the hull\n#             hull = ConvexHull(xy)\n#             plt.fill(xy[hull.vertices, 0], xy[hull.vertices, 1], color=color, alpha=0.2, edgecolor='k')\n#         plt.scatter(xy[:, 0], xy[:, 1], color=color, edgecolor='k', label=f'Cluster {klass}')\n#     else:\n#         plt.scatter(xy[:, 0], xy[:, 1], c='k', marker='x', label='Noise')\n\n# plt.title('Clusters by OPTICS with Manual Threshold and Hulls')\n# plt.xlabel('Feature 1')\n# plt.ylabel('Feature 2')\n# plt.legend()\n# plt.show()\n```\n:::\n\n\n![](ReachabilityElbowOptimal.png)\n\n![](ClustersElbowAutomatic.png)\n\nAs you can see above we get three clusters based on one of the clusters encountering significant overlap. We can stop here and examine why this may be when we don't have synthetic data, but at least for the purposes of this demonstration I will find the point at which I need to set the reachability distance to distinguish that first cluster by hovering over that point in Python's matplotlib plot. As you can see below that value is roughly 0.88 . See that all we have to do is change one line and we can rerun the reachability distance.\n\n::: {#6339ba58 .cell execution_count=8}\n``` {.python .cell-code}\nelbow_cutoff =0.866 ##THIS IS A GREAT EXAMPLE OF WHEN YOUR ELBOW FAILS YOU AND YOU NEED TO PLAY AROUND WITH VALUES!!!!\n\n#  Create a list of colors to use for bars below the threshold, and black for above\n# We'll generate a color map to cycle through a set of colors for segments below the threshold\nbelow_threshold_colors = list(mcolors.TABLEAU_COLORS.values())  # Get a set of unique colors\ncolor_index = 0\ncurrent_color = below_threshold_colors[color_index]\ncolors = []\n\n# Track whether the previous bar was below the threshold\nprevious_bar_below = False\n\n# Assign colors to bars based on their height relative to the threshold\nfor val in reachability:\n    if val <= elbow_cutoff:\n        # If we're transitioning from above to below the threshold, change the color\n        if not previous_bar_below:\n            color_index = (color_index + 1) % len(below_threshold_colors)\n            current_color = below_threshold_colors[color_index]\n        colors.append(current_color)\n        previous_bar_below = True\n    else:\n        colors.append('k')  # Black for bars above the threshold\n        previous_bar_below = False\n\n# Now, let's create the reachability plot with custom bar colors\nplt.figure(figsize=(10, 5))\nplt.axhline(y=elbow_cutoff, color='k', linestyle=':', label='Cutoff for reachability distance.')\nplt.bar(space, reachability, color=colors)\nplt.title('OPTICS Reachability Plot')\nplt.xlabel('Data points')\nplt.ylabel('Reachability Distance')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](OPTICS_files/figure-html/cell-9-output-1.png){width=812 height=449}\n:::\n:::\n\n\nNow that we have broken out the clusters we can now reassign clusters based on the new threshold.\n\n::: {#aae23c76 .cell execution_count=9}\n``` {.python .cell-code}\n# Step 3: Extract clusters using the reachability distance at the elbow point as a cutoff\n# The reachability distance at the elbow is used as a threshold for cluster extraction\nreachability_distance_at_elbow = elbow_cutoff\n\n# Retrieve the ordered reachability distances from the OPTICS model\nordered_reachability = optics_model.reachability_[optics_model.ordering_]\nordered_indices = optics_model.ordering_\n\n# Initialize all points as noise (-1)\nlabels = np.full(ordered_reachability.shape, -1)  \n\n# Manually assign clusters based on the new threshold, apparently OPTICS.expand_clusters or similar doesn't work anymore so here we are...\ncluster_id = 0\nfor i in range(len(ordered_reachability)):\n    if ordered_reachability[i] <= reachability_distance_at_elbow:\n        if labels[ordered_indices[i]] == -1:  # If this point has not been assigned a cluster\n            # Start a new cluster from this point\n            labels[ordered_indices[i]] = cluster_id\n            # Add all directly reachable points to the same cluster\n            for j in range(i + 1, len(ordered_reachability)):\n                if ordered_reachability[j] <= reachability_distance_at_elbow:\n                    labels[ordered_indices[j]] = cluster_id\n                else:\n                    break\n            cluster_id += 1\n\n# Step 4: Plot the clusters based on the new manually set labels\n# plt.figure(figsize=(10, 7))\nunique_labels = set(labels)\n\n# Generate colors for each cluster label\ncolors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\n\n# for klass, color in zip(unique_labels, colors):\n#     class_member_mask = (labels == klass)\n#     xy = data[class_member_mask]\n#     if klass != -1:\n#         # Draw the convex hull and fill it with a light shade of the cluster color\n#         if xy.shape[0] > 2:  # ConvexHull needs at least 3 points to compute the hull\n#             hull = ConvexHull(xy)\n#             plt.fill(xy[hull.vertices, 0], xy[hull.vertices, 1], color=color, alpha=0.2, edgecolor='k')\n#         plt.scatter(xy[:, 0], xy[:, 1], color=color, edgecolor='k', label=f'Cluster {klass}')\n#     else:\n#         plt.scatter(xy[:, 0], xy[:, 1], c='k', marker='x', label='Noise')\n\n# plt.title('Clusters by OPTICS with Manual Threshold and Hulls')\n# plt.xlabel('Feature 1')\n# plt.ylabel('Feature 2')\n# plt.legend()\n# plt.show()\n```\n:::\n\n\n![](Reachability Distance.png)\n\n![](ClustersManual.png)\n\nAnd that's it. Given this data and the significant overlap between cluster's 0 and 1 I would definitely just go with the original reachability distance found by the elbow, but given that we have ZERO underlying knowledge besides here's 4 clusters here we are. Note that the ratio of noise:data changed and more points are discarded. Evaluate whether or not those points are important and if so consider other clustering algorithms(I'm looking at you Esimation Maximization). For an example with differing STD see [here](Optics_Diff_STD.py).\n\nIf we want to use the Xi clustering method on this data by evaluating the relative slopes of the reachability plot or by evaluating the silhouette score of a variety of Xi values we can proceed with the below. Please note that we use the same data as before.\n\n::: {#a97756d5 .cell execution_count=10}\n``` {.python .cell-code}\n#Now instead of using the reachability metric we use the Xi clustering method to apply OPTICS\n#Please note I tried relative differences in reachability distances by calculating their relative slopes and another method using significant slopes\n# using a percentile approach and they both performed poor.\nfrom sklearn.metrics import silhouette_score\nfrom joblib import Parallel, delayed #I have 16 cores and I will use 16 cores....\n# Initial OPTICS model to get reachability distances\noptics_model_initial = OPTICS(min_samples=min_samples)\noptics_model_initial.fit(data)\nreachabilities_initial = optics_model_initial.reachability_[optics_model_initial.ordering_]\n\n# Calculate differences in reachability distances\nreachability_diffs = np.diff(reachabilities_initial)\nreachability_diffs = reachability_diffs[np.isfinite(reachability_diffs)]  # Remove any infinite or NaN values\n\n# Validate differences before calculating statistics\nif len(reachability_diffs) > 0:\n    mean_diff = np.mean(reachability_diffs)\n    std_diff = np.std(reachability_diffs)\n    xi_value = max(0.0, min(mean_diff + std_diff, 1.0))  # Ensure Xi is within [0.0, 1.0] and not NaN\nelse:\n    xi_value = 0.05  # Fallback value if differences calculation fails\nprint(\"Xi Value[Differences]:\", xi_value)\n\n# Calculate relative slopes (normalized by preceding reachability value)\nslopes = np.diff(reachabilities_initial) / reachabilities_initial[:-1]\n# print(slopes)\n# Handle any NaN or infinity from division by very small numbers\nslopes = np.nan_to_num(slopes)\n\n# Determine significant slopes using a percentile approach\nsignificant_slope_threshold = np.percentile(np.abs(slopes), 96)\nprint(significant_slope_threshold)\n# Use this threshold to set Xi\nxi_value = significant_slope_threshold\nprint(\"Xi_value[Significanty Slopes]: \", xi_value)\n\n\n\n#Rewrite Me\n\n# Define a function that computes the silhouette score for a given Xi\ndef compute_silhouette(xi, data, min_samples):\n    model = OPTICS(min_samples=min_samples, xi=xi, cluster_method='xi')\n    model.fit(data)\n    labels = model.labels_\n    if len(set(labels)) > 1 and np.any(labels != -1):\n        return silhouette_score(data, labels)\n    else:\n        return -1  # Return a penalty for bad clustering\n    \n# Trying a range of potential Xi values to find the best based on silhouette score\nxi_values = np.linspace(0.01, 1, 1000)  # Testing Xi values from 0.01 to 0.1\n\n#Note this can take some time to calculate...\nsilhouette_scores = []\n#Silhouette score is a metric used to evaluate the quality of clustering.Aim for highest silhouette score.\n\n# Use joblib to parallelize the computation of silhouette scores. Update article.\nsilhouette_scores = Parallel(n_jobs=-1)(delayed(compute_silhouette)(xi, data, min_samples) for xi in xi_values)\n\n# Plot the silhouette scores for different Xi values\n# plt.figure(figsize=(10, 5))\n# plt.plot(xi_values, silhouette_scores, marker='o')\n# plt.title('Silhouette Scores for Different Xi Values')\n# plt.xlabel('Xi Value')\n# plt.ylabel('Silhouette Score')\n# plt.grid(True)\n# plt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nXi Value[Differences]: 0.06738467759335512\n0.06486873356523164\nXi_value[Significanty Slopes]:  0.06486873356523164\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Jesse\\AppData\\Local\\Temp\\ipykernel_231840\\2141299887.py:25: RuntimeWarning: invalid value encountered in divide\n  slopes = np.diff(reachabilities_initial) / reachabilities_initial[:-1]\n```\n:::\n:::\n\n\n![](SilhouetteScore.png)\n\nAs one can see in the image above, the silhouette score is maximized at a value of: 0.1725225.... and thus that is what we will use. The silhouette score is a measure of how similar an object is to its own cluster compared to other clusters. This metric ranges from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. A score close to 1 signifies that the object is far away from its neighboring clusters, a score of 0 indicates that the object is on or very close to the decision boundary between two neighboring clusters, and a negative score indicates that the object might have been assigned to the wrong cluster. From here we will select the best Xi to use based on the maximum silhouette score.\n\n::: {#e2ec73b6 .cell execution_count=11}\n``` {.python .cell-code}\n# Select the best Xi based on maximum silhouette score\nbest_xi_index = np.argmax(silhouette_scores)\nbest_xi = xi_values[best_xi_index]\nxi_value = best_xi\nprint(\"Xi Value[Silhouette Score Iterative]\", xi_value)\n# Apply OPTICS using dynamically determined xi clustering method\noptics_model = OPTICS(min_samples=min_samples, xi=xi_value, cluster_method='xi')\noptics_model.fit(data)\n\n# Extract the clusters using the xi method\nlabels = optics_model.labels_\n\n# Reachability plot\nreachabilities = optics_model.reachability_[optics_model.ordering_]\nspace = np.arange(len(data))\n\n# plt.figure(figsize=(12, 6))\n# plt.bar(space, reachabilities, color='g')\n# plt.title('OPTICS Reachability Plot with Dynamic Xi Clustering')\n# plt.xlabel('Ordered Data Points')\n# plt.ylabel('Reachability Distance')\n# plt.show()\n\n# Plot the clusters\nunique_labels = set(labels)\n\n# Generate colors for each cluster label\ncolors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))\ncluster_colors = [colors[list(unique_labels).index(label)] if label != -1 else 'k' for label in labels[optics_model.ordering_]]\n\n# plt.figure(figsize=(12, 6))\n# plt.bar(space, reachabilities, color=cluster_colors)\n# plt.title('OPTICS Reachability Plot with Cluster Coloring and Dynamic Xi')\n# plt.xlabel('Ordered Data Points')\n# plt.ylabel('Reachability Distance')\n# plt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nXi Value[Silhouette Score Iterative] 0.17252252252252254\n```\n:::\n:::\n\n\n![](ReachabilityXi.png)\n\n![](ClustersDynamicXi.png)\n\nAs one can see the number of clusters is different here and more likely than not indicative of my ability/willingness to play with Xi values than the inherent performance of OPTICS. Regardless, it was much faster to simply establish a cutoff with the first method described than it was to iterate over Xi for this particular dataset. There may be cases where Xi is better equipped to cluster the dataset and as a result I kept it here for my own reference sometime in the future.\n\nThe resultant clustering is performed below:\n\n::: {#9f6f65f3 .cell execution_count=12}\n``` {.python .cell-code}\n# Scatter plot of clusters\n# plt.figure(figsize=(10, 7))\n\n# for klass, color in zip(unique_labels, colors):\n#     class_member_mask = (labels == klass)\n#     xy = data[class_member_mask]\n#     if klass != -1:\n#         # Draw the convex hull and fill it with a light shade of the cluster color\n#         if xy.shape[0] > 2:  # ConvexHull needs at least 3 points to compute the hull\n#             hull = ConvexHull(xy)\n#             plt.fill(xy[hull.vertices, 0], xy[hull.vertices, 1], color=color, alpha=0.2, edgecolor='k')\n#         plt.scatter(xy[:, 0], xy[:, 1], color=color, edgecolor='k', label=f'Cluster {klass}')\n#     else:\n#         plt.scatter(xy[:, 0], xy[:, 1], c='k', marker='x', label='Noise')\n\n# plt.title('Clusters by OPTICS with Dynamically Determined Xi and Hulls')\n# plt.xlabel('Feature 1')\n# plt.ylabel('Feature 2')\n# plt.legend()\n# plt.show()\n```\n:::\n\n\nFinally, I got a little bit more neurotic and decided to iteratively vary Xi and analyze a number of metrics including the Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index(Variance Ratio Criterion), the Adjusted Rand Index, and Mutual Information. A short description of each is below:\n\n1.  **Silhouette Score**: This metric evaluates how well each data point fits within its assigned cluster compared to other clusters. A high silhouette value indicates that the point is well matched to its own cluster and poorly matched to neighboring clusters, with values ranging from -1 (poor clustering) to +1 (excellent clustering).\n\n2.  **Davies-Bouldin Index**: An internal evaluation scheme where lower values indicate better clustering. It measures the average 'similarity' between each cluster and the cluster most similar to it, where similarity is a ratio of within-cluster distances to between-cluster distances.\n\n3.  **Calinski-Harabasz Index (Variance Ratio Criterion)**: This index is a ratio of the sum of between-clusters dispersion and within-cluster dispersion for all clusters. Essentially, it evaluates clusters by how well-separated they are and how compact they are, with higher values generally indicating better-defined clusters.\n\n4.  **Adjusted Rand Index**: This is a measure of the similarity between two data clusterings. It is adjusted for chance, providing a value that denotes random labeling independently of the number of clusters and samples. It ranges from -1 to +1, where higher values indicate better agreement between clustering assignments.\n\n5.  **Mutual Information**: This measures the amount of information one can obtain about one random variable by observing another random variable. The Adjusted Mutual Information (AMI) score is an adjustment of the Mutual Information Score that accounts for chance, making it a more reliable metric in assessing the agreement of the clustering result with the ground truth, if available. This one is really just thrown because why not?\n\nThe code is below and I parallelized the calculations as it would have taken quite some time otherwise.\n\n::: {#0fb34549 .cell execution_count=13}\n``` {.python .cell-code}\n# Neuroticism:\n\nfrom sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, adjusted_mutual_info_score\nfrom joblib import Parallel, delayed #I have 16 cores and I will use 16 cores....\n\n# Define the function to compute all metrics\ndef compute_metrics(xi, data, labels_true):\n    model = OPTICS(min_samples=min_samples, xi=xi, cluster_method='xi')\n    model.fit(data)\n    labels = model.labels_\n    \n    # Calculate metrics only if valid clustering is formed\n    if len(set(labels)) > 1 and np.any(labels != -1):\n        sil_score = silhouette_score(data, labels)\n        db_score = davies_bouldin_score(data, labels)\n        ch_score = calinski_harabasz_score(data, labels)\n        ar_score = adjusted_rand_score(labels_true, labels)\n        mi_score = adjusted_mutual_info_score(labels_true, labels)\n    else:\n        sil_score = db_score = ch_score = ar_score = mi_score = -1  # Penalty for bad or invalid clustering\n    \n    return (sil_score, db_score, ch_score, ar_score, mi_score)\n\n# Range of Xi values\nxi_values = np.linspace(0.01, 1, 1000)  # Using 20 Xi values for demonstration\n\n# Parallel computation of metrics\nresults = Parallel(n_jobs=-1)(delayed(compute_metrics)(xi, data, labels) for xi in xi_values)\n\n# Extract the results\nsilhouette_scores, db_scores, ch_scores, ar_scores, mi_scores = zip(*results)\n\n# # Plotting all the metrics\n# fig, axs = plt.subplots(5, 1, figsize=(10, 25))\n\n# # Silhouette Score\n# axs[0].plot(xi_values, silhouette_scores, marker='o')\n# axs[0].set_title('Silhouette Score vs Xi')\n# axs[0].set_xlabel('Xi')\n# axs[0].set_ylabel('Silhouette Score')\n\n# # Davies-Bouldin Score\n# axs[1].plot(xi_values, db_scores, marker='o')\n# axs[1].set_title('Davies-Bouldin Score vs Xi')\n# axs[1].set_xlabel('Xi')\n# axs[1].set_ylabel('Davies-Bouldin Score')\n\n# # Calinski-Harabasz Index\n# axs[2].plot(xi_values, ch_scores, marker='o')\n# axs[2].set_title('Calinski-Harabasz Index vs Xi')\n# axs[2].set_xlabel('Xi')\n# axs[2].set_ylabel('Calinski-Harabasz Index')\n\n# # Adjusted Rand Index\n# axs[3].plot(xi_values, ar_scores, marker='o')\n# axs[3].set_title('Adjusted Rand Index vs Xi')\n# axs[3].set_xlabel('Xi')\n# axs[3].set_ylabel('Adjusted Rand Score')\n\n# # Mutual Information\n# axs[4].plot(xi_values, mi_scores, marker='o')\n# axs[4].set_title('Mutual Information vs Xi')\n# axs[4].set_xlabel('Xi')\n# axs[4].set_ylabel('Mutual Information Score')\n\n# plt.tight_layout()\n# plt.show()\n\n# Normalize scores\n# Invert Davies-Bouldin scores since lower is better\ndb_scores = 1 / np.array(db_scores)\n\n# Normalize all scores to be between 0 and 1\nsilhouette_norm = (silhouette_scores - np.min(silhouette_scores)) / (np.max(silhouette_scores) - np.min(silhouette_scores))\ndb_norm = (db_scores - np.min(db_scores)) / (np.max(db_scores) - np.min(db_scores))\nch_norm = (ch_scores - np.min(ch_scores)) / (np.max(ch_scores) - np.min(ch_scores))\nar_norm = (ar_scores - np.min(ar_scores)) / (np.max(ar_scores) - np.min(ar_scores))\nmi_norm = (mi_scores - np.min(mi_scores)) / (np.max(mi_scores) - np.min(mi_scores))\n\n# Assume equal weighting for simplicity, sum normalized scores\nweighted_scores = silhouette_norm + db_norm + ch_norm + ar_norm + mi_norm\n\n# Find the Xi with the highest weighted score\noptimal_xi_index = np.argmax(weighted_scores)\noptimal_xi = xi_values[optimal_xi_index]\n\n# # Plot each metric\n# fig, ax = plt.subplots(5, 1, figsize=(10, 20))\n# ax[0].plot(xi_values, silhouette_norm, label='Silhouette Score')\n# ax[1].plot(xi_values, db_norm, label='Davies-Bouldin Index')\n# ax[2].plot(xi_values, ch_norm, label='Calinski-Harabasz Score')\n# ax[3].plot(xi_values, ar_norm, label='Adjusted Rand Index')\n# ax[4].plot(xi_values, mi_norm, label='Adjusted Mutual Information')\n\n# for a in ax:\n#     a.set_xlabel('Xi Value')\n#     a.set_ylabel('Normalized Score')\n#     a.legend()\n#     a.grid(True)\n\n# plt.tight_layout()\n# plt.show()\n\n# Normalize and invert scores where necessary\nsilhouette_norm = (silhouette_scores - np.min(silhouette_scores)) / (np.max(silhouette_scores) - np.min(silhouette_scores))\ndb_norm = 1 - ((db_scores - np.min(db_scores)) / (np.max(db_scores) - np.min(db_scores)))\nch_norm = (ch_scores - np.min(ch_scores)) / (np.max(ch_scores) - np.min(ch_scores))\nar_norm = (ar_scores - np.min(ar_scores)) / (np.max(ar_scores) - np.min(ar_scores))\nmi_norm = (mi_scores - np.min(mi_scores)) / (np.max(mi_scores) - np.min(mi_scores))\n\n# Calculate the average Xi from the max indices\noptimal_indices = [np.argmax(silhouette_norm), np.argmax(db_norm), np.argmax(ch_norm), np.argmax(ar_norm), np.argmax(mi_norm)]\noptimal_xis = xi_values[optimal_indices]\naverage_xi = np.mean(optimal_xis)\n\nprint(\"Optimal Xi values from each metric:\", optimal_xis)\nprint(\"Average Optimal Xi:\", average_xi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimal Xi values from each metric: [0.17252252 0.26666667 0.04963964 0.17252252 0.17252252]\nAverage Optimal Xi: 0.1667747747747748\n```\n:::\n:::\n\n\nHere are the scores:\n\n![](MetricsScores.png)\n\nAnd the resultant clustering:\n\n![](ReachabilityMetricsXi.png)\n\n![](MetricsClustering.png)\n\nAs you can see the data gets segmented into 2 distinct clusters. Interestingly enough the values obtained for the metrics are below:\n\n| Metric                                             | Xi         |\n|----------------------------------------------------|------------|\n| Silhouette Score                                   | 0.17252252 |\n| Davies-Bouldin Index                               | 0.26666667 |\n| Calinski-Harabasz Index (Variance Ratio Criterion) | 0.04963964 |\n| Adjusted Rand Index                                | 0.05459459 |\n| Mutual Information                                 | 0.05459459 |\n\n: Metrics, Score Normalized, Xi Value\n\nIf we omit the Silhouette Score and Davies-Bouldin Index we get the resulting clustering:\n\n![](MetricsClusteringOmitSilhouetteDBIndex.png)\n\nThis can be viewed as an improvement if the upper left cluster could be considered sparse enough to be considered noise/irrelevant.\n\nOverall, I could see how Xi could be useful in clustering datasets, but I definitely prefer to extract a DBSCAN-like clustering from an OPTICS model and work with that as its pretty fast. Iteratively using Xi has been outlined here and if I ever come across a use case this will be a pretty good reference.\n\nAnd there you have it. Original code [here](Optics.py).\n\n",
    "supporting": [
      "OPTICS_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}